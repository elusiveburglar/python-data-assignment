{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.rottentomatoes.com/top/bestofrt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rottentomatoes = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypage = bs4.BeautifulSoup(rottentomatoes.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytable = mypage.find_next('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytable = mypage.find(\"table\", {\"class\":\"table\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylinks = mytable.select('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [a.attrs.get('href') for a in mytable.select('a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "import requests\n",
      "requests\n",
      " 1/2: pip install requests\n",
      " 1/3: python -m pip install requests\n",
      " 1/4: pip3 install requests\n",
      " 1/5: pip install requests\n",
      " 1/6: $ pip install requests\n",
      " 2/1: import requests\n",
      " 3/1: import request\n",
      " 3/2: pip install requests\n",
      " 3/3: import requests\n",
      " 4/1: python -m pip install --upgrade pip\n",
      " 5/1: import requests\n",
      " 5/2: !pip install requests\n",
      " 5/3: import requests\n",
      " 5/4:\n",
      "import pip\n",
      "\n",
      "def install(package):\n",
      "   pip.main(['install', package])\n",
      "\n",
      "install('requests')\n",
      " 5/5:\n",
      "import sys\n",
      "sys.path\n",
      " 6/1: import requests\n",
      " 6/2: requests\n",
      " 6/3: url = 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime=2014-01-01&endtime=2014-01-02'\n",
      " 6/4: url\n",
      " 6/5: r = requests.get(url)\n",
      " 6/6: data = r.json()\n",
      " 6/7: type(data)\n",
      " 6/8: data.keys()\n",
      " 6/9: data['bbox']\n",
      "6/10: type[data['features']]\n",
      "6/11: type(data['features'])\n",
      "6/12: len(data['features'])\n",
      "6/13: e = data['features'][300]\n",
      "6/14: e\n",
      "6/15: mag = e['properties']['mag']\n",
      "6/16: time = e['properties']['time']\n",
      "6/17: e.keys\n",
      "6/18: e.keys()\n",
      "6/19: e['geometry'].keys()\n",
      "6/20: e['geometry']['coordinates']\n",
      "6/21: lon = e['geometry']['coordinates'][0]\n",
      "6/22: lat = e['geometry']['coordinates'][1]\n",
      "6/23: time, lon, lat, mag\n",
      "6/24:\n",
      "events = []:\n",
      "    for e in data ['features']:\n",
      "        mag = e['properties']['mag']\n",
      "        time = e['properties']['time']\n",
      "        lon = e['geometry']['coordinates'][0]\n",
      "        lat = e['geometry']['coordinates'][1]\n",
      "        events.append([time, lon, lat, mag])\n",
      "6/25:\n",
      "events = []\n",
      "    for e in data ['features']:\n",
      "        mag = e['properties']['mag']\n",
      "        time = e['properties']['time']\n",
      "        lon = e['geometry']['coordinates'][0]\n",
      "        lat = e['geometry']['coordinates'][1]\n",
      "        events.append([time, lon, lat, mag])\n",
      "6/26:\n",
      "events = []\n",
      "for e in data ['features']:\n",
      "    mag = e['properties']['mag']\n",
      "    time = e['properties']['time']\n",
      "    lon = e['geometry']['coordinates'][0]\n",
      "    lat = e['geometry']['coordinates'][1]\n",
      "    events.append([time, lon, lat, mag])\n",
      "6/27: events\n",
      "6/28:\n",
      "import csv\n",
      "csv.file = file('earthquake.csv', 'wb')\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/29:\n",
      "import csv\n",
      "csvfile = file('earthquake.csv', 'wb')\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/30:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', 'wb')\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/31:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', 'w')\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/32:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', 'w')\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/33:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', mode = 'w')\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/34:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', mode = 'w')\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/35:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', mode = 'w')\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/36:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', mode='w')\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/37:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', mode='w')\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/38:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', mode=\"w\")\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/39:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', mode = \"w\")\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/40:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', \"rw+\")\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/41:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', mode = w)\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/42:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', mode = 'w')\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/43:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', \"w\")\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/44:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', \"w\")\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/45:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', \"w\")\n",
      "writer = csv.writer('earthquake.csv')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/46:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', \"w\")\n",
      "writer = csv.writer('csvfile')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/47:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', \"w\")\n",
      "writer = csv.writer('csvfile')\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/48:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', \"w\")\n",
      "writer = csv.writer(csvfile)\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csv.close()\n",
      "6/49:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', \"w\")\n",
      "writer = csv.writer(csvfile)\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csvfile.close()\n",
      "6/50: open('earthquake,csv', \"r\")\n",
      "6/51: open('earthquake.csv', \"r\")\n",
      "6/52:\n",
      "import csv\n",
      "csvfile = open('earthquake.csv', \"w\")\n",
      "writer = csv.writer(csvfile)\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csvfile.close()\n",
      "6/53: o = open('earthquake.csv', \"r\")\n",
      "6/54: o.close()\n",
      "6/55:\n",
      "urls = 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime=2000-01-01&minlatitude=26&maxlatitude=35&minlongitude=97&maxlongitude=109&minmagnitude=4'\n",
      "earth = request.get(urls)\n",
      "6/56:\n",
      "import request\n",
      "urls = 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime=2000-01-01&minlatitude=26&maxlatitude=35&minlongitude=97&maxlongitude=109&minmagnitude=4'\n",
      "earth = request.get(urls)\n",
      "6/57:\n",
      "import requests\n",
      "urls = 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime=2000-01-01&minlatitude=26&maxlatitude=35&minlongitude=97&maxlongitude=109&minmagnitude=4'\n",
      "earth = requests.get(urls)\n",
      "6/58:\n",
      "data = earth.json()\n",
      "mag = earth['properties']['mag']\n",
      "time = earth['properties']['time']\n",
      "lon = earth['geometry']['coordinates'][0]\n",
      "lat = earth['geometry']['coordinates'][1]\n",
      "6/59:\n",
      "import requests\n",
      "urls = 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime=2000-01-01&minlatitude=26&maxlatitude=35&minlongitude=97&maxlongitude=109&minmagnitude=4'\n",
      "r = requests.get(urls)\n",
      "6/60:\n",
      "data = r.json()\n",
      "mag = earth['properties']['mag']\n",
      "time = earth['properties']['time']\n",
      "lon = earth['geometry']['coordinates'][0]\n",
      "lat = earth['geometry']['coordinates'][1]\n",
      "6/61:\n",
      "data = r.json()\n",
      "earth = data['features'][300]\n",
      "mag = earth['properties']['mag']\n",
      "time = earth['properties']['time']\n",
      "lon = earth['geometry']['coordinates'][0]\n",
      "lat = earth['geometry']['coordinates'][1]\n",
      "6/62:\n",
      "data = r.json()\n",
      "earth = data['features'][300]\n",
      "mag = earth['properties']['mag']\n",
      "time = earth['properties']['time']\n",
      "lon = earth['geometry']['coordinates'][0]\n",
      "lat = earth['geometry']['coordinates'][1]\n",
      "events=[]\n",
      "for e in data['features']:\n",
      "    mag = earth['properties']['mag']\n",
      "    time = earth['properties']['time']\n",
      "    lon = earth['geometry']['coordinates'][0]\n",
      "    lat = earth['geometry']['coordinates'][1]\n",
      "    events.append([time, lon, lat, mag])\n",
      "6/63:\n",
      "import csv\n",
      "csvfile = open('sichuanearthquake.scv', \"w\")\n",
      "writer = csv.writer(csvfile)\n",
      "writer.writerrow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csvfile.close()\n",
      "6/64:\n",
      "import csv\n",
      "csvfile = open('sichuanearthquake.scv', \"w\")\n",
      "writer = csv.writer(csvfile)\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csvfile.close()\n",
      "6/65:\n",
      "import csv\n",
      "csvfile = open('sichuanearthquake.scv', \"w\")\n",
      "writer = csv.writer(csvfile)\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csvfile.close()\n",
      "6/66:\n",
      "data = r.json()\n",
      "earth = data['features'][300]\n",
      "mag = earth['properties']['mag']\n",
      "time = earth['properties']['time']\n",
      "lon = earth['geometry']['coordinates'][0]\n",
      "lat = earth['geometry']['coordinates'][1]\n",
      "events=[]\n",
      "for e in data['features']:\n",
      "    mag = data['properties']['mag']\n",
      "    time = data['properties']['time']\n",
      "    lon = data['geometry']['coordinates'][0]\n",
      "    lat = data['geometry']['coordinates'][1]\n",
      "    events.append([time, lon, lat, mag])\n",
      "6/67:\n",
      "data = r.json()\n",
      "earth = data['features']\n",
      "mag = earth['properties']['mag']\n",
      "time = earth['properties']['time']\n",
      "lon = earth['geometry']['coordinates'][0]\n",
      "lat = earth['geometry']['coordinates'][1]\n",
      "events=[]\n",
      "for e in data['features']:\n",
      "    mag = earth['properties']['mag']\n",
      "    time = earth['properties']['time']\n",
      "    lon = earth['geometry']['coordinates'][0]\n",
      "    lat = earth['geometry']['coordinates'][1]\n",
      "    events.append([time, lon, lat, mag])\n",
      "6/68:\n",
      "data = r.json()\n",
      "events=[]\n",
      "for e in data['features']:\n",
      "    mag = earth['properties']['mag']\n",
      "    time = earth['properties']['time']\n",
      "    lon = earth['geometry']['coordinates'][0]\n",
      "    lat = earth['geometry']['coordinates'][1]\n",
      "    events.append([time, lon, lat, mag])\n",
      "6/69:\n",
      "data = r.json()\n",
      "e = data['features']\n",
      "events=[]\n",
      "for e in data['features']:\n",
      "    mag = earth['properties']['mag']\n",
      "    time = earth['properties']['time']\n",
      "    lon = earth['geometry']['coordinates'][0]\n",
      "    lat = earth['geometry']['coordinates'][1]\n",
      "    events.append([time, lon, lat, mag])\n",
      "6/70:\n",
      "data = r.json()\n",
      "earth = data['features']\n",
      "events=[]\n",
      "for earth in data['features']:\n",
      "    mag = earth['properties']['mag']\n",
      "    time = earth['properties']['time']\n",
      "    lon = earth['geometry']['coordinates'][0]\n",
      "    lat = earth['geometry']['coordinates'][1]\n",
      "    events.append([time, lon, lat, mag])\n",
      "6/71:\n",
      "import csv\n",
      "csvfile = open('sichuanearthquake.scv', \"w\")\n",
      "writer = csv.writer(csvfile)\n",
      "writer.writerow(['time', 'lon', 'lat', 'mag'])\n",
      "writer.writerows(events)\n",
      "csvfile.close()\n",
      " 9/1:\n",
      "e = data['quoteSummary']['result'][0]['financialData']['ebitda']\n",
      "e.keys()\n",
      " 9/2: url = 'https://query1.finance.yahoo.com/v10/finance/quoteSummary/AAPL?formatted=true&lang=en-US&region=US&modules=financialData'\n",
      " 9/3: url\n",
      " 9/4: import requests\n",
      " 9/5: r = requests.get(url)\n",
      " 9/6:\n",
      "data = r.json()\n",
      "data\n",
      " 9/7: data.keys()\n",
      " 9/8:\n",
      "e = data['quoteSummary']['result'][0]['financialData']['ebitda']\n",
      "e.keys()\n",
      " 9/9: e['longfmt']\n",
      "9/10: e['fmt']\n",
      "9/11: e['raw']\n",
      "9/12:\n",
      "ebitda = []\n",
      "for e in data['quoteSummary']['result'][0]['financialData']['ebitda']:\n",
      "    raw = e['raw']\n",
      "    fmt = e['fmt']\n",
      "    longFmt = e['longFmt']\n",
      "    events.append([raw,fmt,longFmt])\n",
      "9/13: int(e['raw'])\n",
      "9/14: e['raw']\n",
      "9/15: e['raw']\n",
      "9/16: e['raw']\n",
      "9/17: url = 'https://query1.finance.yahoo.com/v10/finance/quoteSummary/AAPL?formatted=true&lang=en-US&region=US&modules=financialData'\n",
      "9/18: url\n",
      "9/19: import requests\n",
      "9/20: r = requests.get(url)\n",
      "9/21:\n",
      "data = r.json()\n",
      "data\n",
      "9/22: data.keys()\n",
      "9/23:\n",
      "e = data['quoteSummary']['result'][0]['financialData']['ebitda']\n",
      "e.keys()\n",
      "9/24: e['raw']\n",
      "10/1: open('/trade-wars-news1.txt').read()\n",
      "10/2: open('https://github.com/hupili/python-for-data-and-media-communication-gitbook/blob/master/assets/assignment0/trade-wars-news1.txt').read()\n",
      "11/1: read ('python-for-data-and-media-communication-gitbook/assets/assignment0/trade-wars-news2.txt')\n",
      "11/2: read('python-for-data-and-media-communication-gitbook/assets/assignment0/trade-wars-news2.txt')\n",
      "11/3: u = read('python-for-data-and-media-communication-gitbook/assets/assignment0/trade-wars-news2.txt', \"r\")\n",
      "11/4:\n",
      "import sys\n",
      "u = read('python-for-data-and-media-communication-gitbook/assets/assignment0/trade-wars-news2.txt', \"r\")\n",
      "11/5:\n",
      "import sys\n",
      "u = read('https://python-for-data-and-media-communication-gitbook/assets/assignment0/trade-wars-news2.txt', \"r\")\n",
      "11/6:\n",
      "import sys\n",
      "u = read('https://github.com/hupili/python-for-data-and-media-communication-gitbook/blob/master/assets/assignment0/trade-wars-news2.txt', \"r\")\n",
      "11/7:\n",
      "import sys\n",
      "u = read('/trade-wars-news1.txt', \"r\")\n",
      "11/8:\n",
      "import sys\n",
      "u = open('/trade-wars-news1.txt', \"r\")\n",
      "11/9:\n",
      "import sys\n",
      "u = open('C:/Users/burni/Downloads/trade-wars-news1.txt', \"r\")\n",
      "11/10: u\n",
      "11/11: read(U)\n",
      "11/12: u.read()\n",
      "11/13: lines = f.readlines()\n",
      "11/14: lines = u.readlines()\n",
      "11/15: lines\n",
      "11/16: print（lines）\n",
      "11/17: lines\n",
      "11/18: lines = u.read()\n",
      "11/19: lines\n",
      "11/20: u.read()\n",
      "11/21:\n",
      "import sys\n",
      "u = open('C:/Users/burni/Downloads/trade-wars-news1.txt', \"r\")\n",
      "11/22: u.read()\n",
      "11/23: lines\n",
      "11/24:\n",
      "import sys\n",
      "u = open('C:/Users/burni/Downloads/trade-wars-news1.txt', \"r\")\n",
      "11/25: u.read()\n",
      "11/26:\n",
      "import sys\n",
      "u = open('C:/Users/burni/Downloads/trade-wars-news1.txt', \"r\")\n",
      "11/27:\n",
      "import sys\n",
      "u = open('C:/Users/burni/Downloads/trade-wars-news1.txt', encoding='gbk', \"r\")\n",
      "11/28:\n",
      "import sys\n",
      "u = open('C:/Users/burni/Downloads/trade-wars-news1.txt', encoding='gbk')\n",
      "11/29: u.read()\n",
      "11/30: lines\n",
      "11/31:\n",
      "import sys\n",
      "path = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "file = open(path, encoding='gbk')\n",
      "11/32:\n",
      "import sys\n",
      "path = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "file = open(path, encoding='gbk').read()\n",
      "11/33:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "for path in paths:\n",
      "    file = open(path).read()\n",
      "11/34:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "for path in paths:\n",
      "    file = open(path, encoding='gbk').read()\n",
      "11/35:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8').read()\n",
      "11/36: file\n",
      "11/37:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8').readline()\n",
      "11/38: file\n",
      "11/39:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8').readlines()\n",
      "11/40: file\n",
      "11/41:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8').write(readlines())\n",
      "11/42:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8')\n",
      "11/43:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8')\n",
      "    tradewars.write(file.readlines())\n",
      "11/44:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8')\n",
      "    tradewars.write(file.read())\n",
      "11/45: tradewars\n",
      "11/46:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8')\n",
      "    tradewars.write(file.read(), encoding='utf-8')\n",
      "11/47:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8')\n",
      "    tradewars.write(file.read())\n",
      "    \n",
      "tradewars.close()\n",
      "11/48: tradewars\n",
      "11/49:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"w\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8')\n",
      "    tradewars.write(file.read())\n",
      "    \n",
      "tradewars.close()\n",
      "11/50: tradewars\n",
      "11/51:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"w\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8')\n",
      "11/52: file\n",
      "11/53:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"w\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8').read()\n",
      "11/54: file\n",
      "11/55:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"w\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8').readlines()\n",
      "11/56: file\n",
      "11/57:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"w\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8').readlines()\n",
      "    tradewars.append(file)\n",
      "11/58:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8').readlines()\n",
      "    tradewars.write(file)\n",
      "11/59:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8').read()\n",
      "    tradewars.write(file)\n",
      "11/60: file\n",
      "11/61: tradewars\n",
      "11/62:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8').read()\n",
      "    tradewars.write(file)\n",
      "    \n",
      "tradewars.close()\n",
      "11/63: tradewars\n",
      "11/64:\n",
      "import sys\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\", encoding=\"utf-8\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8').read()\n",
      "    tradewars.write(file)\n",
      "    \n",
      "tradewars.close()\n",
      "11/65: tradewars\n",
      "11/66:\n",
      "import sys\n",
      "import codec\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = codecs.open(\"tradewars.txt\", \"a\", encoding=\"utf-8\")\n",
      "for path in paths:\n",
      "    file = codecs.open(path, encoding='utf-8').read()\n",
      "    tradewars.write(file.decode(\"utf-8\"))\n",
      "    \n",
      "tradewars.close()\n",
      "11/67:\n",
      "import sys\n",
      "import codecs\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = codecs.open(\"tradewars.txt\", \"a\", encoding=\"utf-8\")\n",
      "for path in paths:\n",
      "    file = codecs.open(path, encoding='utf-8').read()\n",
      "    tradewars.write(file.decode(\"utf-8\"))\n",
      "    \n",
      "tradewars.close()\n",
      "11/68:\n",
      "import sys\n",
      "import codecs\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = codecs.open(\"tradewars.txt\", \"a\", encoding=\"utf-8\")\n",
      "for path in paths:\n",
      "    file = codecs.open(path, encoding='utf-8').read()\n",
      "    tradewars.write(file)\n",
      "    \n",
      "tradewars.close()\n",
      "11/69: tradewars\n",
      "11/70: tradewars\n",
      "11/71:\n",
      "import sys\n",
      "import codecs\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = codecs.open(\"tradewars.txt\", \"a\", encoding=\"utf-8\")\n",
      "for path in paths:\n",
      "    file = codecs.open(path, encoding='utf-8').read()\n",
      "    tradewars.writelines(file)\n",
      "    \n",
      "tradewars.close()\n",
      "11/72: tradewars\n",
      "11/73:\n",
      "import sys\n",
      "\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\", encoding=\"utf-8\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8').read()\n",
      "    tradewars.writelines(file)\n",
      "    \n",
      "tradewars.close()\n",
      "11/74: tradewars\n",
      "11/75:\n",
      "import sys\n",
      "\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\", encoding=\"utf-8\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8').read()\n",
      "    tradewars.writelines(file)\n",
      "    \n",
      "tradewars.close()\n",
      "11/76: tradewars\n",
      "11/77:\n",
      "import sys\n",
      "\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\", encoding=\"utf-8\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8', errors='ignore').read()\n",
      "    tradewars.writelines(file)\n",
      "    \n",
      "tradewars.close()\n",
      "11/78: tradewars\n",
      "13/1:\n",
      "import sys\n",
      "\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\", encoding=\"utf-8\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8', errors='ignore').read()\n",
      "    tradewars.writelines(file)\n",
      "    \n",
      "tradewars.close()\n",
      "13/2: tradewars\n",
      "13/3: tradewars\n",
      "13/4: tradewars.spilt()\"\"\n",
      "13/5: tradewars.spilt()\n",
      "13/6: tradewars\n",
      "13/7:\n",
      "with open('tradewars.txt') as f:\n",
      "    mylist = list(f)\n",
      "13/8:\n",
      "with open('tradewars.txt', encoding='utf-8') as f:\n",
      "    mylist = list(f)\n",
      "13/9:\n",
      "with open('goodlines.txt') as f:\n",
      "    mylist = [line.rstrip('\\n') for line in f]\n",
      "13/10:\n",
      "with open('tradewars.txt') as f:\n",
      "    mylist = [line.rstrip('\\n') for line in f]\n",
      "13/11:\n",
      "with open('tradewars.txt', encoding='uft=8') as f:\n",
      "    mylist = [line.rstrip('\\n') for line in f]\n",
      "13/12:\n",
      "with open('tradewars.txt', encoding='utf=8') as f:\n",
      "    mylist = [line.rstrip('\\n') for line in f]\n",
      "13/13: mylisT\n",
      "13/14: mylist\n",
      "13/15: mylist.spilt(' ')\n",
      "13/16:\n",
      "a = ' '\n",
      "mylist.spilt(a)\n",
      "13/17:\n",
      "a = ' '\n",
      "mylist.spilt()\n",
      "13/18: str(mylist)\n",
      "13/19: mylist.spilt()\n",
      "13/20: mylist.spilt()\n",
      "13/21: news = str(mylist)\n",
      "13/22: news.spilt()\n",
      "13/23: news.split()\n",
      "13/24: news\n",
      "18/1: news.replace(\"\\'\", \" \")\n",
      "18/2:\n",
      "import sys\n",
      "\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\", encoding=\"utf-8\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8', errors='ignore').read()\n",
      "    tradewars.writelines(file)\n",
      "    \n",
      "tradewars.close()\n",
      "18/3: tradewars\n",
      "18/4:\n",
      "with open('tradewars.txt', encoding='utf-8') as f:\n",
      "    mylist = list(f)\n",
      "18/5:\n",
      "with open('tradewars.txt', encoding='utf=8') as f:\n",
      "    mylist = [line.rstrip('\\n') for line in f]\n",
      "18/6: news = str(mylist)\n",
      "18/7: news.replace(\"\\'\", \" \")\n",
      "18/8: news.replace(\"\\”\", \" \")\n",
      "18/9: news.replace(\"\\”\", \" \")\n",
      "18/10: news.replace(\"\\'\", \" \")\n",
      "18/11: news.replace(\"\\”\", \" \")\n",
      "18/12: news = news.replace(\"\\'\", \" \")\n",
      "18/13: news\n",
      "18/14: news = news.replace(\"，\", \" \")\n",
      "18/15: news\n",
      "18/16: news = news.replace(\",\", \" \")\n",
      "18/17: news\n",
      "18/18: news = news.replace(\".\", \" \")\n",
      "18/19: news\n",
      "18/20: news = news.replace(\"\\\", \" \")\n",
      "18/21: news = news.replace(\"\\\\\", \" \")\n",
      "18/22: news\n",
      "18/23: news = news.replace(\"\\”\", \" \")\n",
      "18/24: news\n",
      "18/25: news = news.replace(\"”\", \" \")\n",
      "18/26: news\n",
      "18/27: news = news.replace(\"“\", \" \")\n",
      "18/28: news\n",
      "18/29: news = news.replace(\"‘\", \" \")\n",
      "18/30:\n",
      "def shanchu(punc):\n",
      "    news = news.replace(punc, \" \")\n",
      "    return\n",
      "18/31: news\n",
      "18/32: shanchu(\")\n",
      "18/33: shanchu(\\\")\n",
      "18/34:\n",
      "def shanchu(punc):\n",
      "    news = news.replace(\"%s\"%(str(punc)), \" \")\n",
      "    return\n",
      "18/35: news\n",
      "18/36: shanchu(\")\n",
      "18/37: shanchu('\"\"')\n",
      "18/38:\n",
      "def shanchu(punc):\n",
      "    news.replace(\"%s\"%(str(punc)), \" \")\n",
      "    return\n",
      "18/39: news\n",
      "18/40: news = shanchu('\"\"')\n",
      "18/41: news\n",
      "18/42: news\n",
      "18/43: news\n",
      "18/44: news\n",
      "18/45: news = news.replace(\"‘\", \" \")\n",
      "18/46: news = str(mylist)\n",
      "18/47: news = news.replace(\"\\'\", \" \")\n",
      "18/48: news = news.replace(\"，\", \" \")\n",
      "18/49: news = news.replace(\",\", \" \")\n",
      "18/50: news = news.replace(\".\", \" \")\n",
      "18/51: news = news.replace(\"\\\\\", \" \")\n",
      "18/52: news = news.replace(\"”\", \" \")\n",
      "18/53: news = news.replace(\"“\", \" \")\n",
      "18/54: news = news.replace(\"‘\", \" \")\n",
      "18/55: news = shanchu('\"')\n",
      "18/56: news\n",
      "18/57: news = str(mylist)\n",
      "18/58: news = news.replace(\"\\'\", \" \")\n",
      "18/59: news = news.replace(\"，\", \" \")\n",
      "18/60: news = news.replace(\",\", \" \")\n",
      "18/61: news = news.replace(\".\", \" \")\n",
      "18/62: news = news.replace(\"\\\\\", \" \")\n",
      "18/63: news = news.replace(\"”\", \" \")\n",
      "18/64: news = news.replace(\"“\", \" \")\n",
      "18/65: news\n",
      "18/66: news = shanchu(/\")\n",
      "18/67: news = shanchu(\\\")\n",
      "18/68:\n",
      "def shanchu(punc):\n",
      "    news.replace(\"%s\"%(punc), \" \")\n",
      "    return\n",
      "18/69: news\n",
      "18/70: news = shanchu('\"')\n",
      "18/71: news\n",
      "18/72: news = str(mylist)\n",
      "18/73: news = news.replace(\"\\'\", \" \")\n",
      "18/74: news = news.replace(\"，\", \" \")\n",
      "18/75: news = news.replace(\",\", \" \")\n",
      "18/76: news = news.replace(\".\", \" \")\n",
      "18/77: news = news.replace(\"\\\\\", \" \")\n",
      "18/78: news = news.replace(\"”\", \" \")\n",
      "18/79: news = news.replace(\"“\", \" \")\n",
      "18/80: news = news.replace(\"‘\", \" \")\n",
      "18/81:\n",
      "def shanchu(punc):\n",
      "    news.replace(\"%s\"%(punc), \" \")\n",
      "    return\n",
      "18/82: news\n",
      "18/83: shanchu('\"')\n",
      "18/84: news = news.replace('/\"', \" \")\n",
      "18/85: news\n",
      "18/86: news = news.replace(\"\"\", \" \")\n",
      "18/87: news = news.replace('\"', \" \")\n",
      "18/88: news\n",
      "18/89: news = news.replace('\"', \" \")\n",
      "18/90: news\n",
      "18/91: news = news.replace(\":\", \" \")\n",
      "18/92: news\n",
      "18/93: news = news.replace(\"（\", \" \")\n",
      "18/94: news = news.replace(\"(\", \" \")\n",
      "18/95: news = news.replace(\"）\", \" \")\n",
      "18/96: news = news.replace(\")\", \" \")\n",
      "18/97: news = news.replace(\"U S \", \"U.S.\")\n",
      "18/98: news\n",
      "18/99: news.strip(' in ')\n",
      "18/100: news.strip(' of ')\n",
      "18/101: news.strip('of')\n",
      "18/102: news = news.replace(\". \", \" \")\n",
      "18/103: news = news.replace(\"\\\\\", \" \")\n",
      "18/104: news = news.replace(\"”\", \" \")\n",
      "18/105: news = news.replace(\"“\", \" \")\n",
      "18/106: news = news.replace(\"‘\", \" \")\n",
      "18/107: news = news.replace('\"', \" \")\n",
      "18/108: news = news.replace(\":\", \" \")\n",
      "18/109: news\n",
      "18/110: news = news.replace(\"U.S \", \"U.S.\")\n",
      "18/111: news\n",
      "18/112: news\n",
      "18/113: news = news.replace(\",\", \" \")\n",
      "18/114: news = str(mylist)\n",
      "18/115: news = news.replace(\"\\'\", \" \")\n",
      "18/116: news = news.replace(\"，\", \" \")\n",
      "18/117: news = news.replace(\",\", \" \")\n",
      "18/118: news\n",
      "18/119: news = news.replace(\". \", \" \")\n",
      "18/120: news = news.replace(\".\\b\", \" \")\n",
      "18/121: news\n",
      "18/122: news = news.replace(\"\\\\\", \" \")\n",
      "18/123: news = news.replace(\"”\", \" \")\n",
      "18/124: news = news.replace(\"“\", \" \")\n",
      "18/125: news = news.replace(\"‘\", \" \")\n",
      "18/126: news = news.replace('\"', \" \")\n",
      "18/127: news = news.replace(\":\", \" \")\n",
      "18/128: news\n",
      "18/129: news = news.replace(\"(\", \" \")\n",
      "18/130: news = news.replace(\")\", \" \")\n",
      "18/131: news = news.replace(\"U.S \", \"U.S.\")\n",
      "18/132: news\n",
      "18/133: news = news.replace(\"(\", \" \")\n",
      "18/134: news = news.replace(\")\", \" \")\n",
      "18/135: news = news.replace(\"U.S \", \"U.S.\")\n",
      "18/136: news\n",
      "18/137: news.strip('of')\n",
      "18/138: news.split(' ')\n",
      "18/139: words = words.remove(\"remove\")\n",
      "18/140: words = news.split(' ')\n",
      "18/141: words = words.remove(\"remove\")\n",
      "18/142: words = words.remove(remove)\n",
      "18/143: words = words.remove('the')\n",
      "18/144: words\n",
      "18/145: print(words)\n",
      "18/146: words = words.remove(\"the\")\n",
      "18/147: words = news.split(' ')\n",
      "18/148: words = words.remove(\"the\")\n",
      "18/149: print(words)\n",
      "18/150: words\n",
      "18/151: words = news.split(' ')\n",
      "18/152: words\n",
      "18/153: list(set(words))\n",
      "18/154: words = news.split(' ')\n",
      "18/155: list(set(words))\n",
      "18/156:\n",
      "def filter(word):\n",
      "    \n",
      "    while word in words:\n",
      "    print (words.remove(word))\n",
      "    \n",
      "    return\n",
      "18/157:\n",
      "def filter(word):\n",
      "    \n",
      "    while word in words:\n",
      "    print(words.remove(word))\n",
      "    \n",
      "    return\n",
      "18/158:\n",
      "def filter(word):\n",
      "    \n",
      "    while word in words:\n",
      "        print(words.remove(word))\n",
      "    \n",
      "    return\n",
      "18/159: filter('the')\n",
      "18/160:\n",
      "stopwords = ['the', 'a', 'an', 'it', 'that', 'to', 'its', 'of' , 'for']\n",
      "for word in list(words):  # iterating on a copy since removing will mess things up\n",
      "    if word in stopwords:\n",
      "        words.remove(word)\n",
      "18/161: words\n",
      "21/1: words\n",
      "21/2: words = news.split(' ')\n",
      "21/3: news\n",
      "21/4: news = news.replace(\"(\", \" \")\n",
      "21/5: news = news.replace(\")\", \" \")\n",
      "21/6: news = news.replace(\"U.S \", \"U.S.\")\n",
      "21/7: news\n",
      "21/8:\n",
      "import sys\n",
      "\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\", encoding=\"utf-8\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8', errors='ignore').read()\n",
      "    tradewars.writelines(file)\n",
      "    \n",
      "tradewars.close()\n",
      "21/9: tradewars\n",
      "21/10:\n",
      "with open('tradewars.txt', encoding='utf-8') as f:\n",
      "    mylist = list(f)\n",
      "21/11:\n",
      "with open('tradewars.txt', encoding='utf=8') as f:\n",
      "    mylist = [line.rstrip('\\n') for line in f]\n",
      "21/12: news = str(mylist)\n",
      "21/13: news = news.replace(\"\\'\", \" \")\n",
      "21/14: news = news.replace(\"，\", \" \")\n",
      "21/15: news = news.replace(\",\", \" \")\n",
      "21/16: news\n",
      "21/17: news = news.replace(\".\\b\", \" \")\n",
      "21/18: news\n",
      "21/19: news = news.replace(\"\\\\\", \" \")\n",
      "21/20: news = news.replace(\"”\", \" \")\n",
      "21/21: news = news.replace(\"“\", \" \")\n",
      "21/22: news = news.replace(\"‘\", \" \")\n",
      "21/23: news = news.replace('\"', \" \")\n",
      "21/24: news = news.replace(\":\", \" \")\n",
      "21/25: news\n",
      "21/26: news = news.replace(\"(\", \" \")\n",
      "21/27: news = news.replace(\")\", \" \")\n",
      "21/28: news = news.replace(\"U.S \", \"U.S.\")\n",
      "21/29: news\n",
      "21/30: words = news.split(' ')\n",
      "21/31:\n",
      "stopwords = ['the', 'a', 'an', 'it', 'that', 'to', 'its', 'of' , 'for']\n",
      "for word in list(words):  # iterating on a copy since removing will mess things up\n",
      "    if word in stopwords:\n",
      "        words.remove(word)\n",
      "21/32: words\n",
      "21/33:\n",
      "delete = [' ', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be']\n",
      "for a in delete:\n",
      "    words.delete(a)\n",
      "21/34:\n",
      "delete = [' ', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be']\n",
      "for a in delete:\n",
      "    words.remove(a)\n",
      "21/35:\n",
      "delete = [' ', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be']\n",
      "while a in delete:\n",
      "    words.remove(a)\n",
      "21/36:\n",
      "delete = [' ', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be']\n",
      "while i in delete:\n",
      "    words.remove(a)\n",
      "21/37:\n",
      "delete = [' ', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be']\n",
      "while i in delete:\n",
      "    words.remove(i)\n",
      "21/38:\n",
      "delete = [' ', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be']\n",
      "while i in delete:\n",
      "    if i in words\n",
      "    words.remove(i)\n",
      "21/39:\n",
      "delete = [' ', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be']\n",
      "while i in delete:\n",
      "    if i in words:\n",
      "    words.remove(i)\n",
      "21/40:\n",
      "delete = [' ', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be']\n",
      "while i in delete:\n",
      "    if i in words:\n",
      "    words.pop(i)\n",
      "21/41:\n",
      "delete = [' ', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be']\n",
      "while i in delete:\n",
      "    if i in words:\n",
      "        words.pop(i)\n",
      "21/42:\n",
      "delete = [' ', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be']\n",
      "newords = [elem for elem in words if elem not in delete]\n",
      "21/43: newords\n",
      "21/44:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be']\n",
      "newords = [elem for elem in words if elem not in delete]\n",
      "21/45: newords\n",
      "21/46:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/47: newords\n",
      "21/48: words\n",
      "21/49: newords = [elem in words]\n",
      "21/50: newords = [elem for elem in words]\n",
      "21/51: newords\n",
      "21/52:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/53: newords = [elem for elem in words]\n",
      "21/54: newords\n",
      "21/55: len(newords)\n",
      "21/56: newords\n",
      "21/57:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'my']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/58: newords = [elem for elem in words]\n",
      "21/59: newords\n",
      "21/60:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'my', 'By', 'by', 'What']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/61: newords = [elem for elem in words]\n",
      "21/62: newords\n",
      "21/63:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'my', 'By', 'by', 'What', 'The']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/64: newords = [elem for elem in words]\n",
      "21/65: newords\n",
      "21/66:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'my', 'By', 'by', 'What', 'The','’]\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/67: newords = [elem for elem in words]\n",
      "21/68:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'my', 'By', 'by', 'What', 'The','’']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/69: newords = [elem for elem in words]\n",
      "21/70: newords\n",
      "21/71:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and', 'my', 'By', 'by', 'What', 'The','’']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/72: newords = [elem for elem in words]\n",
      "21/73: newords\n",
      "21/74:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by', 'What', 'The','’']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/75: newords = [elem for elem in words]\n",
      "21/76: newords\n",
      "21/77:\n",
      "from collections import Counter\n",
      "Counter(newords)\n",
      "21/78:\n",
      "from collections import Counter\n",
      "d = Counter(newords)\n",
      "21/79:\n",
      "s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
      "    for k, v in s:\n",
      "        k, v\n",
      "21/80:\n",
      "s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
      "for k, v in s:\n",
      "    k, v\n",
      "21/81: s\n",
      "21/82:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by',\n",
      "          'at', 'What', 'The','’']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/83: newords = [elem for elem in words]\n",
      "21/84: newords\n",
      "21/85:\n",
      "from collections import Counter\n",
      "d = Counter(newords)\n",
      "21/86:\n",
      "s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
      "for k, v in s:\n",
      "    k, v\n",
      "21/87: s\n",
      "21/88:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by',\n",
      "          'at', 'What', 'The','’', 'which', 'will', ''s', 'we']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/89:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by',\n",
      "          'at', 'What', 'The','’', 'which', 'will', '’s', 'we']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/90: newords = [elem for elem in words]\n",
      "21/91: newords\n",
      "21/92:\n",
      "from collections import Counter\n",
      "d = Counter(newords)\n",
      "21/93:\n",
      "s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
      "for k, v in s:\n",
      "    k, v\n",
      "21/94:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by',\n",
      "          'at', 'What', 'The','’', 'which', 'will', 's', 'we']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/95: newords = [elem for elem in words]\n",
      "21/96: newords\n",
      "21/97:\n",
      "from collections import Counter\n",
      "d = Counter(newords)\n",
      "21/98:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by',\n",
      "          'at', 'What', 'The','’', 'which', 'will', 's', 'we', 'so', 'all']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/99: newords = [elem for elem in words]\n",
      "21/100: newords\n",
      "21/101:\n",
      "from collections import Counter\n",
      "d = Counter(newords)\n",
      "21/102:\n",
      "s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
      "for k, v in s:\n",
      "    k, v\n",
      "21/103: s\n",
      "21/104:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by',\n",
      "          'at', 'What', 'The','’', 'which', 'will', 's', 'we', 'so', 'all', 'This', 'who']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/105: newords = [elem for elem in words]\n",
      "21/106: newords\n",
      "21/107:\n",
      "from collections import Counter\n",
      "d = Counter(newords)\n",
      "21/108:\n",
      "s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
      "for k, v in s:\n",
      "    k, v\n",
      "21/109:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by',\n",
      "          'at', 'What', 'The','’', 'which', 'will', 's', 'we', 'so', 'all', 'This', 'who', 'than', 'but']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/110: newords = [elem for elem in words]\n",
      "21/111: newords\n",
      "21/112:\n",
      "from collections import Counter\n",
      "d = Counter(newords)\n",
      "21/113:\n",
      "s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
      "for k, v in s:\n",
      "    k, v\n",
      "21/114: s\n",
      "21/115:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by',\n",
      "          'at', 'What', 'The','’', 'which', 'will', 's', 'we', 'so', 'all', 'This', 'who', 'than', 'but', 'with', 'as', 'from']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/116: newords = [elem for elem in words]\n",
      "21/117: newords\n",
      "21/118:\n",
      "from collections import Counter\n",
      "d = Counter(newords)\n",
      "21/119:\n",
      "s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
      "for k, v in s:\n",
      "    k, v\n",
      "21/120: s\n",
      "21/121:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by',\n",
      "          'at', 'What', 'The','’', 'which', 'will', 's', 'we', 'so', 'all', 'This', 'who', 'than', 'but', 'with', 'as',\n",
      "          'from', 'also', 'have', 'would']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/122: newords = [elem for elem in words]\n",
      "21/123: newords\n",
      "21/124:\n",
      "from collections import Counter\n",
      "d = Counter(newords)\n",
      "21/125:\n",
      "s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
      "for k, v in s:\n",
      "    k, v\n",
      "21/126: s\n",
      "21/127:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by',\n",
      "          'at', 'What', 'The','’', 'which', 'will', 's', 'we', 'so', 'all', 'This', 'who', 'than', 'but', 'with', 'as',\n",
      "          'from', 'also', 'have', 'would', 'not','was','He']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/128: newords = [elem for elem in words]\n",
      "21/129: newords\n",
      "21/130:\n",
      "from collections import Counter\n",
      "d = Counter(newords)\n",
      "21/131:\n",
      "s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
      "for k, v in s:\n",
      "    k, v\n",
      "21/132: s\n",
      "21/133:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by',\n",
      "          'at', 'What', 'The','’', 'which', 'will', 's', 'we', 'so', 'all', 'This', 'who', 'than', 'but', 'with', 'as',\n",
      "          'from', 'also', 'have', 'would', 'not','was','He', 'been','while','or','His']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/134: newords = [elem for elem in words]\n",
      "21/135: newords\n",
      "21/136:\n",
      "from collections import Counter\n",
      "d = Counter(newords)\n",
      "21/137:\n",
      "s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
      "for k, v in s:\n",
      "    k, v\n",
      "21/138: s\n",
      "21/139:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by',\n",
      "          'at', 'What', 'The','’', 'which', 'will', 's', 'we', 'so', 'all', 'This', 'who', 'than', 'but', 'with', 'as',\n",
      "          'from', 'also', 'have', 'would', 'not','was','He', 'been','while','or','His', 'his']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/140: newords = [elem for elem in words]\n",
      "21/141: newords\n",
      "21/142:\n",
      "from collections import Counter\n",
      "d = Counter(newords)\n",
      "21/143:\n",
      "s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
      "for k, v in s:\n",
      "    k, v\n",
      "21/144:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by',\n",
      "          'at', 'What', 'The','’', 'which', 'will', 's', 'we', 'so', 'all', 'This', 'who', 'than', 'but', 'with', 'as',\n",
      "          'from', 'also', 'have', 'would', 'not','was','He', 'been','while','or','His', 'his', 'can', 'had','']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "21/145: newords = [elem for elem in words]\n",
      "21/146: newords\n",
      "21/147:\n",
      "from collections import Counter\n",
      "d = Counter(newords)\n",
      "21/148:\n",
      "s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
      "for k, v in s:\n",
      "    k, v\n",
      "21/149: s\n",
      "21/150: s[15]\n",
      "21/151: s[:15]\n",
      "21/152:\n",
      "import csv\n",
      "csvfile = open('tradewarcount.csv', \"w\")\n",
      "writer = csv.writer(csvfile)\n",
      "writer.writerow(['keyword', 'frequency'])\n",
      "writer.writerows(s)\n",
      "csvfile.close()\n",
      "21/153:\n",
      "import csv\n",
      "csvfile = open('tradewarcount.csv', \"w\", encoding = \"utf-8\")\n",
      "writer = csv.writer(csvfile)\n",
      "writer.writerow(['keyword', 'frequency'])\n",
      "writer.writerows(s)\n",
      "csvfile.close()\n",
      "27/1: s = s[:15]\n",
      "27/2:\n",
      "import sys\n",
      "\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\", encoding=\"utf-8\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8', errors='ignore').read()\n",
      "    tradewars.writelines(file)\n",
      "    \n",
      "tradewars.close()\n",
      "27/3: tradewars\n",
      "27/4:\n",
      "with open('tradewars.txt', encoding='utf-8') as f:\n",
      "    mylist = list(f)\n",
      "27/5:\n",
      "with open('tradewars.txt', encoding='utf=8') as f:\n",
      "    mylist = [line.rstrip('\\n') for line in f]\n",
      "27/6: news = str(mylist)\n",
      "27/7: news = news.replace(\"\\'\", \" \")\n",
      "27/8: news = news.replace(\"，\", \" \")\n",
      "27/9: news = news.replace(\",\", \" \")\n",
      "27/10: news\n",
      "27/11: news = news.replace(\".\\b\", \" \")\n",
      "27/12: news\n",
      "27/13: news = news.replace(\"\\\\\", \" \")\n",
      "27/14: news = news.replace(\"”\", \" \")\n",
      "27/15: news = news.replace(\"“\", \" \")\n",
      "27/16: news = news.replace(\"‘\", \" \")\n",
      "27/17: news = news.replace('\"', \" \")\n",
      "27/18: news = news.replace(\":\", \" \")\n",
      "27/19: news\n",
      "27/20: news = news.replace(\"(\", \" \")\n",
      "27/21: news = news.replace(\")\", \" \")\n",
      "27/22: news = news.replace(\"U.S \", \"U.S.\")\n",
      "27/23: news\n",
      "27/24: words = news.split(' ')\n",
      "27/25:\n",
      "stopwords = ['the', 'a', 'an', 'it', 'that', 'to', 'its', 'of' , 'for']\n",
      "for word in list(words):  # iterating on a copy since removing will mess things up\n",
      "    if word in stopwords:\n",
      "        words.remove(word)\n",
      "27/26: words\n",
      "27/27:\n",
      "delete = ['', 'this', 'on', 'in', 'has', 'that', 'is', 'are', 'they', 'be', '[', ']', 'what', 'and','And', 'I', 'my', 'By', 'by',\n",
      "          'at', 'What', 'The','’', 'which', 'will', 's', 'we', 'so', 'all', 'This', 'who', 'than', 'but', 'with', 'as',\n",
      "          'from', 'also', 'have', 'would', 'not','was','He', 'been','while','or','His', 'his', 'can', 'had','']\n",
      "words[:] = [elem for elem in words if elem not in delete]\n",
      "27/28: newords = [elem for elem in words]\n",
      "27/29: newords\n",
      "27/30:\n",
      "from collections import Counter\n",
      "d = Counter(newords)\n",
      "27/31:\n",
      "s = [(k, d[k]) for k in sorted(d, key=d.get, reverse=True)]\n",
      "for k, v in s:\n",
      "    k, v\n",
      "27/32: s = s[:15]\n",
      "27/33:\n",
      "import csv\n",
      "csvfile = open('tradewarcount.csv', \"w\", encoding = \"utf-8\")\n",
      "writer = csv.writer(csvfile)\n",
      "writer.writerow(['keyword', 'frequency'])\n",
      "writer.writerows(s)\n",
      "csvfile.close()\n",
      "28/1: !pip install bs4\n",
      "28/2: import bs4\n",
      "28/3:\n",
      "import bs4\n",
      "import request\n",
      "r = request.text('https://github.com/hupili/python-for-data-and-media-communication-gitbook/blob/master/session-F2018.md')\n",
      "28/4:\n",
      "import bs4\n",
      "import request\n",
      "r = request.text('https://github.com/hupili/python-for-data-and-media-communication-gitbook/blob/master/session-F2018.md')\n",
      "28/5:\n",
      "import bs4\n",
      "import requests\n",
      "r = request.text('https://github.com/hupili/python-for-data-and-media-communication-gitbook/blob/master/session-F2018.md')\n",
      "28/6:\n",
      "import bs4\n",
      "import requests\n",
      "r = requests.text('https://github.com/hupili/python-for-data-and-media-communication-gitbook/blob/master/session-F2018.md')\n",
      "28/7:\n",
      "import bs4\n",
      "import requests\n",
      "r = requests.get('https://github.com/hupili/python-for-data-and-media-communication-gitbook/blob/master/session-F2018.md')\n",
      "28/8: mypage = bs4.BeautifulSoup(r.text)\n",
      "28/9: myh2 = mypage.find('h2')\n",
      "28/10: myh2.text\n",
      "28/11: myh2.attrs\n",
      "28/12: myh2.find('a').attrs\n",
      "28/13: myh2.find('a').attrs['href']\n",
      "28/14: myh2.find('ul').attrs\n",
      "28/15: myh2 = mypage.find('ul')\n",
      "28/16: myh2.text\n",
      "28/17: myh2.attrs\n",
      "28/18: myh2.find('ul').attrs\n",
      "28/19: myh2 = mypage.find('ul')\n",
      "28/20: myh2.text\n",
      "28/21: myh2.attrs\n",
      "28/22: myh2.find('class').attrs\n",
      "28/23: myh2.attrs['class']\n",
      "28/24: myh2.attrs['class']('d-flex')\n",
      "28/25: myh2.attrs['class']('\n",
      "28/26: myh2.attrs['class']\n",
      "28/27: myh2 = mypage.find('a')\n",
      "28/28: myh2.text\n",
      "28/29: myarticle = mypage.find('article')\n",
      "28/30: myarticle\n",
      "28/31: myul = myarticle.find('ul')\n",
      "28/32: myul\n",
      "28/33: myli = myul.find(\"li\")\n",
      "28/34: myli.text\n",
      "28/35: mya = myul.find(\"a\")\n",
      "28/36: mya.text\n",
      "30/1: r.text\n",
      "30/2: import requests\n",
      "30/3: r = requests.get('https://www.imdb.com/chart/top')\n",
      "30/4: type(r)\n",
      "30/5: r.text\n",
      "30/6: import bs4\n",
      "30/7: mypage = bs4.BeautifulSoup(r.text)\n",
      "30/8: mytable = mypage.find('table')\n",
      "30/9: mytable\n",
      "30/10: myas = mytable.find_all('a')\n",
      "30/11: myas\n",
      "30/12:\n",
      "data = []\n",
      "for mya in myas:\n",
      "    data.append((mya.text,mya.attrs['href']))\n",
      "30/13: data\n",
      "30/14: type(data)\n",
      "30/15: mytable\n",
      "30/16: mytable\n",
      "30/17: myas = mytable.find_all('a')\n",
      "32/1: import requests\n",
      "32/2: r = requests.get('https://www.imdb.com/chart/top')\n",
      "32/3: type(r)\n",
      "32/4: r.text\n",
      "32/5: import bs4\n",
      "32/6: mypage = bs4.BeautifulSoup(r.text)\n",
      "32/7: mytable = mypage.find('table')\n",
      "32/8: myas = mytable.find_all('a')\n",
      "32/9: myas\n",
      "32/10: myas = mytable.find_all('a')\n",
      "32/11:\n",
      "data = []\n",
      "for mya in myas:\n",
      "    data.append((mya.text,mya.attrs['title']))\n",
      "32/12: data\n",
      "32/13: type(myas)\n",
      "32/14: myas\n",
      "32/15: type(myas)\n",
      "32/16: myas\n",
      "32/17: myas.find('a')\n",
      "32/18: myas.find_all('a')\n",
      "32/19: myas.find('a')\n",
      "32/20: myas.find('a').attrs['title']\n",
      "32/21: myas.find_all('a').attrs['title']\n",
      "32/22: myas\n",
      "32/23: myas.attrs['title']\n",
      "32/24: myas.attrs['title']\n",
      "32/25: myas = mytable.find_all('a')\n",
      "32/26: myas\n",
      "32/27: myas.attrs['title']\n",
      "32/28: myas = mytable.find_all('a')\n",
      "32/29: mytable.find('a')\n",
      "32/30: mytable.find('a href')\n",
      "32/31: mytable.find('a href')\n",
      "32/32: mytable.find('a href')\n",
      "32/33: mytable\n",
      "32/34: mytable = mypage.find('table')\n",
      "32/35: mytable\n",
      "32/36: mytable.find('a')\n",
      "32/37: mytable.find('title')\n",
      "32/38: mytable.find('a')\n",
      "33/1: url = 'http://www.imdb.com/chart/top'\n",
      "33/2: response = requests.get(url)\n",
      "33/3:\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "response = requests.get(url)\n",
      "33/4: soup = BeautifulSoup(response.text, 'lxml')\n",
      "33/5:\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "response = requests.get(url)\n",
      "33/6: soup = BeautifulSoup(response.text, 'lxml')\n",
      "33/7:\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "response = requests.get(url)\n",
      "33/8: soup = BeautifulSoup(response.text, 'lxml')\n",
      "33/9: soup = BeautifulSoup(response.text)\n",
      "33/10: BeautifulSoup(response.text)\n",
      "33/11: soup = BeautifulSoup(response.text)\n",
      "33/12: movies = soup.select('td.titleColumn')\n",
      "33/13: soup.select('td.titleColumn')\n",
      "33/14: movies = soup.select('td.titleColumn')\n",
      "33/15: help(soup.select)\n",
      "33/16: crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
      "33/17: a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
      "33/18: a.attrs.get('title') for a in soup.select('td.titleColumn a')\n",
      "33/19: movies = soup.select('td.titleColumn')\n",
      "33/20: a.attrs.get('title') for a in soup.select('td.titleColumn a')\n",
      "33/21:\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "response = requests.get(url)\n",
      "33/22: soup = BeautifulSoup(response.text)\n",
      "33/23: movies = soup.select('td.titleColumn')\n",
      "33/24: a.attrs.get('title') for a in soup.select('td.titleColumn a')\n",
      "33/25: title = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
      "33/26: soup.select('td titleColumn a')\n",
      "33/27: soup.select('td.titleColumn a')\n",
      "33/28: soup.find_all('td.titleColumn a')\n",
      "33/29: soup.find_all('td.titleColumn a')\n",
      "33/30: soup.find_all(\"td\",class = \"titleColumn a')\n",
      "33/31: soup.find_all(\"td\",class_= \"titleColumn a')\n",
      "33/32: soup.find_all(\"td\", class_=\"titleColumn a')\n",
      "33/33: soup.find_all(\"td\", class_=\"titleColumn a')\n",
      "33/34: soup.find_all(\"td\", class_=\"titleColumn a\")\n",
      "33/35: soup.find_all(\"td\", class_=\"titleColumn a\")\n",
      "33/36: soup.find_all(\"td\", class_=\"titleColumn a\").text\n",
      "33/37: soup.find_all(\"td.titleColumn a\")\n",
      "33/38: soup.find_all(\"td.titleColumn a\")\n",
      "33/39: soup.find_all('td.titleColumn a')\n",
      "33/40: soup.select('td.titleColumn a')\n",
      "33/41: soup.find_all(\"td\", class='titleColumn a')\n",
      "34/1: news.split(' ')\n",
      "34/2: news\n",
      "34/3:\n",
      "import sys\n",
      "\n",
      "paths = ['C:/Users/burni/Downloads/trade-wars-news1.txt', 'C:/Users/burni/Downloads/trade-wars-news2.txt', 'C:/Users/burni/Downloads/trade-wars-news3.txt', 'C:/Users/burni/Downloads/trade-wars-news4.txt', 'C:/Users/burni/Downloads/trade-wars-news5.txt']\n",
      "tradewars = open(\"tradewars.txt\", \"a\", encoding=\"utf-8\")\n",
      "for path in paths:\n",
      "    file = open(path, encoding='utf-8', errors='ignore').read()\n",
      "    tradewars.writelines(file)\n",
      "    \n",
      "tradewars.close()\n",
      "34/4: tradewars\n",
      "34/5:\n",
      "with open('tradewars.txt', encoding='utf-8') as f:\n",
      "    mylist = list(f)\n",
      "34/6:\n",
      "with open('tradewars.txt', encoding='utf=8') as f:\n",
      "    mylist = [line.rstrip('\\n') for line in f]\n",
      "34/7: news = str(mylist)\n",
      "34/8: news = news.replace(\"\\'\", \" \")\n",
      "34/9: news = news.replace(\"，\", \" \")\n",
      "34/10: news = news.replace(\",\", \" \")\n",
      "34/11: news\n",
      "34/12: news = news.replace(\".\\b\", \" \")\n",
      "34/13: news\n",
      "34/14: news = news.replace(\"\\\\\", \" \")\n",
      "34/15: news = news.replace(\"”\", \" \")\n",
      "34/16: news = news.replace(\"“\", \" \")\n",
      "34/17: news = news.replace(\"‘\", \" \")\n",
      "34/18: news = news.replace('\"', \" \")\n",
      "34/19: news = news.replace(\":\", \" \")\n",
      "34/20: news\n",
      "34/21: news = news.replace(\"(\", \" \")\n",
      "34/22: news = news.replace(\")\", \" \")\n",
      "34/23: news = news.replace(\"U.S \", \"U.S.\")\n",
      "34/24: news\n",
      "34/25: news.split(' ')\n",
      "34/26: news.replace(\"\\'\", \" \")\n",
      "34/27: news = str(mylist)\n",
      "34/28: str(mylist)\n",
      "34/29: news = str(mylist)\n",
      "37/1: pip install --upgrade google-api-python-client\n",
      "37/2: $ pip install --upgrade google-api-python-client\n",
      "37/3: !pip install --upgrade google-api-python-client\n",
      "37/4: !pip install --upgrade google-api-python-client\n",
      "37/5: pip install --upgrade google-auth google-auth-oauthlib google-auth-httplib2\n",
      "37/6: !pip install --upgrade google-auth google-auth-oauthlib google-auth-httplib2\n",
      "37/7: !python quickstart.py\n",
      "37/8: !python quickstart.py\n",
      "38/1: !python quickstart.py\n",
      "39/1: !python quickstart.py\n",
      "39/2: !pip install --upgrade flask\n",
      "39/3: !pip install --upgrade requests\n",
      "39/4: !python quickstart.py\n",
      "40/1: !pip install --upgrade google-api-python-client\n",
      "40/2: !pip install --upgrade google-auth google-auth-oauthlib google-auth-httplib2\n",
      "40/3: !pip install --upgrade flask\n",
      "40/4: !pip install --upgrade requests\n",
      "40/5:\n",
      "from apiclient.discovery import build #pip install google-api-python-client\n",
      "from apiclient.errors import HttpError #pip install google-api-python-client\n",
      "from oauth2client.tools import argparser #pip install oauth2client\n",
      "import pandas as pd #pip install pandas\n",
      "40/6: !pip install --upgrade google-api-python-client\n",
      "40/7: !pip install --upgrade google-auth google-auth-oauthlib google-auth-httplib2\n",
      "40/8: !pip install --upgrade flask\n",
      "40/9: !pip install --upgrade requests\n",
      "40/10:\n",
      "from apiclient.discovery import build #pip install google-api-python-client\n",
      "from apiclient.errors import HttpError #pip install google-api-python-client\n",
      "from oauth2client.tools import argparser #pip install oauth2client\n",
      "import pandas as pd #pip install pandas\n",
      "40/11: !pip install oauth2client\n",
      "40/12:\n",
      "from apiclient.discovery import build #pip install google-api-python-client\n",
      "from apiclient.errors import HttpError #pip install google-api-python-client\n",
      "from oauth2client.tools import argparser #pip install oauth2client\n",
      "import pandas as pd #pip install pandas\n",
      "40/13:\n",
      "!pip install oauth2client\n",
      "!pip install pandas\n",
      "40/14:\n",
      "from apiclient.discovery import build #pip install google-api-python-client\n",
      "from apiclient.errors import HttpError #pip install google-api-python-client\n",
      "from oauth2client.tools import argparser #pip install oauth2client\n",
      "import pandas as pd #pip install pandas\n",
      "40/15:\n",
      "from apiclient.discovery import build #pip install google-api-python-client\n",
      "from apiclient.errors import HttpError #pip install google-api-python-client\n",
      "from oauth2client.tools import argparser #pip install oauth2client\n",
      "import pandas as pd #pip install pandas\n",
      "40/16:\n",
      "DEVELOPER_KEY = \"1JUEz7xYuF7ETU5IuIn-4tCE\" \n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "40/17:\n",
      "argparser.add_argument(\"--q\", help=\"Search term\", default=\"kpop\")\n",
      "#change the default to the search term you want to search\n",
      "argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "options = args\n",
      "40/18: youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)\n",
      "40/19:\n",
      "# Call the search.list method to retrieve results matching the specified\n",
      " # query term.\n",
      "search_response = youtube.search().list(\n",
      " q=options.q,\n",
      " type=\"video\",\n",
      " part=\"id,snippet\",\n",
      " maxResults=options.max_results\n",
      ").execute()\n",
      "40/20:\n",
      "argparser.add_argument(\"--q\", help=\"Search term\", default=\"kpop\")\n",
      "#change the default to the search term you want to search\n",
      "argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "options = args\n",
      "40/21:\n",
      "argparser.add_argument(\"--q\", help=\"Search term\", default=\"kpop\")\n",
      "#change the default to the search term you want to search\n",
      "argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "options = args\n",
      "40/22:\n",
      "DEVELOPER_KEY = \"1JUEz7xYuF7ETU5IuIn-4tCE\" \n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "40/23:\n",
      "argparser.add_argument(\"--q\", help=\"Search term\", default=\"kpop\")\n",
      "#change the default to the search term you want to search\n",
      "argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "options = args\n",
      "40/24:\n",
      "parser.add_argument(\"--q\", help=\"Search term\", default=\"kpop\")\n",
      "#change the default to the search term you want to search\n",
      "parser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "options = args\n",
      "40/25:\n",
      "argparser.add_argument(\"--q\", help=\"Search term\", default=\"kpop\")\n",
      "#change the default to the search term you want to search\n",
      "argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "options = args\n",
      "40/26:\n",
      "if __name__ == \"__main__\":\n",
      "  argparser = argparse.ArgumentParser(conflict_handler='resolve')\n",
      "  argparser.add_argument(\"--q\", help=\"Search term\", default=\"kpop\")\n",
      "  argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "  args = argparser.parse_args()\n",
      "\n",
      "options = args\n",
      "40/27:\n",
      "from apiclient.discovery import build #pip install google-api-python-client\n",
      "from apiclient.errors import HttpError #pip install google-api-python-client\n",
      "from oauth2client.tools import argparser #pip install oauth2client\n",
      "import pandas as pd #pip install pandas\n",
      "import argparse\n",
      "40/28:\n",
      "DEVELOPER_KEY = \"1JUEz7xYuF7ETU5IuIn-4tCE\" \n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "40/29:\n",
      "if __name__ == \"__main__\":\n",
      "  argparser = argparse.ArgumentParser(conflict_handler='resolve')\n",
      "  argparser.add_argument(\"--q\", help=\"Search term\", default=\"kpop\")\n",
      "  argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "  args = argparser.parse_args()\n",
      "\n",
      "options = args\n",
      "40/30: youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)\n",
      "40/31:\n",
      "if __name__ == \"__main__\":\n",
      "  argparser = argparse.ArgumentParser(conflict_handler='resolve')\n",
      "  argparser.add_argument(\"--q\", help=\"Search term\", default=\"kpop\")\n",
      "  argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "  args = argparser.parse_args()\n",
      "\n",
      "options = args\n",
      "40/32: youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)\n",
      "40/33:\n",
      "if __name__ == \"__main__\":\n",
      "  argpars = argparse.ArgumentParser(conflict_handler='resolve')\n",
      "  argpars.add_argument(\"--q\", help=\"Search term\", default=\"kpop\")\n",
      "  argpars.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "  args = argpars.parse_args()\n",
      "\n",
      "options = args\n",
      "40/34:\n",
      "!pip3 uninstall -y rq-scheduler\n",
      "!pip3 install rq-scheduler==0.7.0\n",
      "40/35:\n",
      "from apiclient.discovery import build #pip install google-api-python-client\n",
      "from apiclient.errors import HttpError #pip install google-api-python-client\n",
      "from oauth2client.tools import argparser #pip install oauth2client\n",
      "import pandas as pd #pip install pandas\n",
      "import argparse\n",
      "40/36:\n",
      "DEVELOPER_KEY = \"1JUEz7xYuF7ETU5IuIn-4tCE\" \n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "40/37:\n",
      "argparser.add_argument(\"--q\", help=\"Search term\", default=\"ALS Ice Bucket Challenge\")\n",
      "#change the default to the search term you want to search\n",
      "argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "\n",
      "options = args\n",
      "40/38:\n",
      "argparse.add_argument(\"--q\", help=\"Search term\", default=\"ALS Ice Bucket Challenge\")\n",
      "#change the default to the search term you want to search\n",
      "argparse.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparse.parse_args()\n",
      "\n",
      "options = args\n",
      "40/39:\n",
      "argparser.add_argument(\"--q\", help=\"Search term\", default=\"ALS Ice Bucket Challenge\")\n",
      "#change the default to the search term you want to search\n",
      "argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "\n",
      "options = args\n",
      "40/40: help(argparser)\n",
      "40/41:\n",
      "parser = argparse.ArgumentParser(add_help=False)\n",
      "argparser.add_argument(\"--q\", help=\"Search term\", default=\"ALS Ice Bucket Challenge\")\n",
      "#change the default to the search term you want to search\n",
      "argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "\n",
      "options = args\n",
      "40/42:\n",
      "parser = argparser.ArgumentParser(add_help=False)\n",
      "argparser.add_argument(\"--q\", help=\"Search term\", default=\"ALS Ice Bucket Challenge\")\n",
      "#change the default to the search term you want to search\n",
      "argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "\n",
      "options = args\n",
      "40/43:\n",
      "parser = argparse.ArgumentParser(add_help=False)\n",
      "argparser.add_argument(\"--q\", help=\"Search term\", default=\"ALS Ice Bucket Challenge\")\n",
      "#change the default to the search term you want to search\n",
      "argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "\n",
      "options = args\n",
      "40/44:\n",
      "parser = argparse.ArgumentParser(add_help=False)\n",
      "argparse.add_argument(\"--q\", help=\"Search term\", default=\"ALS Ice Bucket Challenge\")\n",
      "#change the default to the search term you want to search\n",
      "argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "\n",
      "options = args\n",
      "40/45:\n",
      "parser = argparse.ArgumentParser(add_help=False)\n",
      "argparser.add_argument(\"--q\", help=\"Search term\", default=\"ALS Ice Bucket Challenge\")\n",
      "#change the default to the search term you want to search\n",
      "argparser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "\n",
      "options = args\n",
      "40/46:\n",
      "parser = argparse.ArgumentParser(add_help=False)\n",
      "parser.add_argument(\"--q\", help=\"Search term\", default=\"ALS Ice Bucket Challenge\")\n",
      "#change the default to the search term you want to search\n",
      "parser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "\n",
      "options = args\n",
      "40/47:\n",
      "parser = argparse.ArgumentParser(add_help=False)\n",
      "parser.add_argument(\"--q\", help=\"Search term\", default=\"ALS Ice Bucket Challenge\")\n",
      "#change the default to the search term you want to search\n",
      "parser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "opt=parser.parse_args(args=[])#添加args=[]\n",
      "\n",
      "options = args\n",
      "40/48:\n",
      "parser = argparse.ArgumentParser(add_help=False)\n",
      "parser.add_argument(\"--q\", help=\"Search term\", default=\"ALS Ice Bucket Challenge\")\n",
      "#change the default to the search term you want to search\n",
      "parser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "opt=parser.parse_args(args=[])#添加args=[]\n",
      "\n",
      "options = args\n",
      "40/49:\n",
      "parser = argparse.ArgumentParser(add_help=False)\n",
      "parser.add_argument(\"--term\", help=\"Search term\", default=\"ALS Ice Bucket Challenge\")\n",
      "#change the default to the search term you want to search\n",
      "parser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "opt=parser.parse_args(args=[])#添加args=[]\n",
      "\n",
      "options = args\n",
      "40/50:\n",
      "parser = argparse.ArgumentParser()\n",
      "parser.add_argument(\"--term\", help=\"Search term\", default=\"ALS Ice Bucket Challenge\")\n",
      "#change the default to the search term you want to search\n",
      "parser.add_argument(\"--max-results\", help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "opt=parser.parse_args(args=[])#添加args=[]\n",
      "\n",
      "options = args\n",
      "40/51:\n",
      "parser = argparse.ArgumentParser()\n",
      "parser.add_argument(\"--search\", type=str, help=\"Search term\", default=\"ALS Ice Bucket Challenge\")\n",
      "#change the default to the search term you want to search\n",
      "parser.add_argument(\"--max-results\", type=int, help=\"Max results\", default=25)\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "opt=parser.parse_args(args=[])#添加args=[]\n",
      "\n",
      "options = args\n",
      "40/52:\n",
      "parser = argparse.ArgumentParser()\n",
      "parser.add_argument(\"--search\", type=str, default=\"ALS Ice Bucket Challenge\", help=\"Search term\")\n",
      "#change the default to the search term you want to search\n",
      "parser.add_argument(\"--max-results\", type=int, default=25, help=\"Max results\" )\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "opt=parser.parse_args(args=[])#添加args=[]\n",
      "\n",
      "options = args\n",
      "40/53:\n",
      "parser = argparse.ArgumentParser()\n",
      "parser.add_argument(\"--search\", type=str, default=\"ALS Ice Bucket Challenge\", help=\"Search term\")\n",
      "#change the default to the search term you want to search\n",
      "parser.add_argument(\"--max_results\", type=int, default=25, help=\"Max results\" )\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "opt=parser.parse_args(args=[])#添加args=[]\n",
      "\n",
      "options = args\n",
      "40/54:\n",
      "parser = argparse.ArgumentParser()\n",
      "parser.add_argument(\"--search\", type=str, default=\"ALS Ice Bucket Challenge\", help=\"Search term\")\n",
      "#change the default to the search term you want to search\n",
      "parser.add_argument(\"--max_outcome\", type=int, default=25, help=\"Max results\" )\n",
      "#default number of results which are returned. It can vary from 0 - 100\n",
      "args = argparser.parse_args()\n",
      "opt=parser.parse_args(args=[])#添加args=[]\n",
      "\n",
      "options = args\n",
      "40/55:\n",
      "parser = argparse.ArgumentParser()\n",
      "args = parser.parse_args(argv[1:])\n",
      "args = easydict.EasyDict({\n",
      "    \"search\": kpop,\n",
      "    \"max_results\": 25\n",
      "})\n",
      "\n",
      "options = args\n",
      "40/56:\n",
      "!pip install oauth2client\n",
      "!pip install pandas\n",
      "!pip install easydict\n",
      "40/57:\n",
      "import easydict\n",
      "parser = argparse.ArgumentParser()\n",
      "args = parser.parse_args(argv[1:])\n",
      "args = easydict.EasyDict({\n",
      "    \"search\": kpop,\n",
      "    \"max_results\": 25\n",
      "})\n",
      "\n",
      "options = args\n",
      "40/58:\n",
      "import easydict\n",
      "parser = argparse.ArgumentParser()\n",
      "\n",
      "args = easydict.EasyDict({\n",
      "    \"search\": kpop,\n",
      "    \"max_results\": 25\n",
      "})\n",
      "\n",
      "options = args\n",
      "40/59:\n",
      "import easydict\n",
      "parser = argparse.ArgumentParser()\n",
      "\n",
      "args = easydict.EasyDict({\n",
      "    \"search\": kpop\n",
      "    \"max_results\": 25\n",
      "})\n",
      "\n",
      "options = args\n",
      "40/60:\n",
      "import easydict\n",
      "parser = argparse.ArgumentParser()\n",
      "\n",
      "args = easydict.EasyDict({\n",
      "    \"search\": kpop\n",
      "    \"max_results\": 25\n",
      "})\n",
      "\n",
      "options = args\n",
      "40/61: youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)\n",
      "40/62:\n",
      "import easydict\n",
      "parser = argparse.ArgumentParser()\n",
      "\n",
      "args = easydict.EasyDict({\n",
      "    \"search\": kpop\n",
      "    \"max_results\": 25\n",
      "})\n",
      "\n",
      "options = args\n",
      "40/63:\n",
      "import easydict\n",
      "parser = argparse.ArgumentParser()\n",
      "\n",
      "args = easydict.EasyDict({\n",
      "    \"search\": \"kpop\",\n",
      "    \"max_results\": 25\n",
      "})\n",
      "\n",
      "options = args\n",
      "40/64: youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)\n",
      "40/65:\n",
      "# Call the search.list method to retrieve results matching the specified\n",
      " # query term.\n",
      "search_response = youtube.search().list(\n",
      " q=options.q,\n",
      " type=\"video\",\n",
      " part=\"id,snippet\",\n",
      " maxResults=options.max_results\n",
      ").execute()\n",
      "40/66:\n",
      "# Call the search.list method to retrieve results matching the specified\n",
      " # query term.\n",
      "search_response = youtube.search().list(\n",
      " search=options.search,\n",
      " type=\"video\",\n",
      " part=\"id,snippet\",\n",
      " maxResults=options.max_results\n",
      ").execute()\n",
      "40/67:\n",
      "import easydict\n",
      "parser = argparse.ArgumentParser()\n",
      "\n",
      "args = easydict.EasyDict({\n",
      "    \"search\": \"kpop\",\n",
      "    \"max_results\": 25\n",
      "})\n",
      "\n",
      "options = args\n",
      "40/68: youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)\n",
      "40/69:\n",
      "# Call the search.list method to retrieve results matching the specified\n",
      " # query term.\n",
      "search_response = youtube.search().list(\n",
      " search=options.search,\n",
      " type=\"video\",\n",
      " part=\"id,snippet\",\n",
      " maxResults=options.max_results\n",
      ").execute()\n",
      "40/70:\n",
      "DEVELOPER_KEY = \"paste your API key here\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "title = []\n",
      "channelId = []\n",
      "channelTitle = []\n",
      "categoryId = []\n",
      "videoId = []\n",
      "viewCount = []\n",
      "likeCount = []\n",
      "dislikeCount = []\n",
      "commentCount = []\n",
      "favoriteCount = []\n",
      "category = []\n",
      "tags = []\n",
      "videos = []\n",
      "tags = []\n",
      "max_search = 50\n",
      "order = \"relevance\"\n",
      "token = None\n",
      "location = None\n",
      "location_radius = None\n",
      "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "developerKey=DEVELOPER_KEY)\n",
      "search_result = youtube.search().list(q=\"put the channel name here\", type=\"video\",     \n",
      "pageToken=token, order=order, part=\"id,snippet\",\n",
      "                                  maxResults=max_search, location=location,\n",
      "                                  locationRadius=location_radius, channelId='put the     \n",
      "channel ID here').execute()  # this line is to get the videos of the channel by the \n",
      "name of it\n",
      "for search_result in search_result.get(\"items\", []):\n",
      "if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "    title.append(search_result['snippet']['title'])  # the title of the video\n",
      "    videoId.append(search_result['id']['videoId'])  # the ID of the video\n",
      "    response = youtube.videos().list(part='statistics, snippet',     \n",
      "id=search_result['id'][\n",
      "        'videoId']).execute()  # this is the other request because the statistics and     \n",
      "snippet require this because of the API\n",
      "    channelId.append(response['items'][0]['snippet']['channelId'])  # channel ID, \n",
      "which is constant here\n",
      "    channelTitle.append(response['items'][0]['snippet']['channelTitle'])    # channel \n",
      "title, also constant\n",
      "    categoryId.append(response['items'][0]['snippet']['categoryId'])        # stores \n",
      "the categories of the videos\n",
      "    favoriteCount.append(response['items'][0]['statistics']['favoriteCount'])   # \n",
      "stores the favourite count of the videos\n",
      "    viewCount.append(response['items'][0]['statistics']['viewCount'])           # \n",
      "stores the view counts\n",
      "    \"\"\"  \n",
      "\n",
      "    the likes and dislikes had a bug all along, which required the if else instead of \n",
      "just behaving like the viewCount\"\"\"\n",
      "    if 'likeCount' in response['items'][0]['statistics'].keys():             # checks \n",
      "for likes count then restores it, if no likes stores 0\n",
      "        likeCount.append(response['items'][0]['statistics']['likeCount'])\n",
      "    else:\n",
      "        likeCount.append('0')\n",
      "    if 'dislikeCount' in response['items'][0]['statistics'].keys():             # checks for dislikes count then stores it, if no dislikes found returns 0\n",
      "        dislikeCount.append(response['items'][0]['statistics']['dislikeCount'])\n",
      "    else:\n",
      "        likeCount.append('0')\n",
      "    if 'commentCount' in response['items'][0]['statistics'].keys():             # checks for comment count then stores it, if no comment found stores 0\n",
      "        commentCount.append(response['items'][0]['statistics']['commentCount'])\n",
      "    else:\n",
      "        commentCount.append('0')\n",
      "    if 'tags' in response['items'][0]['snippet'].keys():                        # checks for tags count then stores it, if none found stores 0\n",
      "        tags.append(response['items'][0]['snippet']['tags'])\n",
      "    else:\n",
      "        tags.append('0')\n",
      "youtube_dict = {\n",
      "'tags': tags,\n",
      "'channelId': channelId,\n",
      "'channelTitle': channelTitle,\n",
      "'categoryId': categoryId,\n",
      "'title': title,\n",
      "'videoId': videoId,\n",
      "'viewCount': viewCount,\n",
      "'likeCount': likeCount,\n",
      "'dislikeCount': dislikeCount,\n",
      "'commentCount': commentCount,\n",
      "'favoriteCount': favoriteCount\n",
      "}\n",
      "for x in youtube_dict:\n",
      "print(x)\n",
      "for y in youtube_dict[x]:\n",
      "    print(y)\n",
      "40/72:\n",
      "DEVELOPER_KEY = \"paste your API key here\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "title = []\n",
      "channelId = []\n",
      "channelTitle = []\n",
      "categoryId = []\n",
      "videoId = []\n",
      "viewCount = []\n",
      "likeCount = []\n",
      "dislikeCount = []\n",
      "commentCount = []\n",
      "favoriteCount = []\n",
      "category = []\n",
      "tags = []\n",
      "videos = []\n",
      "tags = []\n",
      "max_search = 50\n",
      "order = \"relevance\"\n",
      "token = None\n",
      "location = None\n",
      "location_radius = None\n",
      "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "developerKey=DEVELOPER_KEY)\n",
      "search_result = youtube.search().list(q=\"put the channel name here\", type=\"video\",     \n",
      "                                      pageToken=token, order=order, part=\"id,snippet\", maxResults=max_search, location=location,\n",
      "                                      locationRadius=location_radius, channelId='put the channel ID here').execute()\n",
      "#this line is to get the videos of the channel by the name of it\n",
      "for search_result in search_result.get(\"items\", []):\n",
      "if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "    title.append(search_result['snippet']['title']) \n",
      "    # the title of the video\n",
      "    videoId.append(search_result['id']['videoId']) \n",
      "    # the ID of the video\n",
      "    response = youtube.videos().list(part='statistics, snippet', id=search_result['id']['videoId']).execute() \n",
      "    # this is the other request because the statistics and snippet require this because of the API    channelId.append(response['items'][0]['snippet']['channelId'])  # channel ID, which is constant here\n",
      "    channelTitle.append(response['items'][0]['snippet']['channelTitle'])    \n",
      "    # channel title, also constant\n",
      "    categoryId.append(response['items'][0]['snippet']['categoryId'])        \n",
      "    # stores the categories of the videos\n",
      "    favoriteCount.append(response['items'][0]['statistics']['favoriteCount'])   \n",
      "    # stores the favourite count of the videos\n",
      "    viewCount.append(response['items'][0]['statistics']['viewCount'])           \n",
      "    # stores the view counts\n",
      "\n",
      "    if 'likeCount' in response['items'][0]['statistics'].keys():             # checks for likes count then restores it, if no likes stores 0\n",
      "        likeCount.append(response['items'][0]['statistics']['likeCount'])\n",
      "    else:\n",
      "        likeCount.append('0')\n",
      "    if 'dislikeCount' in response['items'][0]['statistics'].keys():             # checks for dislikes count then stores it, if no dislikes found returns 0\n",
      "        dislikeCount.append(response['items'][0]['statistics']['dislikeCount'])\n",
      "    else:\n",
      "        likeCount.append('0')\n",
      "    if 'commentCount' in response['items'][0]['statistics'].keys():             # checks for comment count then stores it, if no comment found stores 0\n",
      "        commentCount.append(response['items'][0]['statistics']['commentCount'])\n",
      "    else:\n",
      "        commentCount.append('0')\n",
      "    if 'tags' in response['items'][0]['snippet'].keys():                        # checks for tags count then stores it, if none found stores 0\n",
      "        tags.append(response['items'][0]['snippet']['tags'])\n",
      "    else:\n",
      "        tags.append('0')\n",
      "youtube_dict = {\n",
      "'tags': tags,\n",
      "'channelId': channelId,\n",
      "'channelTitle': channelTitle,\n",
      "'categoryId': categoryId,\n",
      "'title': title,\n",
      "'videoId': videoId,\n",
      "'viewCount': viewCount,\n",
      "'likeCount': likeCount,\n",
      "'dislikeCount': dislikeCount,\n",
      "'commentCount': commentCount,\n",
      "'favoriteCount': favoriteCount\n",
      "}\n",
      "for x in youtube_dict:\n",
      "    print(x)\n",
      "for y in youtube_dict[x]:\n",
      "    print(y)\n",
      "40/73:\n",
      "DEVELOPER_KEY = \"1JUEz7xYuF7ETU5IuIn-4tCE\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "title = []\n",
      "channelId = []\n",
      "channelTitle = []\n",
      "categoryId = []\n",
      "videoId = []\n",
      "viewCount = []\n",
      "likeCount = []\n",
      "dislikeCount = []\n",
      "commentCount = []\n",
      "favoriteCount = []\n",
      "category = []\n",
      "tags = []\n",
      "videos = []\n",
      "tags = []\n",
      "max_search = 50\n",
      "order = \"relevance\"\n",
      "token = None\n",
      "location = None\n",
      "location_radius = None\n",
      "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "developerKey=DEVELOPER_KEY)\n",
      "search_result = youtube.search().list(q=\"SMTOWN, type=\"video\",     \n",
      "                                      pageToken=token, order=order, part=\"id,snippet\", maxResults=max_search, location=location,\n",
      "                                      locationRadius=location_radius, channelId='UCEf_Bc-KVd7onSeifS3py9g').execute()\n",
      "#this line is to get the videos of the channel by the name of it\n",
      "for search_result in search_result.get(\"items\", []):\n",
      "if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "    title.append(search_result['snippet']['title']) \n",
      "    # the title of the video\n",
      "    videoId.append(search_result['id']['videoId']) \n",
      "    # the ID of the video\n",
      "    response = youtube.videos().list(part='statistics, snippet', id=search_result['id']['videoId']).execute() \n",
      "    # this is the other request because the statistics and snippet require this because of the API    channelId.append(response['items'][0]['snippet']['channelId'])  # channel ID, which is constant here\n",
      "    channelTitle.append(response['items'][0]['snippet']['channelTitle'])    \n",
      "    # channel title, also constant\n",
      "    categoryId.append(response['items'][0]['snippet']['categoryId'])        \n",
      "    # stores the categories of the videos\n",
      "    favoriteCount.append(response['items'][0]['statistics']['favoriteCount'])   \n",
      "    # stores the favourite count of the videos\n",
      "    viewCount.append(response['items'][0]['statistics']['viewCount'])           \n",
      "    # stores the view counts\n",
      "\n",
      "    if 'likeCount' in response['items'][0]['statistics'].keys():             # checks for likes count then restores it, if no likes stores 0\n",
      "        likeCount.append(response['items'][0]['statistics']['likeCount'])\n",
      "    else:\n",
      "        likeCount.append('0')\n",
      "    if 'dislikeCount' in response['items'][0]['statistics'].keys():             # checks for dislikes count then stores it, if no dislikes found returns 0\n",
      "        dislikeCount.append(response['items'][0]['statistics']['dislikeCount'])\n",
      "    else:\n",
      "        likeCount.append('0')\n",
      "    if 'commentCount' in response['items'][0]['statistics'].keys():             # checks for comment count then stores it, if no comment found stores 0\n",
      "        commentCount.append(response['items'][0]['statistics']['commentCount'])\n",
      "    else:\n",
      "        commentCount.append('0')\n",
      "    if 'tags' in response['items'][0]['snippet'].keys():                        # checks for tags count then stores it, if none found stores 0\n",
      "        tags.append(response['items'][0]['snippet']['tags'])\n",
      "    else:\n",
      "        tags.append('0')\n",
      "youtube_dict = {\n",
      "'tags': tags,\n",
      "'channelId': channelId,\n",
      "'channelTitle': channelTitle,\n",
      "'categoryId': categoryId,\n",
      "'title': title,\n",
      "'videoId': videoId,\n",
      "'viewCount': viewCount,\n",
      "'likeCount': likeCount,\n",
      "'dislikeCount': dislikeCount,\n",
      "'commentCount': commentCount,\n",
      "'favoriteCount': favoriteCount\n",
      "}\n",
      "for x in youtube_dict:\n",
      "    print(x)\n",
      "for y in youtube_dict[x]:\n",
      "    print(y)\n",
      "40/74:\n",
      "DEVELOPER_KEY = \"1JUEz7xYuF7ETU5IuIn-4tCE\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "title = []\n",
      "channelId = []\n",
      "channelTitle = []\n",
      "categoryId = []\n",
      "videoId = []\n",
      "viewCount = []\n",
      "likeCount = []\n",
      "dislikeCount = []\n",
      "commentCount = []\n",
      "favoriteCount = []\n",
      "category = []\n",
      "tags = []\n",
      "videos = []\n",
      "tags = []\n",
      "max_search = 50\n",
      "order = \"relevance\"\n",
      "token = None\n",
      "location = None\n",
      "location_radius = None\n",
      "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "developerKey=DEVELOPER_KEY)\n",
      "search_result = youtube.search().list(q=\"SMTOWN, type=\"video\",     \n",
      "                                      pageToken=token, order=order, part=\"id,snippet\", maxResults=max_search, location=location,\n",
      "                                      locationRadius=location_radius, channelId='UCEf_Bc-KVd7onSeifS3py9g').execute()\n",
      "#this line is to get the videos of the channel by the name of it\n",
      "for search_result in search_result.get(\"items\", []):\n",
      "if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "    title.append(search_result['snippet']['title']) \n",
      "    # the title of the video\n",
      "    videoId.append(search_result['id']['videoId']) \n",
      "    # the ID of the video\n",
      "    response = youtube.videos().list(part='statistics, snippet', id=search_result['id']['videoId']).execute() \n",
      "    # this is the other request because the statistics and snippet require this because of the API    channelId.append(response['items'][0]['snippet']['channelId'])  # channel ID, which is constant here\n",
      "    channelTitle.append(response['items'][0]['snippet']['channelTitle'])    \n",
      "    # channel title, also constant\n",
      "    categoryId.append(response['items'][0]['snippet']['categoryId'])        \n",
      "    # stores the categories of the videos\n",
      "    favoriteCount.append(response['items'][0]['statistics']['favoriteCount'])   \n",
      "    # stores the favourite count of the videos\n",
      "    viewCount.append(response['items'][0]['statistics']['viewCount'])           \n",
      "    # stores the view counts\n",
      "\n",
      "    if 'likeCount' in response['items'][0]['statistics'].keys():             # checks for likes count then restores it, if no likes stores 0\n",
      "        likeCount.append(response['items'][0]['statistics']['likeCount'])\n",
      "    else:\n",
      "        likeCount.append('0')\n",
      "    if 'dislikeCount' in response['items'][0]['statistics'].keys():             # checks for dislikes count then stores it, if no dislikes found returns 0\n",
      "        dislikeCount.append(response['items'][0]['statistics']['dislikeCount'])\n",
      "    else:\n",
      "        likeCount.append('0')\n",
      "    if 'commentCount' in response['items'][0]['statistics'].keys():             # checks for comment count then stores it, if no comment found stores 0\n",
      "        commentCount.append(response['items'][0]['statistics']['commentCount'])\n",
      "    else:\n",
      "        commentCount.append('0')\n",
      "    if 'tags' in response['items'][0]['snippet'].keys():                        # checks for tags count then stores it, if none found stores 0\n",
      "        tags.append(response['items'][0]['snippet']['tags'])\n",
      "    else:\n",
      "        tags.append('0')\n",
      "youtube_dict = {\n",
      "'tags': tags,\n",
      "'channelId': channelId,\n",
      "'channelTitle': channelTitle,\n",
      "'categoryId': categoryId,\n",
      "'title': title,\n",
      "'videoId': videoId,\n",
      "'viewCount': viewCount,\n",
      "'likeCount': likeCount,\n",
      "'dislikeCount': dislikeCount,\n",
      "'commentCount': commentCount,\n",
      "'favoriteCount': favoriteCount\n",
      "}\n",
      "for x in youtube_dict:\n",
      "    print(x)\n",
      "for y in youtube_dict[x]:\n",
      "    print(y)\n",
      "40/75:\n",
      "DEVELOPER_KEY = \"1JUEz7xYuF7ETU5IuIn-4tCE\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "title = []\n",
      "channelId = []\n",
      "channelTitle = []\n",
      "categoryId = []\n",
      "videoId = []\n",
      "viewCount = []\n",
      "likeCount = []\n",
      "dislikeCount = []\n",
      "commentCount = []\n",
      "favoriteCount = []\n",
      "category = []\n",
      "tags = []\n",
      "videos = []\n",
      "tags = []\n",
      "max_search = 50\n",
      "order = \"relevance\"\n",
      "token = None\n",
      "location = None\n",
      "location_radius = None\n",
      "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "developerKey=DEVELOPER_KEY)\n",
      "search_result = youtube.search().list(q=\"SMTOWN“, type=\"video\",     \n",
      "                                      pageToken=token, order=order, part=\"id,snippet\", maxResults=max_search, location=location,\n",
      "                                      locationRadius=location_radius, channelId='UCEf_Bc-KVd7onSeifS3py9g').execute()\n",
      "#this line is to get the videos of the channel by the name of it\n",
      "for search_result in search_result.get(\"items\", []):\n",
      "if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "    title.append(search_result['snippet']['title']) \n",
      "    # the title of the video\n",
      "    videoId.append(search_result['id']['videoId']) \n",
      "    # the ID of the video\n",
      "    response = youtube.videos().list(part='statistics, snippet', id=search_result['id']['videoId']).execute() \n",
      "    # this is the other request because the statistics and snippet require this because of the API    channelId.append(response['items'][0]['snippet']['channelId'])  # channel ID, which is constant here\n",
      "    channelTitle.append(response['items'][0]['snippet']['channelTitle'])    \n",
      "    # channel title, also constant\n",
      "    categoryId.append(response['items'][0]['snippet']['categoryId'])        \n",
      "    # stores the categories of the videos\n",
      "    favoriteCount.append(response['items'][0]['statistics']['favoriteCount'])   \n",
      "    # stores the favourite count of the videos\n",
      "    viewCount.append(response['items'][0]['statistics']['viewCount'])           \n",
      "    # stores the view counts\n",
      "\n",
      "    if 'likeCount' in response['items'][0]['statistics'].keys():             # checks for likes count then restores it, if no likes stores 0\n",
      "        likeCount.append(response['items'][0]['statistics']['likeCount'])\n",
      "    else:\n",
      "        likeCount.append('0')\n",
      "    if 'dislikeCount' in response['items'][0]['statistics'].keys():             # checks for dislikes count then stores it, if no dislikes found returns 0\n",
      "        dislikeCount.append(response['items'][0]['statistics']['dislikeCount'])\n",
      "    else:\n",
      "        likeCount.append('0')\n",
      "    if 'commentCount' in response['items'][0]['statistics'].keys():             # checks for comment count then stores it, if no comment found stores 0\n",
      "        commentCount.append(response['items'][0]['statistics']['commentCount'])\n",
      "    else:\n",
      "        commentCount.append('0')\n",
      "    if 'tags' in response['items'][0]['snippet'].keys():                        # checks for tags count then stores it, if none found stores 0\n",
      "        tags.append(response['items'][0]['snippet']['tags'])\n",
      "    else:\n",
      "        tags.append('0')\n",
      "youtube_dict = {\n",
      "'tags': tags,\n",
      "'channelId': channelId,\n",
      "'channelTitle': channelTitle,\n",
      "'categoryId': categoryId,\n",
      "'title': title,\n",
      "'videoId': videoId,\n",
      "'viewCount': viewCount,\n",
      "'likeCount': likeCount,\n",
      "'dislikeCount': dislikeCount,\n",
      "'commentCount': commentCount,\n",
      "'favoriteCount': favoriteCount\n",
      "}\n",
      "for x in youtube_dict:\n",
      "    print(x)\n",
      "for y in youtube_dict[x]:\n",
      "    print(y)\n",
      "40/76:\n",
      "DEVELOPER_KEY = \"1JUEz7xYuF7ETU5IuIn-4tCE\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "title = []\n",
      "channelId = []\n",
      "channelTitle = []\n",
      "categoryId = []\n",
      "videoId = []\n",
      "viewCount = []\n",
      "likeCount = []\n",
      "dislikeCount = []\n",
      "commentCount = []\n",
      "favoriteCount = []\n",
      "category = []\n",
      "tags = []\n",
      "videos = []\n",
      "tags = []\n",
      "max_search = 50\n",
      "order = \"relevance\"\n",
      "token = None\n",
      "location = None\n",
      "location_radius = None\n",
      "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "developerKey=DEVELOPER_KEY)\n",
      "search_result = youtube.search().list(q=\"SMTOWN\", type=\"video\",     \n",
      "                                      pageToken=token, order=order, part=\"id,snippet\", maxResults=max_search, location=location,\n",
      "                                      locationRadius=location_radius, channelId='UCEf_Bc-KVd7onSeifS3py9g').execute()\n",
      "#this line is to get the videos of the channel by the name of it\n",
      "for search_result in search_result.get(\"items\", []):\n",
      "if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "    title.append(search_result['snippet']['title']) \n",
      "    # the title of the video\n",
      "    videoId.append(search_result['id']['videoId']) \n",
      "    # the ID of the video\n",
      "    response = youtube.videos().list(part='statistics, snippet', id=search_result['id']['videoId']).execute() \n",
      "    # this is the other request because the statistics and snippet require this because of the API    channelId.append(response['items'][0]['snippet']['channelId'])  # channel ID, which is constant here\n",
      "    channelTitle.append(response['items'][0]['snippet']['channelTitle'])    \n",
      "    # channel title, also constant\n",
      "    categoryId.append(response['items'][0]['snippet']['categoryId'])        \n",
      "    # stores the categories of the videos\n",
      "    favoriteCount.append(response['items'][0]['statistics']['favoriteCount'])   \n",
      "    # stores the favourite count of the videos\n",
      "    viewCount.append(response['items'][0]['statistics']['viewCount'])           \n",
      "    # stores the view counts\n",
      "\n",
      "    if 'likeCount' in response['items'][0]['statistics'].keys():             # checks for likes count then restores it, if no likes stores 0\n",
      "        likeCount.append(response['items'][0]['statistics']['likeCount'])\n",
      "    else:\n",
      "        likeCount.append('0')\n",
      "    if 'dislikeCount' in response['items'][0]['statistics'].keys():             # checks for dislikes count then stores it, if no dislikes found returns 0\n",
      "        dislikeCount.append(response['items'][0]['statistics']['dislikeCount'])\n",
      "    else:\n",
      "        likeCount.append('0')\n",
      "    if 'commentCount' in response['items'][0]['statistics'].keys():             # checks for comment count then stores it, if no comment found stores 0\n",
      "        commentCount.append(response['items'][0]['statistics']['commentCount'])\n",
      "    else:\n",
      "        commentCount.append('0')\n",
      "    if 'tags' in response['items'][0]['snippet'].keys():                        # checks for tags count then stores it, if none found stores 0\n",
      "        tags.append(response['items'][0]['snippet']['tags'])\n",
      "    else:\n",
      "        tags.append('0')\n",
      "youtube_dict = {\n",
      "'tags': tags,\n",
      "'channelId': channelId,\n",
      "'channelTitle': channelTitle,\n",
      "'categoryId': categoryId,\n",
      "'title': title,\n",
      "'videoId': videoId,\n",
      "'viewCount': viewCount,\n",
      "'likeCount': likeCount,\n",
      "'dislikeCount': dislikeCount,\n",
      "'commentCount': commentCount,\n",
      "'favoriteCount': favoriteCount\n",
      "}\n",
      "for x in youtube_dict:\n",
      "    print(x)\n",
      "for y in youtube_dict[x]:\n",
      "    print(y)\n",
      "40/77:\n",
      "DEVELOPER_KEY = \"1JUEz7xYuF7ETU5IuIn-4tCE\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "title = []\n",
      "channelId = []\n",
      "channelTitle = []\n",
      "categoryId = []\n",
      "videoId = []\n",
      "viewCount = []\n",
      "likeCount = []\n",
      "dislikeCount = []\n",
      "commentCount = []\n",
      "favoriteCount = []\n",
      "category = []\n",
      "tags = []\n",
      "videos = []\n",
      "tags = []\n",
      "max_search = 50\n",
      "order = \"relevance\"\n",
      "token = None\n",
      "location = None\n",
      "location_radius = None\n",
      "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "developerKey=DEVELOPER_KEY)\n",
      "search_result = youtube.search().list(q=\"SMTOWN\", type=\"video\",     \n",
      "                                      pageToken=token, order=order, part=\"id,snippet\", maxResults=max_search, location=location,\n",
      "                                      locationRadius=location_radius, channelId='UCEf_Bc-KVd7onSeifS3py9g').execute()\n",
      "#this line is to get the videos of the channel by the name of it\n",
      "for search_result in search_result.get(\"items\", []):\n",
      "if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "    title.append(search_result['snippet']['title']) \n",
      "    # the title of the video\n",
      "    videoId.append(search_result['id']['videoId']) \n",
      "    # the ID of the video\n",
      "    response = youtube.videos().list(part='statistics, snippet', id=search_result['id']['videoId']).execute() \n",
      "    # this is the other request because the statistics and snippet require this because of the API    channelId.append(response['items'][0]['snippet']['channelId'])  # channel ID, which is constant here\n",
      "    channelTitle.append(response['items'][0]['snippet']['channelTitle'])    \n",
      "    # channel title, also constant\n",
      "    categoryId.append(response['items'][0]['snippet']['categoryId'])        \n",
      "    # stores the categories of the videos\n",
      "    favoriteCount.append(response['items'][0]['statistics']['favoriteCount'])   \n",
      "    # stores the favourite count of the videos\n",
      "    viewCount.append(response['items'][0]['statistics']['viewCount'])           \n",
      "    # stores the view counts\n",
      "\n",
      "if 'likeCount' in response['items'][0]['statistics'].keys():             # checks for likes count then restores it, if no likes stores 0\n",
      "    likeCount.append(response['items'][0]['statistics']['likeCount'])\n",
      "else:\n",
      "    likeCount.append('0')\n",
      "if 'dislikeCount' in response['items'][0]['statistics'].keys():             # checks for dislikes count then stores it, if no dislikes found returns 0\n",
      "    dislikeCount.append(response['items'][0]['statistics']['dislikeCount'])\n",
      "else:\n",
      "    likeCount.append('0')\n",
      "if 'commentCount' in response['items'][0]['statistics'].keys():             # checks for comment count then stores it, if no comment found stores 0\n",
      "    commentCount.append(response['items'][0]['statistics']['commentCount'])\n",
      "else:\n",
      "    commentCount.append('0')\n",
      "if 'tags' in response['items'][0]['snippet'].keys():                        # checks for tags count then stores it, if none found stores 0\n",
      "    tags.append(response['items'][0]['snippet']['tags'])\n",
      "else:\n",
      "    tags.append('0')\n",
      "youtube_dict = {\n",
      "'tags': tags,\n",
      "'channelId': channelId,\n",
      "'channelTitle': channelTitle,\n",
      "'categoryId': categoryId,\n",
      "'title': title,\n",
      "'videoId': videoId,\n",
      "'viewCount': viewCount,\n",
      "'likeCount': likeCount,\n",
      "'dislikeCount': dislikeCount,\n",
      "'commentCount': commentCount,\n",
      "'favoriteCount': favoriteCount\n",
      "}\n",
      "for x in youtube_dict:\n",
      "    print(x)\n",
      "for y in youtube_dict[x]:\n",
      "    print(y)\n",
      "40/78:\n",
      "DEVELOPER_KEY = \"1JUEz7xYuF7ETU5IuIn-4tCE\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "title = []\n",
      "channelId = []\n",
      "channelTitle = []\n",
      "categoryId = []\n",
      "videoId = []\n",
      "viewCount = []\n",
      "likeCount = []\n",
      "dislikeCount = []\n",
      "commentCount = []\n",
      "favoriteCount = []\n",
      "category = []\n",
      "tags = []\n",
      "videos = []\n",
      "tags = []\n",
      "max_search = 50\n",
      "order = \"relevance\"\n",
      "token = None\n",
      "location = None\n",
      "location_radius = None\n",
      "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "developerKey=DEVELOPER_KEY)\n",
      "search_result = youtube.search().list(q=\"SMTOWN\", type=\"video\",     \n",
      "                                      pageToken=token, order=order, part=\"id,snippet\", maxResults=max_search, location=location,\n",
      "                                      locationRadius=location_radius, channelId='UCEf_Bc-KVd7onSeifS3py9g').execute()\n",
      "#this line is to get the videos of the channel by the name of it\n",
      "for search_result in search_result.get(\"items\", []):\n",
      "    if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "        title.append(search_result['snippet']['title']) \n",
      "        # the title of the video\n",
      "        videoId.append(search_result['id']['videoId']) \n",
      "        # the ID of the video\n",
      "        response = youtube.videos().list(part='statistics, snippet', id=search_result['id']['videoId']).execute() \n",
      "        # this is the other request because the statistics and snippet require this because of the API    channelId.append(response['items'][0]['snippet']['channelId'])  # channel ID, which is constant here\n",
      "        channelTitle.append(response['items'][0]['snippet']['channelTitle'])    \n",
      "        # channel title, also constant\n",
      "        categoryId.append(response['items'][0]['snippet']['categoryId'])        \n",
      "        # stores the categories of the videos\n",
      "        favoriteCount.append(response['items'][0]['statistics']['favoriteCount'])   \n",
      "        # stores the favourite count of the videos\n",
      "        viewCount.append(response['items'][0]['statistics']['viewCount'])           \n",
      "        # stores the view counts\n",
      "        if 'likeCount' in response['items'][0]['statistics'].keys():             # checks for likes count then restores it, if no likes stores 0\n",
      "            likeCount.append(response['items'][0]['statistics']['likeCount'])\n",
      "        else:\n",
      "            likeCount.append('0')\n",
      "        if 'dislikeCount' in response['items'][0]['statistics'].keys():             # checks for dislikes count then stores it, if no dislikes found returns 0\n",
      "            dislikeCount.append(response['items'][0]['statistics']['dislikeCount'])\n",
      "        else:\n",
      "            likeCount.append('0')\n",
      "        if 'commentCount' in response['items'][0]['statistics'].keys():             # checks for comment count then stores it, if no comment found stores 0\n",
      "            commentCount.append(response['items'][0]['statistics']['commentCount'])\n",
      "        else:\n",
      "            commentCount.append('0')\n",
      "        if 'tags' in response['items'][0]['snippet'].keys():                        # checks for tags count then stores it, if none found stores 0\n",
      "            tags.append(response['items'][0]['snippet']['tags'])\n",
      "        else:\n",
      "            tags.append('0')\n",
      "youtube_dict = {\n",
      "'tags': tags,\n",
      "'channelId': channelId,\n",
      "'channelTitle': channelTitle,\n",
      "'categoryId': categoryId,\n",
      "'title': title,\n",
      "'videoId': videoId,\n",
      "'viewCount': viewCount,\n",
      "'likeCount': likeCount,\n",
      "'dislikeCount': dislikeCount,\n",
      "'commentCount': commentCount,\n",
      "'favoriteCount': favoriteCount\n",
      "}\n",
      "for x in youtube_dict:\n",
      "    print(x)\n",
      "for y in youtube_dict[x]:\n",
      "    print(y)\n",
      "40/79:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "title = []\n",
      "channelId = []\n",
      "channelTitle = []\n",
      "categoryId = []\n",
      "videoId = []\n",
      "viewCount = []\n",
      "likeCount = []\n",
      "dislikeCount = []\n",
      "commentCount = []\n",
      "favoriteCount = []\n",
      "category = []\n",
      "tags = []\n",
      "videos = []\n",
      "tags = []\n",
      "max_search = 50\n",
      "order = \"relevance\"\n",
      "token = None\n",
      "location = None\n",
      "location_radius = None\n",
      "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "developerKey=DEVELOPER_KEY)\n",
      "search_result = youtube.search().list(q=\"SMTOWN\", type=\"video\",     \n",
      "                                      pageToken=token, order=order, part=\"id,snippet\", maxResults=max_search, location=location,\n",
      "                                      locationRadius=location_radius, channelId='UCEf_Bc-KVd7onSeifS3py9g').execute()\n",
      "#this line is to get the videos of the channel by the name of it\n",
      "for search_result in search_result.get(\"items\", []):\n",
      "    if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "        title.append(search_result['snippet']['title']) \n",
      "        # the title of the video\n",
      "        videoId.append(search_result['id']['videoId']) \n",
      "        # the ID of the video\n",
      "        response = youtube.videos().list(part='statistics, snippet', id=search_result['id']['videoId']).execute() \n",
      "        # this is the other request because the statistics and snippet require this because of the API    channelId.append(response['items'][0]['snippet']['channelId'])  # channel ID, which is constant here\n",
      "        channelTitle.append(response['items'][0]['snippet']['channelTitle'])    \n",
      "        # channel title, also constant\n",
      "        categoryId.append(response['items'][0]['snippet']['categoryId'])        \n",
      "        # stores the categories of the videos\n",
      "        favoriteCount.append(response['items'][0]['statistics']['favoriteCount'])   \n",
      "        # stores the favourite count of the videos\n",
      "        viewCount.append(response['items'][0]['statistics']['viewCount'])           \n",
      "        # stores the view counts\n",
      "        if 'likeCount' in response['items'][0]['statistics'].keys():             # checks for likes count then restores it, if no likes stores 0\n",
      "            likeCount.append(response['items'][0]['statistics']['likeCount'])\n",
      "        else:\n",
      "            likeCount.append('0')\n",
      "        if 'dislikeCount' in response['items'][0]['statistics'].keys():             # checks for dislikes count then stores it, if no dislikes found returns 0\n",
      "            dislikeCount.append(response['items'][0]['statistics']['dislikeCount'])\n",
      "        else:\n",
      "            likeCount.append('0')\n",
      "        if 'commentCount' in response['items'][0]['statistics'].keys():             # checks for comment count then stores it, if no comment found stores 0\n",
      "            commentCount.append(response['items'][0]['statistics']['commentCount'])\n",
      "        else:\n",
      "            commentCount.append('0')\n",
      "        if 'tags' in response['items'][0]['snippet'].keys():                        # checks for tags count then stores it, if none found stores 0\n",
      "            tags.append(response['items'][0]['snippet']['tags'])\n",
      "        else:\n",
      "            tags.append('0')\n",
      "youtube_dict = {\n",
      "'tags': tags,\n",
      "'channelId': channelId,\n",
      "'channelTitle': channelTitle,\n",
      "'categoryId': categoryId,\n",
      "'title': title,\n",
      "'videoId': videoId,\n",
      "'viewCount': viewCount,\n",
      "'likeCount': likeCount,\n",
      "'dislikeCount': dislikeCount,\n",
      "'commentCount': commentCount,\n",
      "'favoriteCount': favoriteCount\n",
      "}\n",
      "for x in youtube_dict:\n",
      "    print(x)\n",
      "for y in youtube_dict[x]:\n",
      "    print(y)\n",
      "40/80:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "title = []\n",
      "channelId = []\n",
      "channelTitle = []\n",
      "categoryId = []\n",
      "videoId = []\n",
      "viewCount = []\n",
      "likeCount = []\n",
      "dislikeCount = []\n",
      "commentCount = []\n",
      "favoriteCount = []\n",
      "category = []\n",
      "tags = []\n",
      "videos = []\n",
      "tags = []\n",
      "max_search = 50\n",
      "order = \"relevance\"\n",
      "token = None\n",
      "location = None\n",
      "location_radius = None\n",
      "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "developerKey=DEVELOPER_KEY)\n",
      "search_result = youtube.search().list(q=\"SMTOWN\", type=\"video\",     \n",
      "                                      pageToken=token, order=order, part=\"id,snippet\", maxResults=max_search, location=location,\n",
      "                                      locationRadius=location_radius, channelId='UCEf_Bc-KVd7onSeifS3py9g').execute()\n",
      "#this line is to get the videos of the channel by the name of it\n",
      "for search_result in search_result.get(\"items\", []):\n",
      "    if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "        title.append(search_result['snippet']['title']) \n",
      "        # the title of the video\n",
      "        videoId.append(search_result['id']['videoId']) \n",
      "        # the ID of the video\n",
      "        response = youtube.videos().list(part='statistics, snippet', id=search_result['id']['videoId']).execute() \n",
      "        # this is the other request because the statistics and snippet require this because of the API    channelId.append(response['items'][0]['snippet']['channelId'])  # channel ID, which is constant here\n",
      "        channelTitle.append(response['items'][0]['snippet']['channelTitle'])    \n",
      "        # channel title, also constant\n",
      "        categoryId.append(response['items'][0]['snippet']['categoryId'])        \n",
      "        # stores the categories of the videos\n",
      "        favoriteCount.append(response['items'][0]['statistics']['favoriteCount'])   \n",
      "        # stores the favourite count of the videos\n",
      "        viewCount.append(response['items'][0]['statistics']['viewCount'])           \n",
      "        # stores the view counts\n",
      "        if 'likeCount' in response['items'][0]['statistics'].keys():             # checks for likes count then restores it, if no likes stores 0\n",
      "            likeCount.append(response['items'][0]['statistics']['likeCount'])\n",
      "        else:\n",
      "            likeCount.append('0')\n",
      "        if 'dislikeCount' in response['items'][0]['statistics'].keys():             # checks for dislikes count then stores it, if no dislikes found returns 0\n",
      "            dislikeCount.append(response['items'][0]['statistics']['dislikeCount'])\n",
      "        else:\n",
      "            likeCount.append('0')\n",
      "        if 'commentCount' in response['items'][0]['statistics'].keys():             # checks for comment count then stores it, if no comment found stores 0\n",
      "            commentCount.append(response['items'][0]['statistics']['commentCount'])\n",
      "        else:\n",
      "            commentCount.append('0')\n",
      "        if 'tags' in response['items'][0]['snippet'].keys():                        # checks for tags count then stores it, if none found stores 0\n",
      "            tags.append(response['items'][0]['snippet']['tags'])\n",
      "        else:\n",
      "            tags.append('0')\n",
      "youtube_dict = {\n",
      "'tags': tags,\n",
      "'channelId': channelId,\n",
      "'channelTitle': channelTitle,\n",
      "'categoryId': categoryId,\n",
      "'title': title,\n",
      "'videoId': videoId,\n",
      "'viewCount': viewCount,\n",
      "'likeCount': likeCount,\n",
      "'dislikeCount': dislikeCount,\n",
      "'commentCount': commentCount,\n",
      "'favoriteCount': favoriteCount\n",
      "}\n",
      "for x in youtube_dict:\n",
      "    print(x)\n",
      "for y in youtube_dict[x]:\n",
      "    print(y)\n",
      "40/81:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "def youtube_search_list(q, max_results=10):\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
      "    developerKey=DEVELOPER_KEY)\n",
      "\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    search_response = youtube.search().list(\n",
      "        q=q,\n",
      "        part='id,snippet',\n",
      "        maxResults=max_results,\n",
      "        order='viewCount'\n",
      "      ).execute()\n",
      "\n",
      "    return search_response\n",
      "\n",
      "def youtube_search_video(q='spinners', max_results=10):\n",
      "    max_results = max_results\n",
      "    order = \"viewCount\"\n",
      "    token = None\n",
      "    location = None\n",
      "    location_radius = None\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "    developerKey=DEVELOPER_KEY)\n",
      "    q=q\n",
      "    #Return list of matching records up to max_search\n",
      "    search_result = youtube_search_list(q, max_results)\n",
      "\n",
      "    videos_list = []\n",
      "    for search_result in search_result.get(\"items\", []):\n",
      "\n",
      "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "            temp_dict_ = {}\n",
      "            #Available from initial search\n",
      "            temp_dict_['title'] = search_result['snippet']['title']  \n",
      "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
      "\n",
      "            #Secondary call to find statistics results for individual video\n",
      "            response = youtube.videos().list(\n",
      "                part='statistics, snippet', \n",
      "                id=search_result['id']['videoId']\n",
      "                    ).execute()  \n",
      "            response_statistics = response['items'][0]['statistics']\n",
      "            response_snippet = response['items'][0]['snippet']\n",
      "\n",
      "\n",
      "            snippet_list = ['publishedAt','channelId', 'description', \n",
      "                            'channelTitle', 'tags', 'categoryId', \n",
      "                            'liveBroadcastContent', 'defaultLanguage', ]\n",
      "            for val in snippet_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_snippet[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
      "\n",
      "            stats_list = ['favoriteCount', 'viewCount', 'likeCount', \n",
      "                          'dislikeCount', 'commentCount']\n",
      "            for val in stats_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_statistics[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
      "\n",
      "            #add back to main list\n",
      "            videos_list.append(temp_dict_)\n",
      "\n",
      "    return videos_list\n",
      "40/82:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "def youtube_search_list(q, max_results=10):\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
      "    developerKey=DEVELOPER_KEY)\n",
      "\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    search_response = youtube.search().list(\n",
      "        q=q,\n",
      "        part='id,snippet',\n",
      "        maxResults=max_results,\n",
      "        order='viewCount'\n",
      "      ).execute()\n",
      "\n",
      "    return search_response\n",
      "\n",
      "def youtube_search_video(q='spinners', max_results=10):\n",
      "    max_results = max_results\n",
      "    order = \"viewCount\"\n",
      "    token = None\n",
      "    location = None\n",
      "    location_radius = None\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "    developerKey=DEVELOPER_KEY)\n",
      "    q=q\n",
      "    #Return list of matching records up to max_search\n",
      "    search_result = youtube_search_list(q, max_results)\n",
      "\n",
      "    videos_list = []\n",
      "    for search_result in search_result.get(\"items\", []):\n",
      "\n",
      "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "            temp_dict_ = {}\n",
      "            #Available from initial search\n",
      "            temp_dict_['title'] = search_result['snippet']['title']  \n",
      "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
      "\n",
      "            #Secondary call to find statistics results for individual video\n",
      "            response = youtube.videos().list(\n",
      "                part='statistics, snippet', \n",
      "                id=search_result['id']['videoId']\n",
      "                    ).execute()  \n",
      "            response_statistics = response['items'][0]['statistics']\n",
      "            response_snippet = response['items'][0]['snippet']\n",
      "\n",
      "\n",
      "            snippet_list = ['publishedAt','channelId', 'description', \n",
      "                            'channelTitle', 'tags', 'categoryId', \n",
      "                            'liveBroadcastContent', 'defaultLanguage', ]\n",
      "            for val in snippet_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_snippet[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
      "\n",
      "            stats_list = ['favoriteCount', 'viewCount', 'likeCount', \n",
      "                          'dislikeCount', 'commentCount']\n",
      "            for val in stats_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_statistics[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
      "\n",
      "            #add back to main list\n",
      "            videos_list.append(temp_dict_)\n",
      "\n",
      "    return videos_list\n",
      "40/83:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "def youtube_search_list(q, max_results=10):\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
      "    developerKey=DEVELOPER_KEY)\n",
      "\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    search_response = youtube.search().list(\n",
      "        q=q,\n",
      "        part='id,snippet',\n",
      "        maxResults=max_results,\n",
      "        order='viewCount'\n",
      "      ).execute()\n",
      "\n",
      "    return search_response\n",
      "\n",
      "def youtube_search_video(q='spinners', max_results=10):\n",
      "    max_results = max_results\n",
      "    order = \"viewCount\"\n",
      "    token = None\n",
      "    location = None\n",
      "    location_radius = None\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "    developerKey=DEVELOPER_KEY)\n",
      "    q=q\n",
      "    #Return list of matching records up to max_search\n",
      "    search_result = youtube_search_list(q, max_results)\n",
      "\n",
      "    videos_list = []\n",
      "    for search_result in search_result.get(\"items\", []):\n",
      "\n",
      "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "            temp_dict_ = {}\n",
      "            #Available from initial search\n",
      "            temp_dict_['title'] = search_result['snippet']['title']  \n",
      "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
      "\n",
      "            #Secondary call to find statistics results for individual video\n",
      "            response = youtube.videos().list(\n",
      "                part='statistics, snippet', \n",
      "                id=search_result['id']['videoId']\n",
      "                    ).execute()  \n",
      "            response_statistics = response['items'][0]['statistics']\n",
      "            response_snippet = response['items'][0]['snippet']\n",
      "\n",
      "\n",
      "            snippet_list = ['publishedAt','channelId', 'description', \n",
      "                            'channelTitle', 'tags', 'categoryId', \n",
      "                            'liveBroadcastContent', 'defaultLanguage', ]\n",
      "            for val in snippet_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_snippet[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
      "\n",
      "            stats_list = ['favoriteCount', 'viewCount', 'likeCount', \n",
      "                          'dislikeCount', 'commentCount']\n",
      "            for val in stats_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_statistics[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
      "\n",
      "            #add back to main list\n",
      "            videos_list.append(temp_dict_)\n",
      "\n",
      "    return videos_list\n",
      "40/84: print(video_list)\n",
      "40/85: videos_list\n",
      "40/86: videos_list\n",
      "40/87:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "def youtube_search_list(q, max_results=10):\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
      "    developerKey=DEVELOPER_KEY)\n",
      "\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    search_response = youtube.search().list(\n",
      "        q=q,\n",
      "        part='id,snippet',\n",
      "        maxResults=max_results,\n",
      "        order='viewCount'\n",
      "      ).execute()\n",
      "\n",
      "    return search_response\n",
      "\n",
      "def youtube_search_video(q='spinners', max_results=10):\n",
      "    max_results = max_results\n",
      "    order = \"viewCount\"\n",
      "    token = None\n",
      "    location = None\n",
      "    location_radius = None\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "    developerKey=DEVELOPER_KEY)\n",
      "    q=q\n",
      "    #Return list of matching records up to max_search\n",
      "    search_result = youtube_search_list(q, max_results)\n",
      "\n",
      "    videos_list = []\n",
      "    for search_result in search_result.get(\"items\", []):\n",
      "\n",
      "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "            temp_dict_ = {}\n",
      "            #Available from initial search\n",
      "            temp_dict_['title'] = search_result['snippet']['title']  \n",
      "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
      "\n",
      "            #Secondary call to find statistics results for individual video\n",
      "            response = youtube.videos().list(\n",
      "                part='statistics, snippet', \n",
      "                id=search_result['id']['videoId']\n",
      "                    ).execute()  \n",
      "            response_statistics = response['items'][0]['statistics']\n",
      "            response_snippet = response['items'][0]['snippet']\n",
      "\n",
      "\n",
      "            snippet_list = ['publishedAt','channelId', 'description', \n",
      "                            'channelTitle', 'tags', 'categoryId', \n",
      "                            'liveBroadcastContent', 'defaultLanguage', ]\n",
      "            for val in snippet_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_snippet[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
      "\n",
      "            stats_list = ['favoriteCount', 'viewCount', 'likeCount', \n",
      "                          'dislikeCount', 'commentCount']\n",
      "            for val in stats_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_statistics[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
      "\n",
      "            #add back to main list\n",
      "            videos_list.append(temp_dict_)\n",
      "\n",
      "    return videos_list\n",
      "40/88:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "def youtube_search_list(q, max_results=10):\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
      "    developerKey=DEVELOPER_KEY)\n",
      "\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    search_response = youtube.search().list(\n",
      "        q=q,\n",
      "        part='id,snippet',\n",
      "        maxResults=max_results,\n",
      "        order='viewCount'\n",
      "      ).execute()\n",
      "\n",
      "    return search_response\n",
      "\n",
      "def youtube_search_video(q='spinners', max_results=10):\n",
      "    max_results = max_results\n",
      "    order = \"viewCount\"\n",
      "    token = None\n",
      "    location = None\n",
      "    location_radius = None\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "    developerKey=DEVELOPER_KEY)\n",
      "    q=q\n",
      "    #Return list of matching records up to max_search\n",
      "    search_result = youtube_search_list(q, max_results)\n",
      "\n",
      "    videos_list = []\n",
      "    for search_result in search_result.get(\"items\", []):\n",
      "\n",
      "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "            temp_dict_ = {}\n",
      "            #Available from initial search\n",
      "            temp_dict_['title'] = search_result['snippet']['title']  \n",
      "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
      "\n",
      "            #Secondary call to find statistics results for individual video\n",
      "            response = youtube.videos().list(\n",
      "                part='statistics, snippet', \n",
      "                id=search_result['id']['videoId']\n",
      "                    ).execute()  \n",
      "            response_statistics = response['items'][0]['statistics']\n",
      "            response_snippet = response['items'][0]['snippet']\n",
      "\n",
      "\n",
      "            snippet_list = ['publishedAt','channelId', 'description', \n",
      "                            'channelTitle', 'tags', 'categoryId', \n",
      "                            'liveBroadcastContent', 'defaultLanguage', ]\n",
      "            for val in snippet_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_snippet[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
      "\n",
      "            stats_list = ['favoriteCount', 'viewCount', 'likeCount', \n",
      "                          'dislikeCount', 'commentCount']\n",
      "            for val in stats_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_statistics[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
      "\n",
      "            #add back to main list\n",
      "            videos_list.append(temp_dict_)\n",
      "\n",
      "    return videos_list\n",
      "40/89: videos_list\n",
      "41/1:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "def youtube_search_list(q, max_results=10):\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
      "    developerKey=DEVELOPER_KEY)\n",
      "\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    search_response = youtube.search().list(\n",
      "        q=q,\n",
      "        part='id,snippet',\n",
      "        maxResults=max_results,\n",
      "        order='viewCount'\n",
      "      ).execute()\n",
      "\n",
      "    return search_response\n",
      "\n",
      "def youtube_search_video(q='spinners', max_results=10):\n",
      "    max_results = max_results\n",
      "    order = \"viewCount\"\n",
      "    token = None\n",
      "    location = None\n",
      "    location_radius = None\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "    developerKey=DEVELOPER_KEY)\n",
      "    q=q\n",
      "    #Return list of matching records up to max_search\n",
      "    search_result = youtube_search_list(q, max_results)\n",
      "\n",
      "    videos_list = []\n",
      "    for search_result in search_result.get(\"items\", []):\n",
      "\n",
      "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "            temp_dict_ = {}\n",
      "            #Available from initial search\n",
      "            temp_dict_['title'] = search_result['snippet']['title']  \n",
      "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
      "\n",
      "            #Secondary call to find statistics results for individual video\n",
      "            response = youtube.videos().list(\n",
      "                part='statistics, snippet', \n",
      "                id=search_result['id']['videoId']\n",
      "                    ).execute()  \n",
      "            response_statistics = response['items'][0]['statistics']\n",
      "            response_snippet = response['items'][0]['snippet']\n",
      "\n",
      "\n",
      "            snippet_list = ['publishedAt','channelId', 'description', \n",
      "                            'channelTitle', 'tags', 'categoryId', \n",
      "                            'liveBroadcastContent', 'defaultLanguage', ]\n",
      "            for val in snippet_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_snippet[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
      "\n",
      "            stats_list = ['favoriteCount', 'viewCount', 'likeCount', \n",
      "                          'dislikeCount', 'commentCount']\n",
      "            for val in stats_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_statistics[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
      "\n",
      "            #add back to main list\n",
      "            videos_list.append(temp_dict_)\n",
      "\n",
      "    return videos_list\n",
      "41/2: videos_list\n",
      "41/3:\n",
      "youtube_search_list(\"EXO\")\n",
      "youtube_search_video()\n",
      "41/4: !pip install --upgrade google-api-python-client\n",
      "41/5: !pip install --upgrade google-auth google-auth-oauthlib google-auth-httplib2\n",
      "41/6: !pip install --upgrade flask\n",
      "41/7: !pip install --upgrade requests\n",
      "41/8: !pip install --upgrade google-auth google-auth-oauthlib google-auth-httplib2\n",
      "41/9: !pip install --upgrade flask\n",
      "41/10: !pip install --upgrade requests\n",
      "41/11: !pip install --upgrade google-auth google-auth-oauthlib google-auth-httplib2\n",
      "41/12: !pip install --upgrade flask\n",
      "41/13: !pip install --upgrade requests\n",
      "41/14:\n",
      "!pip install oauth2client\n",
      "!pip install pandas\n",
      "!pip install easydict\n",
      "41/15:\n",
      "!pip3 uninstall -y rq-scheduler\n",
      "!pip3 install rq-scheduler==0.7.0\n",
      "41/16:\n",
      "from apiclient.discovery import build #pip install google-api-python-client\n",
      "from apiclient.errors import HttpError #pip install google-api-python-client\n",
      "from oauth2client.tools import argparser #pip install oauth2client\n",
      "import pandas as pd #pip install pandas\n",
      "import argparse\n",
      "41/17:\n",
      "DEVELOPER_KEY = \"1JUEz7xYuF7ETU5IuIn-4tCE\" \n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "41/18:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "def youtube_search_list(q, max_results=10):\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
      "    developerKey=DEVELOPER_KEY)\n",
      "\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    search_response = youtube.search().list(\n",
      "        q=q,\n",
      "        part='id,snippet',\n",
      "        maxResults=max_results,\n",
      "        order='viewCount'\n",
      "      ).execute()\n",
      "\n",
      "    return search_response\n",
      "\n",
      "def youtube_search_video(q='spinners', max_results=10):\n",
      "    max_results = max_results\n",
      "    order = \"viewCount\"\n",
      "    token = None\n",
      "    location = None\n",
      "    location_radius = None\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "    developerKey=DEVELOPER_KEY)\n",
      "    q=q\n",
      "    #Return list of matching records up to max_search\n",
      "    search_result = youtube_search_list(q, max_results)\n",
      "\n",
      "    videos_list = []\n",
      "    for search_result in search_result.get(\"items\", []):\n",
      "\n",
      "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "            temp_dict_ = {}\n",
      "            #Available from initial search\n",
      "            temp_dict_['title'] = search_result['snippet']['title']  \n",
      "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
      "\n",
      "            #Secondary call to find statistics results for individual video\n",
      "            response = youtube.videos().list(\n",
      "                part='statistics, snippet', \n",
      "                id=search_result['id']['videoId']\n",
      "                    ).execute()  \n",
      "            response_statistics = response['items'][0]['statistics']\n",
      "            response_snippet = response['items'][0]['snippet']\n",
      "\n",
      "\n",
      "            snippet_list = ['publishedAt','channelId', 'description', \n",
      "                            'channelTitle', 'tags', 'categoryId', \n",
      "                            'liveBroadcastContent', 'defaultLanguage', ]\n",
      "            for val in snippet_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_snippet[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
      "\n",
      "            stats_list = ['favoriteCount', 'viewCount', 'likeCount', \n",
      "                          'dislikeCount', 'commentCount']\n",
      "            for val in stats_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_statistics[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
      "\n",
      "            #add back to main list\n",
      "            videos_list.append(temp_dict_)\n",
      "\n",
      "    return videos_list\n",
      "41/19:\n",
      "youtube_search_list(\"EXO\")\n",
      "youtube_search_video()\n",
      "41/20:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "def youtube_search_list(q, max_results=10):\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
      "    developerKey=DEVELOPER_KEY)\n",
      "\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    search_response = youtube.search().list(\n",
      "        q=q,\n",
      "        part='id,snippet',\n",
      "        maxResults=max_results,\n",
      "        order='viewCount'\n",
      "      ).execute()\n",
      "\n",
      "    return search_response\n",
      "\n",
      "def youtube_search_video(q='EXO', max_results=10):\n",
      "    max_results = max_results\n",
      "    order = \"viewCount\"\n",
      "    token = None\n",
      "    location = None\n",
      "    location_radius = None\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "    developerKey=DEVELOPER_KEY)\n",
      "    q=q\n",
      "    #Return list of matching records up to max_search\n",
      "    search_result = youtube_search_list(q, max_results)\n",
      "\n",
      "    videos_list = []\n",
      "    for search_result in search_result.get(\"items\", []):\n",
      "\n",
      "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "            temp_dict_ = {}\n",
      "            #Available from initial search\n",
      "            temp_dict_['title'] = search_result['snippet']['title']  \n",
      "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
      "\n",
      "            #Secondary call to find statistics results for individual video\n",
      "            response = youtube.videos().list(\n",
      "                part='statistics, snippet', \n",
      "                id=search_result['id']['videoId']\n",
      "                    ).execute()  \n",
      "            response_statistics = response['items'][0]['statistics']\n",
      "            response_snippet = response['items'][0]['snippet']\n",
      "\n",
      "\n",
      "            snippet_list = ['publishedAt','channelId', 'description', \n",
      "                            'channelTitle', 'tags', 'categoryId', \n",
      "                            'liveBroadcastContent', 'defaultLanguage', ]\n",
      "            for val in snippet_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_snippet[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
      "\n",
      "            stats_list = ['favoriteCount', 'viewCount', 'likeCount', \n",
      "                          'dislikeCount', 'commentCount']\n",
      "            for val in stats_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_statistics[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
      "\n",
      "            #add back to main list\n",
      "            videos_list.append(temp_dict_)\n",
      "\n",
      "    return videos_list\n",
      "41/21:\n",
      "youtube_search_list(\"EXO\")\n",
      "youtube_search_video()\n",
      "41/22:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "def youtube_search_list(q, max_results=10):\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
      "    developerKey=DEVELOPER_KEY)\n",
      "\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    search_response = youtube.search().list(\n",
      "        q=q,\n",
      "        part='id,snippet',\n",
      "        maxResults=max_results,\n",
      "        order='viewCount'\n",
      "      ).execute()\n",
      "\n",
      "    return search_response\n",
      "\n",
      "def youtube_search_video(q='EXO', max_results=10):\n",
      "    max_results = max_results\n",
      "    order = \"viewCount\"\n",
      "    token = None\n",
      "    location = None\n",
      "    location_radius = None\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "    developerKey=DEVELOPER_KEY)\n",
      "    q=q\n",
      "    #Return list of matching records up to max_search\n",
      "    search_result = youtube_search_list(q, max_results)\n",
      "\n",
      "    videos_list = []\n",
      "    for search_result in search_result.get(\"items\", []):\n",
      "\n",
      "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "            temp_dict_ = {}\n",
      "            #Available from initial search\n",
      "            temp_dict_['title'] = search_result['snippet']['title']  \n",
      "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
      "\n",
      "            #Secondary call to find statistics results for individual video\n",
      "            response = youtube.videos().list(\n",
      "                part='statistics, snippet', \n",
      "                id=search_result['id']['videoId']\n",
      "                    ).execute()  \n",
      "            response_statistics = response['items'][0]['statistics']\n",
      "            response_snippet = response['items'][0]['snippet']\n",
      "\n",
      "\n",
      "            snippet_list = ['publishedAt', \n",
      "                            'channelTitle', 'tags', 'categoryId', ]\n",
      "            for val in snippet_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_snippet[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
      "\n",
      "            stats_list = ['favoriteCount', 'viewCount', 'likeCount', \n",
      "                          'dislikeCount', 'commentCount']\n",
      "            for val in stats_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_statistics[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
      "\n",
      "            #add back to main list\n",
      "            videos_list.append(temp_dict_)\n",
      "\n",
      "    return videos_list\n",
      "41/23:\n",
      "youtube_search_list(\"EXO\")\n",
      "youtube_search_video()\n",
      "41/24:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "def youtube_search_list(q, max_results=10):\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
      "    developerKey=DEVELOPER_KEY)\n",
      "\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    search_response = youtube.search().list(\n",
      "        q=q,\n",
      "        part='id,snippet',\n",
      "        maxResults=max_results,\n",
      "        order='viewCount'\n",
      "      ).execute()\n",
      "\n",
      "    return search_response\n",
      "\n",
      "def youtube_search_video(q='EXO', max_results=10):\n",
      "    max_results = max_results\n",
      "    order = \"viewCount\"\n",
      "    token = None\n",
      "    location = None\n",
      "    location_radius = None\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "    developerKey=DEVELOPER_KEY)\n",
      "    q=q\n",
      "    #Return list of matching records up to max_search\n",
      "    search_result = youtube_search_list(q, max_results)\n",
      "\n",
      "    videos_list = []\n",
      "    for search_result in search_result.get(\"items\", []):\n",
      "\n",
      "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "            temp_dict_ = {}\n",
      "            #Available from initial search\n",
      "            temp_dict_['title'] = search_result['snippet']['title']  \n",
      "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
      "\n",
      "            #Secondary call to find statistics results for individual video\n",
      "            response = youtube.videos().list(\n",
      "                part='statistics, snippet', \n",
      "                id=search_result['id']['videoId']\n",
      "                    ).execute()  \n",
      "            response_statistics = response['items'][0]['statistics']\n",
      "            response_snippet = response['items'][0]['snippet']\n",
      "\n",
      "\n",
      "            snippet_list = ['publishedAt', \n",
      "                            'channelTitle', 'categoryId', ]\n",
      "            for val in snippet_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_snippet[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
      "\n",
      "            stats_list = ['favoriteCount', 'viewCount', 'likeCount', \n",
      "                          'dislikeCount', 'commentCount']\n",
      "            for val in stats_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_statistics[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
      "\n",
      "            #add back to main list\n",
      "            videos_list.append(temp_dict_)\n",
      "\n",
      "    return videos_list\n",
      "41/25:\n",
      "youtube_search_list(\"EXO\")\n",
      "youtube_search_video()\n",
      "42/1:\n",
      "!pip install --upgrade google-api-python-client\n",
      "!pip install --upgrade google-auth google-auth-oauthlib google-auth-httplib2\n",
      "42/2: python yt_analytics_v2.py\n",
      "42/3: !python yt_analytics_v2.py\n",
      "42/4: !python yt_analytics_v2.py\n",
      "42/5: !python yt_analytics_v2.py\n",
      "42/6: !python yt_analytics_v2.py\n",
      "42/7: !python yt_analytics_v2.py\n",
      "42/8: !python yt_analytics_v2.py\n",
      "42/9: !python yt_analytics_v2.py\n",
      "42/10: !python yt_analytics_v2.py\n",
      "42/11: !python yt_analytics_v2.py\n",
      "43/1: !python yt_analytics_v2.py\n",
      "43/2: !python yt_analytics_v2.py\n",
      "44/1:\n",
      "!pip install --upgrade google-api-python-client\n",
      "!pip install --upgrade google-auth google-auth-oauthlib google-auth-httplib2\n",
      "44/2: !python yt_analytics_v2.py\n",
      "44/3: !python yt_analytics_v2.py\n",
      "41/26:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "def youtube_search_list(q, max_results=10):\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
      "    developerKey=DEVELOPER_KEY)\n",
      "\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    search_response = youtube.search().list(\n",
      "        q=q,\n",
      "        part='id,snippet',\n",
      "        maxResults=max_results,\n",
      "        order='viewCount'\n",
      "      ).execute()\n",
      "\n",
      "    return search_response\n",
      "\n",
      "def youtube_search_video(q='EXO', max_results=100):\n",
      "    max_results = max_results\n",
      "    order = \"viewCount\"\n",
      "    token = None\n",
      "    location = None\n",
      "    location_radius = None\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "    developerKey=DEVELOPER_KEY)\n",
      "    q=q\n",
      "    #Return list of matching records up to max_search\n",
      "    search_result = youtube_search_list(q, max_results)\n",
      "\n",
      "    videos_list = []\n",
      "    for search_result in search_result.get(\"items\", []):\n",
      "\n",
      "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "            temp_dict_ = {}\n",
      "            #Available from initial search\n",
      "            temp_dict_['title'] = search_result['snippet']['title']  \n",
      "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
      "\n",
      "            #Secondary call to find statistics results for individual video\n",
      "            response = youtube.videos().list(\n",
      "                part='statistics, snippet', \n",
      "                id=search_result['id']['videoId']\n",
      "                    ).execute()  \n",
      "            response_statistics = response['items'][0]['statistics']\n",
      "            response_snippet = response['items'][0]['snippet']\n",
      "\n",
      "\n",
      "            snippet_list = ['publishedAt', \n",
      "                            'channelTitle', 'categoryId', ]\n",
      "            for val in snippet_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_snippet[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
      "\n",
      "            stats_list = ['favoriteCount', 'viewCount', 'likeCount', \n",
      "                          'dislikeCount', 'commentCount']\n",
      "            for val in stats_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_statistics[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
      "\n",
      "            #add back to main list\n",
      "            videos_list.append(temp_dict_)\n",
      "\n",
      "    return videos_list\n",
      "41/27:\n",
      "youtube_search_list(\"EXO\")\n",
      "youtube_search_video()\n",
      "41/28:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "def youtube_search_list(q, max_results=10):\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
      "    developerKey=DEVELOPER_KEY)\n",
      "\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    search_response = youtube.search().list(\n",
      "        q=q,\n",
      "        part='id,snippet',\n",
      "        maxResults=max_results,\n",
      "        order='viewCount'\n",
      "      ).execute()\n",
      "\n",
      "    return search_response\n",
      "\n",
      "def youtube_search_video(q='EXO', max_results=50):\n",
      "    max_results = max_results\n",
      "    order = \"viewCount\"\n",
      "    token = None\n",
      "    location = None\n",
      "    location_radius = None\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "    developerKey=DEVELOPER_KEY)\n",
      "    q=q\n",
      "    #Return list of matching records up to max_search\n",
      "    search_result = youtube_search_list(q, max_results)\n",
      "\n",
      "    videos_list = []\n",
      "    for search_result in search_result.get(\"items\", []):\n",
      "\n",
      "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "            temp_dict_ = {}\n",
      "            #Available from initial search\n",
      "            temp_dict_['title'] = search_result['snippet']['title']  \n",
      "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
      "\n",
      "            #Secondary call to find statistics results for individual video\n",
      "            response = youtube.videos().list(\n",
      "                part='statistics, snippet', \n",
      "                id=search_result['id']['videoId']\n",
      "                    ).execute()  \n",
      "            response_statistics = response['items'][0]['statistics']\n",
      "            response_snippet = response['items'][0]['snippet']\n",
      "\n",
      "\n",
      "            snippet_list = ['publishedAt', \n",
      "                            'channelTitle', 'categoryId', ]\n",
      "            for val in snippet_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_snippet[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
      "\n",
      "            stats_list = ['favoriteCount', 'viewCount', 'likeCount', \n",
      "                          'dislikeCount', 'commentCount']\n",
      "            for val in stats_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_statistics[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
      "\n",
      "            #add back to main list\n",
      "            videos_list.append(temp_dict_)\n",
      "\n",
      "    return videos_list\n",
      "41/29:\n",
      "youtube_search_list(\"EXO\")\n",
      "youtube_search_video()\n",
      "41/30:\n",
      "DEVELOPER_KEY = \"AIzaSyCkj8oRv5FoQ6udpFInxcthILZQqV9W50o\"\n",
      "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
      "YOUTUBE_API_VERSION = \"v3\"\n",
      "def youtube_search_list(q, max_results=10):\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
      "    developerKey=DEVELOPER_KEY)\n",
      "\n",
      "  # Call the search.list method to retrieve results matching the specified\n",
      "  # query term.\n",
      "    search_response = youtube.search().list(\n",
      "        q=q,\n",
      "        part='id,snippet',\n",
      "        maxResults=max_results,\n",
      "        order='viewCount'\n",
      "      ).execute()\n",
      "\n",
      "    return search_response\n",
      "\n",
      "def youtube_search_video(q='EXO', max_results=50):\n",
      "    max_results = max_results\n",
      "    order = \"viewCount\"\n",
      "    token = None\n",
      "    location = None\n",
      "    location_radius = None\n",
      "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,         \n",
      "    developerKey=DEVELOPER_KEY)\n",
      "    q=q\n",
      "    #Return list of matching records up to max_search\n",
      "    search_result = youtube_search_list(q, max_results)\n",
      "\n",
      "    videos_list = []\n",
      "    for search_result in search_result.get(\"items\", []):\n",
      "\n",
      "        if search_result[\"id\"][\"kind\"] == 'youtube#video':\n",
      "            temp_dict_ = {}\n",
      "            #Available from initial search\n",
      "            temp_dict_['title'] = search_result['snippet']['title']  \n",
      "            temp_dict_['vidId'] = search_result['id']['videoId']  \n",
      "\n",
      "            #Secondary call to find statistics results for individual video\n",
      "            response = youtube.videos().list(\n",
      "                part='statistics, snippet', \n",
      "                id=search_result['id']['videoId']\n",
      "                    ).execute()  \n",
      "            response_statistics = response['items'][0]['statistics']\n",
      "            response_snippet = response['items'][0]['snippet']\n",
      "\n",
      "\n",
      "            snippet_list = ['publishedAt', \n",
      "                            'channelTitle', 'categoryId', ]\n",
      "            for val in snippet_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_snippet[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'    \n",
      "\n",
      "            stats_list = ['viewCount', 'likeCount', \n",
      "                          'dislikeCount', 'commentCount']\n",
      "            for val in stats_list:\n",
      "                try:\n",
      "                    temp_dict_[val] = response_statistics[val]\n",
      "                except:\n",
      "                    #Not stored if not present\n",
      "                    temp_dict_[val] = 'xxNoneFoundxx'\n",
      "\n",
      "            #add back to main list\n",
      "            videos_list.append(temp_dict_)\n",
      "\n",
      "    return videos_list\n",
      "41/31:\n",
      "youtube_search_list(\"EXO\")\n",
      "youtube_search_video()\n",
      "45/1: import requests\n",
      "45/2: request = \"https://www.rottentomatoes.com/top/bestofrt/\"\n",
      "45/3: get.request()\n",
      "45/4: url = \"https://www.rottentomatoes.com/top/bestofrt/\"\n",
      "45/5: get.requests()\n",
      "45/6: url.request()\n",
      "45/7: rottentomatoes = request.get(url)\n",
      "45/8: rottentomatoes = requests.get(url)\n",
      "45/9: r.text\n",
      "45/10: rottentomatoes.text\n",
      "45/11: r.text\n",
      "45/12: import bs4\n",
      "46/1: data\n",
      "45/13: mypage = bs4.BeautifulSoup(rottentomatoes.text)\n",
      "45/14: mytable = mypage.find('table')\n",
      "45/15: mytable\n",
      "45/16: mytable = mypage.find('tr')\n",
      "45/17: mytable = mypage.find('table')\n",
      "45/18: mytable\n",
      "45/19: mytable = mypage.find('tr')\n",
      "45/20: mytable\n",
      "45/21: mytable = mypage.find('href')\n",
      "45/22: mytable\n",
      "45/23: mytable = mypage.find('td')\n",
      "45/24: mytable\n",
      "45/25: mytable = mypage.find('tbody')\n",
      "45/26: mytable\n",
      "45/27: mytable = mypage.find('class')\n",
      "45/28: mytable\n",
      "45/29: mytable = mypage.find('type')\n",
      "45/30: mytable\n",
      "45/31: mytable = mypage.find('@type')\n",
      "45/32: mytable\n",
      "45/33: from bs4 import BeautifulSoup\n",
      "45/34: link = [a.attrs.get('a href') for a in mytable.select('td')]\n",
      "47/1: mytable = mypage.find('table')\n",
      "47/2: mytable\n",
      "47/3: mypage = bs4.BeautifulSoup(r.text)\n",
      "47/4: import bs4\n",
      "47/5: mypage = bs4.BeautifulSoup(r.text)\n",
      "45/35: mytable = mypage.find(\"table\")\n",
      "45/36: from bs4 import BeautifulSoup\n",
      "45/37: link = [a.attrs.get('a href') for a in mytable.select('td')]\n",
      "45/38: link\n",
      "47/6: mytable = mypage.find('table')\n",
      "45/39: link = [a.attrs.get('a href') for a in mytable.select('td a')]\n",
      "45/40: link\n",
      "45/41: mytable\n",
      "45/42: mytable = mypage.find_all(\"table\")\n",
      "45/43: from bs4 import BeautifulSoup\n",
      "45/44: mytable\n",
      "47/7: import bs4\n",
      "47/8: mypage = bs4.BeautifulSoup(r.text)\n",
      "47/9: mytable = mypage.find('table')\n",
      "47/10: mytable\n",
      "47/11: import requests\n",
      "47/12: r = requests.get('https://www.imdb.com/chart/top')\n",
      "47/13: type(r)\n",
      "47/14: r.text\n",
      "47/15: import bs4\n",
      "47/16: mypage = bs4.BeautifulSoup(r.text)\n",
      "47/17: mytable = mypage.find('table')\n",
      "47/18: mytable\n",
      "45/45: mytable\n",
      "47/19: crew = [a.attrs.get('a href') for a in mytable.select('td.titleColumn a')]\n",
      "47/20: crew\n",
      "47/21: crew = [a.attrs.get('href') for a in mytable.select('td.titleColumn a')]\n",
      "47/22: crew\n",
      "45/46: link = [a.attrs.get('href') for a in mytable.select('td.titleColumn a')]\n",
      "45/47: link = [a.attrs.get('class') for a in mytable.select('td.titleColumn a')]\n",
      "45/48: link = [a.attrs.get('href') for a in mytable.select('td')]\n",
      "45/49: link = [a.attrs.get('href') for a in mytable.select('td.middle_col a')]\n",
      "45/50: link = [a.attrs.get('href') for a in mytable.select('td.unstyled article a')]\n",
      "45/51: mytable.find('href')\n",
      "45/52: mytable.find('a')\n",
      "45/53: mytable.find('unstyled article')\n",
      "45/54: mytable.find(\"unstyled article\")\n",
      "45/55: mytable.select(\"unstyled article\")\n",
      "45/56: mytable.select(\"td\")\n",
      "45/57: mytable.select(\"tdr)\n",
      "45/58: mytable.select('tr')\n",
      "49/1: import pd\n",
      "49/2: import pandas\n",
      "49/3: pd.read_csv('openrise_sample.csv')\n",
      "49/4: import pandas as pd\n",
      "49/5: pd.read_csv('openrise_sample.csv')\n",
      "49/6: pd.read_csv('openrice_sample.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/7: pd.read_csv('openrice_sample.csv', header=None)\n",
      "49/8: df = pd.read_csv('openrice_sample.csv', header=None)\n",
      "45/59: mytable = mypage.find(\"table\")\n",
      "45/60: from bs4 import BeautifulSoup\n",
      "45/61: mytable\n",
      "45/62: mytable.select('tr')\n",
      "45/63: mytable = mypage.select(\"table\")\n",
      "45/64: from bs4 import BeautifulSoup\n",
      "45/65: mytable\n",
      "45/66: mytable.select('tr')\n",
      "49/9: df.columns\n",
      "49/10:\n",
      "df.columns = [\n",
      "    'title',\n",
      "    'address',\n",
      "    'price-range',\n",
      "    'style',\n",
      "    'likes',\n",
      "    'comments',\n",
      "    'bookmarks',\n",
      "    'discount'\n",
      "]\n",
      "49/11:\n",
      "df.columns = [\n",
      "    ' ',\n",
      "    'title',\n",
      "    'address',\n",
      "    'price-range',\n",
      "    'style',\n",
      "    'likes',\n",
      "    'comments',\n",
      "    'bookmarks',\n",
      "    'discount'\n",
      "]\n",
      "49/12: df\n",
      "49/13:\n",
      "df.columns = [\n",
      "    'title',\n",
      "    'address',\n",
      "    'price-range',\n",
      "    'style',\n",
      "    'likes',\n",
      "    'comments',\n",
      "    'bookmarks',\n",
      "    'discount'\n",
      "]\n",
      "49/14: import matplotlib\n",
      "49/15: !pip install matplotlib\n",
      "49/16:\n",
      "df.columns = [\n",
      "    'title',\n",
      "    'address',\n",
      "    'price-range',\n",
      "    'style',\n",
      "    'special',\n",
      "    'likes',\n",
      "    'comments',\n",
      "    'bookmarks',\n",
      "    'discount'\n",
      "]\n",
      "49/17: df\n",
      "49/18: df['likes'].hist(bins=5)\n",
      "49/19: df['likes'].hist(bins=5)\n",
      "49/20: df['country'].value_counts()\n",
      "49/21: df['style'].value_counts()\n",
      "49/22: style.hist(\"\")\n",
      "49/23: style.hist()\n",
      "49/24: style = df['style'].value_counts()\n",
      "49/25: style.hist()\n",
      "49/26: df['style'].hist()\n",
      "49/27: df['style'].hist(bins=10)\n",
      "49/28: df['style'].hist(bins=10)\n",
      "49/29: df['style'].hist(bins=10)\n",
      "49/30: df['bookmarks'].hist(bins=10)\n",
      "49/31: df['bookmarks'].hist(bins=10)\n",
      "49/32: df['bookmarks'].hist(bins=10)\n",
      "49/33: df = pd.read_csv('openrice_sample.csv', header=None)\n",
      "49/34: df['bookmarks'].hist(bins=10)\n",
      "51/1:\n",
      "from bs4 import BeautifulSoup\n",
      "import time\n",
      "import json\n",
      "from contextlib import contextmanager\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import ElementNotVisibleException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "\n",
      "base_url = 'https://www.rottentomatoes.com'\n",
      "51/2: !pip install selenium\n",
      "51/3:\n",
      "from bs4 import BeautifulSoup\n",
      "import time\n",
      "import json\n",
      "from contextlib import contextmanager\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import ElementNotVisibleException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "\n",
      "base_url = 'https://www.rottentomatoes.com'\n",
      "51/4: browse_urls = ['https://www.rottentomatoes.com/top/bestofrt/']\n",
      "51/5:\n",
      "class text_to_be_not_present_in_element(object):\n",
      "    \"\"\" An expectation for checking if the given text is present in the\n",
      "    specified element.\n",
      "    locator, text\n",
      "    \"\"\"\n",
      "    def __init__(self, locator, text_):\n",
      "        self.locator = locator\n",
      "        self.text = text_\n",
      "\n",
      "    def __call__(self, driver):\n",
      "        try :\n",
      "            element_text = EC._find_element(driver, self.locator).text\n",
      "            return not(self.text in element_text)\n",
      "        except StaleElementReferenceException:\n",
      "            return False\n",
      "51/6:\n",
      "def parse_movie_urls():\n",
      "    '''Creates dictionary of all movies under Browse All of RT using div class = \"mb-movie\"\n",
      "        and storing it as {title:url}'''\n",
      "    \n",
      "    #create dict\n",
      "    movie_list = {}\n",
      "\n",
      "    for browse_url in browse_urls:\n",
      "\n",
      "\n",
      "        #start selenium driver\n",
      "        driver = webdriver.Firefox()\n",
      "        driver.get(browse_url)\n",
      "        \n",
      "        soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
      "        show_count = soup.find('span',attrs={'id':'showing-count'}).text\n",
      "        total_count = int(show_count.split(' ')[3])\n",
      "        \n",
      "        #clicking loop: keep expanding page till it is showing all movies\n",
      "        elem = driver.find_element_by_xpath('//*[@id=\"show-more-btn\"]/button')\n",
      "\n",
      "        while(True):\n",
      "            \n",
      "\n",
      "            #check current count vs total count\n",
      "            soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
      "            show_count = soup.find('span',attrs={'id':'showing-count'}).text\n",
      "\n",
      "            print(show_count)\n",
      "\n",
      "            current_count = int(show_count.split(' ')[1])\n",
      "\n",
      "            #if it has finished clicking, break out of while loop\n",
      "            if(current_count >= (total_count-1)):\n",
      "                break\n",
      "\n",
      "            else:\n",
      "                #continue clicking\n",
      "                try:\n",
      "                    elem.click()\n",
      "                    wait = WebDriverWait(driver, 90)\n",
      "                    #wait until mb-movies is not stale\n",
      "                    # elem = wait.until((float(driver.find_element_by_class_name('mb-movies').get_attribute(\"style\").split('opacity: ')[1][0]) == 0.5))\n",
      "                    # elem = wait.until((float(driver.find_element_by_class_name('mb-movies').get_attribute(\"style\").split('opacity: ')[1][0]) == 1))\n",
      "                    wait = wait.until(text_to_be_not_present_in_element((By.ID,'showing-count'),show_count))\n",
      "                except ElementNotVisibleException:\n",
      "                    break\n",
      "                except TimeoutException:\n",
      "                    break\n",
      "        \n",
      "        print('Scraping now')\n",
      "        \n",
      "        #record each movie title and its url inside dict\n",
      "        soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
      "        movies = soup.find('div', {'class' :\"mb-movies\"})\n",
      "        for movie in movies:\n",
      "            url = movie.find('a',{'class' : \"popoverTrigger\"})['href']\n",
      "            title = movie.find('h3',{'class' : \"movieTitle\"}).text\n",
      "\n",
      "            movie_list[title] = url\n",
      "        \n",
      "        driver.quit()\n",
      "\n",
      "    with open('movie_urls.txt','w') as file:\n",
      "        json.dump(movie_list,file, indent=4)\n",
      "\n",
      "    return movie_list\n",
      "    \n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    #run scraper and print completion time\n",
      "    print('Running')\n",
      "    start = time.time()\n",
      "    movie_urls = parse_movie_urls()\n",
      "    end = time.time() - start\n",
      "    print(\"Completed, time: \" + str(end) + \" secs\")\n",
      "51/7:\n",
      "def parse_movie_urls():\n",
      "    '''Creates dictionary of all movies under Browse All of RT using div class = \"mb-movie\"\n",
      "        and storing it as {title:url}'''\n",
      "    \n",
      "    #create dict\n",
      "    movie_list = {}\n",
      "\n",
      "    for browse_url in browse_urls:\n",
      "\n",
      "\n",
      "        #start selenium driver\n",
      "        driver = webdriver.Chrome()\n",
      "        driver.get(browse_url)\n",
      "        \n",
      "        soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
      "        show_count = soup.find('span',attrs={'id':'showing-count'}).text\n",
      "        total_count = int(show_count.split(' ')[3])\n",
      "        \n",
      "        #clicking loop: keep expanding page till it is showing all movies\n",
      "        elem = driver.find_element_by_xpath('//*[@id=\"show-more-btn\"]/button')\n",
      "\n",
      "        while(True):\n",
      "            \n",
      "\n",
      "            #check current count vs total count\n",
      "            soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
      "            show_count = soup.find('span',attrs={'id':'showing-count'}).text\n",
      "\n",
      "            print(show_count)\n",
      "\n",
      "            current_count = int(show_count.split(' ')[1])\n",
      "\n",
      "            #if it has finished clicking, break out of while loop\n",
      "            if(current_count >= (total_count-1)):\n",
      "                break\n",
      "\n",
      "            else:\n",
      "                #continue clicking\n",
      "                try:\n",
      "                    elem.click()\n",
      "                    wait = WebDriverWait(driver, 90)\n",
      "                    #wait until mb-movies is not stale\n",
      "                    # elem = wait.until((float(driver.find_element_by_class_name('mb-movies').get_attribute(\"style\").split('opacity: ')[1][0]) == 0.5))\n",
      "                    # elem = wait.until((float(driver.find_element_by_class_name('mb-movies').get_attribute(\"style\").split('opacity: ')[1][0]) == 1))\n",
      "                    wait = wait.until(text_to_be_not_present_in_element((By.ID,'showing-count'),show_count))\n",
      "                except ElementNotVisibleException:\n",
      "                    break\n",
      "                except TimeoutException:\n",
      "                    break\n",
      "        \n",
      "        print('Scraping now')\n",
      "        \n",
      "        #record each movie title and its url inside dict\n",
      "        soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
      "        movies = soup.find('div', {'class' :\"mb-movies\"})\n",
      "        for movie in movies:\n",
      "            url = movie.find('a',{'class' : \"popoverTrigger\"})['href']\n",
      "            title = movie.find('h3',{'class' : \"movieTitle\"}).text\n",
      "\n",
      "            movie_list[title] = url\n",
      "        \n",
      "        driver.quit()\n",
      "\n",
      "    with open('movie_urls.txt','w') as file:\n",
      "        json.dump(movie_list,file, indent=4)\n",
      "\n",
      "    return movie_list\n",
      "    \n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    #run scraper and print completion time\n",
      "    print('Running')\n",
      "    start = time.time()\n",
      "    movie_urls = parse_movie_urls()\n",
      "    end = time.time() - start\n",
      "    print(\"Completed, time: \" + str(end) + \" secs\")\n",
      "51/8:\n",
      "def parse_movie_urls():\n",
      "    '''Creates dictionary of all movies under Browse All of RT using div class = \"mb-movie\"\n",
      "        and storing it as {title:url}'''\n",
      "    \n",
      "    #create dict\n",
      "    movie_list = {}\n",
      "\n",
      "    for browse_url in browse_urls:\n",
      "\n",
      "\n",
      "        #start selenium driver\n",
      "        driver = webdriver.Chrome()\n",
      "        driver.get(browse_url)\n",
      "        \n",
      "        soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
      "        show_count = soup.find('span',attrs={'id':'showing-count'}).text\n",
      "        total_count = int(show_count.split(' ')[3])\n",
      "        \n",
      "        #clicking loop: keep expanding page till it is showing all movies\n",
      "        elem = driver.find_element_by_xpath('//*[@id=\"show-more-btn\"]/button')\n",
      "\n",
      "        while(True):\n",
      "            \n",
      "\n",
      "            #check current count vs total count\n",
      "            soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
      "            show_count = soup.find('span',attrs={'id':'showing-count'}).text\n",
      "\n",
      "            print(show_count)\n",
      "\n",
      "            current_count = int(show_count.split(' ')[1])\n",
      "\n",
      "            #if it has finished clicking, break out of while loop\n",
      "            if(current_count >= (total_count-1)):\n",
      "                break\n",
      "\n",
      "            else:\n",
      "                #continue clicking\n",
      "                try:\n",
      "                    elem.click()\n",
      "                    wait = WebDriverWait(driver, 90)\n",
      "                    #wait until mb-movies is not stale\n",
      "                    # elem = wait.until((float(driver.find_element_by_class_name('mb-movies').get_attribute(\"style\").split('opacity: ')[1][0]) == 0.5))\n",
      "                    # elem = wait.until((float(driver.find_element_by_class_name('mb-movies').get_attribute(\"style\").split('opacity: ')[1][0]) == 1))\n",
      "                    wait = wait.until(text_to_be_not_present_in_element((By.ID,'showing-count'),show_count))\n",
      "                except ElementNotVisibleException:\n",
      "                    break\n",
      "                except TimeoutException:\n",
      "                    break\n",
      "        \n",
      "        print('Scraping now')\n",
      "        \n",
      "        #record each movie title and its url inside dict\n",
      "        soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
      "        movies = soup.find('div', {'class' :\"mb-movies\"})\n",
      "        for movie in movies:\n",
      "            url = movie.find('a',{'class' : \"popoverTrigger\"})['href']\n",
      "            title = movie.find('h3',{'class' : \"movieTitle\"}).text\n",
      "\n",
      "            movie_list[title] = url\n",
      "        \n",
      "        driver.quit()\n",
      "\n",
      "    with open('movie_urls.txt','w') as file:\n",
      "        json.dump(movie_list,file, indent=4)\n",
      "\n",
      "    return movie_list\n",
      "    \n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    #run scraper and print completion time\n",
      "    print('Running')\n",
      "    start = time.time()\n",
      "    movie_urls = parse_movie_urls()\n",
      "    end = time.time() - start\n",
      "    print(\"Completed, time: \" + str(end) + \" secs\")\n",
      "52/1:\n",
      "from bs4 import BeautifulSoup\n",
      "import time\n",
      "import json\n",
      "from contextlib import contextmanager\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from selenium.common.exceptions import ElementNotVisibleException\n",
      "from selenium.common.exceptions import TimeoutException\n",
      "from selenium.common.exceptions import StaleElementReferenceException\n",
      "\n",
      "base_url = 'https://www.rottentomatoes.com'\n",
      "\n",
      "#split into sections around ~2000 in side\n",
      "browse_urls = ['https://www.rottentomatoes.com/browse/dvd-all/?maxTomato=10&services=amazon;amazon_prime;fandango_now;hbo_go;itunes;netflix_iw;vudu',\n",
      "            'https://www.rottentomatoes.com/browse/dvd-all/?minTomato=11&maxTomato=20&services=amazon;amazon_prime;fandango_now;hbo_go;itunes;netflix_iw;vudu',\n",
      "            'https://www.rottentomatoes.com/browse/dvd-all/?minTomato=21&maxTomato=30&services=amazon;amazon_prime;fandango_now;hbo_go;itunes;netflix_iw;vudu',\n",
      "            'https://www.rottentomatoes.com/browse/dvd-all/?minTomato=31&maxTomato=40&services=amazon;amazon_prime;fandango_now;hbo_go;itunes;netflix_iw;vudu',\n",
      "            'https://www.rottentomatoes.com/browse/dvd-all/?minTomato=41&maxTomato=50&services=amazon;amazon_prime;fandango_now;hbo_go;itunes;netflix_iw;vudu',\n",
      "            'https://www.rottentomatoes.com/browse/dvd-all/?minTomato=51&maxTomato=60&services=amazon;amazon_prime;fandango_now;hbo_go;itunes;netflix_iw;vudu',\n",
      "            'https://www.rottentomatoes.com/browse/dvd-all/?minTomato=61&maxTomato=70&services=amazon;amazon_prime;fandango_now;hbo_go;itunes;netflix_iw;vudu',\n",
      "            'https://www.rottentomatoes.com/browse/dvd-all/?minTomato=71&maxTomato=80&services=amazon;amazon_prime;fandango_now;hbo_go;itunes;netflix_iw;vudu',\n",
      "            'https://www.rottentomatoes.com/browse/dvd-all/?minTomato=81&maxTomato=85&services=amazon;amazon_prime;fandango_now;hbo_go;itunes;netflix_iw;vudu',\n",
      "            'https://www.rottentomatoes.com/browse/dvd-all/?minTomato=86&maxTomato=90&services=amazon;amazon_prime;fandango_now;hbo_go;itunes;netflix_iw;vudu',\n",
      "            'https://www.rottentomatoes.com/browse/dvd-all/?minTomato=91&maxTomato=95&services=amazon;amazon_prime;fandango_now;hbo_go;itunes;netflix_iw;vudu',\n",
      "            'https://www.rottentomatoes.com/browse/dvd-all/?minTomato=96&services=amazon;amazon_prime;fandango_now;hbo_go;itunes;netflix_iw;vudu']\n",
      "\n",
      "class text_to_be_not_present_in_element(object):\n",
      "    \"\"\" An expectation for checking if the given text is present in the\n",
      "    specified element.\n",
      "    locator, text\n",
      "    \"\"\"\n",
      "    def __init__(self, locator, text_):\n",
      "        self.locator = locator\n",
      "        self.text = text_\n",
      "\n",
      "    def __call__(self, driver):\n",
      "        try :\n",
      "            element_text = EC._find_element(driver, self.locator).text\n",
      "            return not(self.text in element_text)\n",
      "        except StaleElementReferenceException:\n",
      "            return False\n",
      "\n",
      "def parse_movie_urls():\n",
      "    '''Creates dictionary of all movies under Browse All of RT using div class = \"mb-movie\"\n",
      "        and storing it as {title:url}'''\n",
      "    \n",
      "    #create dict\n",
      "    movie_list = {}\n",
      "\n",
      "    for browse_url in browse_urls:\n",
      "\n",
      "\n",
      "        #start selenium driver\n",
      "        driver = webdriver.Firefox()\n",
      "        driver.get(browse_url)\n",
      "        \n",
      "        soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
      "        show_count = soup.find('span',attrs={'id':'showing-count'}).text\n",
      "        total_count = int(show_count.split(' ')[3])\n",
      "        \n",
      "        #clicking loop: keep expanding page till it is showing all movies\n",
      "        elem = driver.find_element_by_xpath('//*[@id=\"show-more-btn\"]/button')\n",
      "\n",
      "        while(True):\n",
      "            \n",
      "\n",
      "            #check current count vs total count\n",
      "            soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
      "            show_count = soup.find('span',attrs={'id':'showing-count'}).text\n",
      "\n",
      "            print(show_count)\n",
      "\n",
      "            current_count = int(show_count.split(' ')[1])\n",
      "\n",
      "            #if it has finished clicking, break out of while loop\n",
      "            if(current_count >= (total_count-1)):\n",
      "                break\n",
      "\n",
      "            else:\n",
      "                #continue clicking\n",
      "                try:\n",
      "                    elem.click()\n",
      "                    wait = WebDriverWait(driver, 90)\n",
      "                    #wait until mb-movies is not stale\n",
      "                    # elem = wait.until((float(driver.find_element_by_class_name('mb-movies').get_attribute(\"style\").split('opacity: ')[1][0]) == 0.5))\n",
      "                    # elem = wait.until((float(driver.find_element_by_class_name('mb-movies').get_attribute(\"style\").split('opacity: ')[1][0]) == 1))\n",
      "                    wait = wait.until(text_to_be_not_present_in_element((By.ID,'showing-count'),show_count))\n",
      "                except ElementNotVisibleException:\n",
      "                    break\n",
      "                except TimeoutException:\n",
      "                    break\n",
      "        \n",
      "        print('Scraping now')\n",
      "        \n",
      "        #record each movie title and its url inside dict\n",
      "        soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
      "        movies = soup.find('div', {'class' :\"mb-movies\"})\n",
      "        for movie in movies:\n",
      "            url = movie.find('a',{'class' : \"popoverTrigger\"})['href']\n",
      "            title = movie.find('h3',{'class' : \"movieTitle\"}).text\n",
      "\n",
      "            movie_list[title] = url\n",
      "        \n",
      "        driver.quit()\n",
      "\n",
      "    with open('movie_urls.txt','w') as file:\n",
      "        json.dump(movie_list,file, indent=4)\n",
      "\n",
      "    return movie_list\n",
      "    \n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    #run scraper and print completion time\n",
      "    print('Running')\n",
      "    start = time.time()\n",
      "    movie_urls = parse_movie_urls()\n",
      "    end = time.time() - start\n",
      "    print(\"Completed, time: \" + str(end) + \" secs\")\n",
      "56/1:\n",
      "from selenium import webdriver\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "r = requests.get('https://www.rottentomatoes.com/browse/dvd-streaming-all/?maxTomato=10')\n",
      "\n",
      "soup= bs4.BeautifulSoup(r.text)\n",
      "\n",
      "show_count = soup.find('span',attrs={'id':'count-link'}).text\n",
      "total_count = int(show_count.split(' ')[3])\n",
      "56/2:\n",
      "from selenium import webdriver\n",
      "from bs4 import BeautifulSoup\n",
      "import bs4\n",
      "import requests\n",
      "\n",
      "r = requests.get('https://www.rottentomatoes.com/browse/dvd-streaming-all/?maxTomato=10')\n",
      "\n",
      "soup= bs4.BeautifulSoup(r.text)\n",
      "\n",
      "show_count = soup.find('span',attrs={'id':'count-link'}).text\n",
      "total_count = int(show_count.split(' ')[3])\n",
      "57/1:\n",
      "import os\n",
      "import google.oauth2.credentials\n",
      "import google_auth_oauthlib.flow\n",
      "from googleapiclient.discovery import build\n",
      "from googleapiclient.errors import HttpError\n",
      "from google_auth_oauthlib.flow import InstalledAppFlow\n",
      "\n",
      "SCOPES = ['https://www.googleapis.com/auth/yt-analytics.readonly']\n",
      "\n",
      "API_SERVICE_NAME = 'youtubeAnalytics'\n",
      "API_VERSION = 'v2'\n",
      "CLIENT_SECRETS_FILE = 'CLIENT_SECRETS_FILE.json'\n",
      "def get_service():\n",
      "  flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, SCOPES)\n",
      "  credentials = flow.run_console()\n",
      "  return build(API_SERVICE_NAME, API_VERSION, credentials = credentials)\n",
      "\n",
      "def execute_api_request(client_library_function, **kwargs):\n",
      "  response = client_library_function(\n",
      "    **kwargs\n",
      "  ).execute()\n",
      "\n",
      "  print(response)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  # Disable OAuthlib's HTTPs verification when running locally.\n",
      "  # *DO NOT* leave this option enabled when running in production.\n",
      "  os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'\n",
      "\n",
      "  youtubeAnalytics = get_service()\n",
      "  execute_api_request(\n",
      "      youtubeAnalytics.reports().query,\n",
      "      ids='channel==UCEf_Bc-KVd7onSeifS3py9g',\n",
      "      startDate='2006-03-18',\n",
      "      endDate='2018-11-06',\n",
      "      metrics='estimatedMinutesWatched,views,likes,subscribersGained'\n",
      "      dimensions='day',\n",
      "      sort='day'\n",
      "  )\n",
      "59/1: !python urlscaper.py\n",
      "59/2: !python urlscraper.py\n",
      "60/1: mytable\n",
      "60/2: import requests\n",
      "60/3: url = \"https://www.rottentomatoes.com/top/bestofrt/\"\n",
      "60/4: rottentomatoes = requests.get(url)\n",
      "60/5: rottentomatoes.text\n",
      "60/6: import bs4\n",
      "60/7: mypage = bs4.BeautifulSoup(rottentomatoes.text)\n",
      "60/8: mytable = mypage.select(\"table\")\n",
      "60/9: from bs4 import BeautifulSoup\n",
      "60/10: mytable\n",
      "60/11: link = [a.href.attrs.get('href') for a in mytable.select(\"td.bold a\")]\n",
      "60/12: mytable = mypage.find(\"table\")\n",
      "60/13: from bs4 import BeautifulSoup\n",
      "60/14: mytable\n",
      "60/15: link = [a.href.attrs.get('href') for a in mytable.select(\"td.bold a\")]\n",
      "60/16: [a.href.attrs.get('href') for a in mytable.select(\"td.bold a\")]\n",
      "60/17: help(mytable.find())\n",
      "60/18: mytable = mypage.find(\"table\", class='table')\n",
      "60/19: mytable = mypage.find_all(\"table\", class='table')\n",
      "60/20: mytable = mypage.find_all(\"table\")\n",
      "60/21: mytable\n",
      "60/22: mytable = mypage.find_next('table')\n",
      "60/23: from bs4 import BeautifulSoup\n",
      "60/24: mytable\n",
      "60/25: mytable\n",
      "60/26: mypage = bs4.BeautifulSoup(rottentomatoes.text)\n",
      "60/27: mytable = mypage.find_next('table')\n",
      "60/28: from bs4 import BeautifulSoup\n",
      "60/29: mytable\n",
      "60/30: mytable = mypage.find(\"table\", {\"class\":\"table\"})\n",
      "60/31: mytable\n",
      "60/32: [a.href.attrs.get('href') for a in mytable.select(\"td.bold a\")]\n",
      "60/33: mytable.find_all('a')\n",
      "60/34: mylink = mytable.find_all('a').attrs['href']\n",
      "60/35: mylink = mytable.find_all('a').attrs['href']\n",
      "60/36: mylink = mytable.find_all('a')\n",
      "60/37:\n",
      "links = []\n",
      "for link in mylink:\n",
      "    links.append((mylink.text, mylink.attrs['href']))\n",
      "60/38:\n",
      "links = []\n",
      "for link in mylink:\n",
      "    links.append(mylink.attrs['href'])\n",
      "60/39:\n",
      "links = []\n",
      "for links in mylink:\n",
      "    links.append(mylink.attrs['href'])\n",
      "60/40: mylinks = mytable.find_all('a')\n",
      "60/41: mylinks = mytable.find_all('a')\n",
      "60/42:\n",
      "links = []\n",
      "for mylinks in mylinks:\n",
      "    links.append(mylinks.attrs['href'])\n",
      "60/43:\n",
      "links = []\n",
      "for mylink in mylinks:\n",
      "    links.append(mylinks.attrs['href'])\n",
      "60/44: links\n",
      "60/45: mylinks\n",
      "60/46: mylinks = mytable.find_all('a')\n",
      "60/47: mylinks\n",
      "60/48: mylinks = mytable.select('a')\n",
      "60/49: mylinks\n",
      "60/50: links = [a.attrs.get('href') for a in mytable.select('a')]\n",
      "60/51: links\n",
      "60/52:\n",
      "fulllinks = []\n",
      "for fulllink in fulllinks:\n",
      "    fulllink = 'https://www.rottentomatoes.com%s'.links[]\n",
      "60/53:\n",
      "fulllinks = []\n",
      "for i < 100:\n",
      "    for fulllink in fulllinks:\n",
      "        fulllink = 'https://www.rottentomatoes.com%s'.links[i]\n",
      "60/54:\n",
      "fulllinks = []\n",
      "while i < 100:\n",
      "    for fulllink in fulllinks:\n",
      "        fulllink = 'https://www.rottentomatoes.com%s'.links[i]\n",
      "        i = i+1\n",
      "        fulllinks.append(fulllink)\n",
      "60/55:\n",
      "fulllinks = []\n",
      "i=0\n",
      "while i < 100:\n",
      "    for fulllink in fulllinks:\n",
      "        fulllink = 'https://www.rottentomatoes.com%s'.links[i]\n",
      "        i = i+1\n",
      "        fulllinks.append(fulllink)\n",
      "61/1: import requests\n",
      "61/2: url = \"https://www.rottentomatoes.com/top/bestofrt/\"\n",
      "61/3: rottentomatoes = requests.get(url)\n",
      "61/4: rottentomatoes.text\n",
      "61/5: import bs4\n",
      "61/6: mypage = bs4.BeautifulSoup(rottentomatoes.text)\n",
      "61/7: mytable = mypage.find_next('table')\n",
      "61/8: from bs4 import BeautifulSoup\n",
      "61/9: mytable\n",
      "61/10: [a.href.attrs.get('href') for a in mytable.select(\"td.bold a\")]\n",
      "61/11: mytable = mypage.find(\"table\", {\"class\":\"table\"})\n",
      "61/12: mytable\n",
      "61/13: mylinks = mytable.select('a')\n",
      "61/14: mylinks\n",
      "61/15: links = [a.attrs.get('href') for a in mytable.select('a')]\n",
      "61/16: links\n",
      "61/17:\n",
      "fulllinks = []\n",
      "i=0\n",
      "while i < 100:\n",
      "    for fulllink in fulllinks:\n",
      "        fulllink = 'https://www.rottentomatoes.com%s'.links[i]\n",
      "        i = i+1\n",
      "        fulllinks.append(fulllink)\n",
      "62/1: import requests\n",
      "62/2: url = \"https://www.rottentomatoes.com/top/bestofrt/\"\n",
      "62/3: rottentomatoes = requests.get(url)\n",
      "62/4: rottentomatoes.text\n",
      "62/5: import bs4\n",
      "62/6: mypage = bs4.BeautifulSoup(rottentomatoes.text)\n",
      "62/7: mytable = mypage.find_next('table')\n",
      "62/8: from bs4 import BeautifulSoup\n",
      "62/9: mytable\n",
      "62/10: mytable = mypage.find(\"table\", {\"class\":\"table\"})\n",
      "62/11: mytable\n",
      "62/12: mylinks = mytable.select('a')\n",
      "62/13: mylinks\n",
      "62/14: links = [a.attrs.get('href') for a in mytable.select('a')]\n",
      "62/15: links\n",
      "62/16:\n",
      "fulllinks = []\n",
      "i=0\n",
      "while i < 100:\n",
      "    for fulllink in fulllinks:\n",
      "        fulllink = 'https://www.rottentomatoes.com%s' + links[i]\n",
      "        i = i+1\n",
      "        fulllinks.append(fulllink)\n",
      "63/1: import requests\n",
      "63/2: url = \"https://www.rottentomatoes.com/top/bestofrt/\"\n",
      "63/3: rottentomatoes = requests.get(url)\n",
      "63/4: rottentomatoes.text\n",
      "63/5: import bs4\n",
      "63/6: mypage = bs4.BeautifulSoup(rottentomatoes.text)\n",
      "63/7: mytable = mypage.find_next('table')\n",
      "63/8: from bs4 import BeautifulSoup\n",
      "63/9: mytable\n",
      "63/10: mytable = mypage.find(\"table\", {\"class\":\"table\"})\n",
      "63/11: mytable\n",
      "63/12: mylinks = mytable.select('a')\n",
      "63/13: mylinks\n",
      "63/14: links = [a.attrs.get('href') for a in mytable.select('a')]\n",
      "63/15: 'https://www.rottentomatoes.com' + links[0]\n",
      "63/16:\n",
      "fulllinks = []\n",
      "for link in links:\n",
      "    fulllink = 'https://www.rottentomatoes.com' + link\n",
      "    \n",
      "    fulllinks.append(fulllink)\n",
      "63/17: fulllinks\n",
      "63/18:  open('movie_urls.txt','w') as file\n",
      "63/19:  file = open('movie_urls.txt','w')\n",
      "63/20: file.write(fulllinks)\n",
      "63/21: file.writelines(fulllinks)\n",
      "63/22: file.close()\n",
      "63/23: file.writable(fulllinks)\n",
      "63/24: file.writelines(fulllinks)\n",
      "63/25: file = open('movie_urls.txt','w')\n",
      "63/26: file.writelines(fulllinks)\n",
      "63/27: file.close()\n",
      "63/28: help(file.writelines())\n",
      "63/29: help(file.writelines\n",
      "63/30: help(file.writelines)\n",
      "63/31: help(file.write)\n",
      "63/32: file.close(fulllinks, /)\n",
      "63/33: help(json.dump)\n",
      "63/34:\n",
      "import json\n",
      "import requests\n",
      "63/35: help(json.dump)\n",
      "63/36: file = open('movie_urls.txt','w')\n",
      "63/37: file.writelines(fulllinks)\n",
      "63/38: file.close()\n",
      "64/1:\n",
      "from bs4 import BeautifulSoup\n",
      "from selenium import webdriver\n",
      "import statsmodels.api as sm\n",
      "import locale\n",
      "import json\n",
      "import datetime\n",
      "import pickle\n",
      "locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' )\n",
      "\n",
      "#sequence to initiate webpage for scraping\n",
      "#change url for chromedriver accordingly\n",
      "driver = webdriver.Chrome(\"<url_for_chromedriver_here>/chromedriver\")\n",
      "movie = {}\n",
      "error = 0    #error tracker\n",
      "\n",
      "\n",
      "\"\"\"function for scraping one movie profile\"\"\"\n",
      "def scraper(title, url):\n",
      "#uncomment next line to set url manually \n",
      "#     url = \"https://www.rottentomatoes.com/m/bowfinger\"\n",
      "    driver.get(url)\n",
      "    html = driver.page_source\n",
      "    soup = BeautifulSoup(html, \"lxml\")\n",
      "\n",
      "    try:\n",
      "        \"\"\"All Critics\"\"\"\n",
      "        movie[title][\"all critics\"] = {}\n",
      "        \n",
      "        #All Critics - number\n",
      "        all_critics = soup.find(\"div\", { \"id\" : \"all-critics-numbers\" })\n",
      "\n",
      "        #All Critics - tomatometer score\n",
      "        movie[title][\"all critics\"][\"tomatometer\"] = int(all_critics.find(\"a\", { \"id\" : 'tomato_meter_link' }).text.strip().replace(\"%\", \"\"))\n",
      "\n",
      "        all_critics_2 = all_critics.find(\"div\", { \"id\" : \"scoreStats\" })\n",
      "\n",
      "        #All Critics - average rating\n",
      "        movie[title][\"all critics\"][\"average_rating\"] = float(all_critics_2.find(\"div\", { \"class\" : 'superPageFontColor' }).text.split(\"\\n\")[2].strip().replace(\"/10\", \"\"))\n",
      "        all_critics_info = all_critics_2.findAll(\"span\", { \"class\" : '' })\n",
      "\n",
      "        #All Critics - reviews counted\n",
      "        movie[title][\"all critics\"][\"reviews_counted\"] = int(all_critics_info[0].text)\n",
      "\n",
      "        #All Critics - fresh\n",
      "        movie[title][\"all critics\"][\"fresh\"] = int(all_critics_info[1].text)\n",
      "\n",
      "        #All Critics - rotten\n",
      "        movie[title][\"all critics\"][\"rotten\"] = int(all_critics_info[2].text)\n",
      "\n",
      "        \"\"\"Top Critics - not all movies have sufficient information for Top Critics\"\"\"\n",
      "        find_top = soup.find(\"div\", { \"id\" : \"top-critics-numbers\" }).text\n",
      "        if \"Not Available\" in find_top:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"top critics\"] = {}\n",
      "            \n",
      "            #Top Critics - number\n",
      "            top_critics = soup.find(\"div\", { \"id\" : \"top-critics-numbers\" })\n",
      "\n",
      "            #Top Critics - tomatometer score\n",
      "            movie[title][\"top critics\"][\"tomatometer\"] = int(top_critics.find(\"a\", { \"id\" : 'tomato_meter_link' }).text.strip().replace(\"%\", \"\"))\n",
      "\n",
      "            top_critics_2 = top_critics.find(\"div\", { \"id\" : \"scoreStats\" })\n",
      "\n",
      "            #Top Critics - average rating\n",
      "            movie[title][\"top critics\"][\"average_rating\"] = float(top_critics_2.find(\"div\", { \"class\" : 'superPageFontColor' }).text.split(\"\\n\")[2].strip().replace(\"/10\", \"\"))\n",
      "\n",
      "            top_critics_info = top_critics_2.findAll(\"span\", { \"class\" : '' })\n",
      "\n",
      "            #Top Critics - reviews counted\n",
      "            movie[title][\"top critics\"][\"reviews_counted\"] = int(top_critics_info[0].text)\n",
      "\n",
      "            #Top Critics - fresh\n",
      "            movie[title][\"top critics\"][\"fresh\"] = int(top_critics_info[1].text)  \n",
      "\n",
      "            #Top Critics - rotten\n",
      "            movie[title][\"top critics\"][\"rotten\"] = int(top_critics_info[2].text)\n",
      "\n",
      "\n",
      "        \"\"\"Audience\"\"\"\n",
      "        movie[title][\"audience\"] = {}\n",
      "        \n",
      "        #Audience - score\n",
      "        movie[title][\"audience\"][\"score\"] = int(soup.find(\"div\", { \"class\" : \"meter-value\" }).text.split(\"\\n\")[1].replace(\"%\",\"\"))\n",
      "\n",
      "        audience_info =  soup.find(\"div\", { \"class\" : 'audience-info hidden-xs superPageFontColor' }).text.split(\"\\n\")\n",
      "\n",
      "        #Audience - average rating\n",
      "        movie[title][\"audience\"][\"average rating\"] = float(audience_info[3].strip().replace(\"/5\",\"\"))\n",
      "        #Audience - number\n",
      "        movie[title][\"audience\"][\"number of ratings\"] = int(audience_info[7].strip().replace(\",\", \"\"))\n",
      "\n",
      "        \"\"\"Marketing Information\"\"\"\n",
      "        #Number of Videos\n",
      "        find_videos = soup.find(\"div\", { \"class\" : 'clickForMore viewMoreVideos' })\n",
      "        if find_videos is None:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"number of videos\"] = int(soup.find(\"div\", { \"class\" : 'clickForMore viewMoreVideos' }).text.strip().split(\" \")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
      "        \n",
      "        #Number of Photos\n",
      "        find_photos = soup.find(\"div\", { \"class\" : 'clickForMore viewMorePhotos' })\n",
      "        if find_photos is None:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"number of photos\"] = int(soup.find(\"div\", { \"class\" : 'clickForMore viewMorePhotos' }).text.strip().split(\" \")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
      "        \n",
      "        \n",
      "        \"\"\"General Movie Information\"\"\"\n",
      "        movie_info = soup.find(\"div\", { \"class\" : 'info' })\n",
      "        movie_info_list = movie_info.findAll(\"div\")\n",
      "        \n",
      "        for i in range(0, len(movie_info_list)):\n",
      "            #Age Rating\n",
      "            if \"Rating:\" in str(movie_info_list[i]):\n",
      "                movie[title][\"age rating\"] = movie_info_list[i+1].text.split(\" \")[0]\n",
      "                \n",
      "            #Genres\n",
      "            if \"Genre:\" in str(movie_info_list[i]):\n",
      "                list_of_genres = movie_info_list[i+1].text.split(\",\")\n",
      "                movie[title][\"number of genres\"] = len(list_of_genres)\n",
      "                genres = []\n",
      "                for j in list_of_genres:\n",
      "                    string1 = str(j.strip())\n",
      "                    genres.append(string1)\n",
      "                movie[title][\"genres\"] = genres\n",
      "            \n",
      "            #Directors\n",
      "            if \"Directed By:\" in str(movie_info_list[i]):\n",
      "                list_of_movie_directors = movie_info_list[i+1].text.strip().split(\",\")\n",
      "                movie[title][\"number of directors\"] = len(list_of_movie_directors)\n",
      "                directors = [] \n",
      "                for j in list_of_movie_directors:\n",
      "                    string1 = str(j.strip())\n",
      "                    directors.append(string1)\n",
      "                movie[title][\"directors\"] = directors\n",
      "            \n",
      "            #Writers\n",
      "            if \"Written By:\" in str(movie_info_list[i]):     \n",
      "                list_of_movie_writers = movie_info_list[i+1].text.strip().split(\",\")\n",
      "                movie[title][\"number of writers\"] = len(list_of_movie_writers)\n",
      "                writers = [] \n",
      "                for j in list_of_movie_writers:\n",
      "                    string1 = str(j.strip())\n",
      "                    writers.append(string1)\n",
      "                movie[title][\"writers\"] = writers\n",
      "            \n",
      "            #Release Date\n",
      "            if \"In Theaters:\" in str(movie_info_list[i]): \n",
      "                #Relase Date and Type\n",
      "                release_info = movie_info_list[i+1].text.strip().split(\"\\n\")\n",
      "\n",
      "                #Date in Theatres\n",
      "                string_of_release_date = str(release_info[0].replace(\",\",\"\"))\n",
      "                movie[title][\"release date\"] = datetime.datetime.strptime(string_of_release_date, '%b %d %Y').strftime('%d%m%y')\n",
      "\n",
      "                #Release Type\n",
      "                if release_info[1] is not None:\n",
      "                    movie[title][\"release type\"] = release_info[1].strip()\n",
      "\n",
      "            #DVD Release Date\n",
      "            if \"On DVD:\" in str(movie_info_list[i]):    \n",
      "                string_of_DVD_date = str(movie_info_list[i+1].text.strip().replace(\",\",\"\"))\n",
      "                movie[title][\"DVD date\"] = datetime.datetime.strptime(string_of_DVD_date, '%b %d %Y').strftime('%d%m%y')\n",
      "            \n",
      "            #Box Office\n",
      "            if \"Box Office:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"box office revenue\"] = float(movie_info_list[i+1].text.replace(\",\",\"\").replace(\"$\",\"\"))\n",
      "\n",
      "            #Runtime\n",
      "            if \"Runtime:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"runtime\"] = int(movie_info_list[i+1].text.strip().replace(\" minutes\",\"\"))\n",
      "            \n",
      "            #Studio\n",
      "            if \"Studio:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"studio\"] = str(movie_info_list[i+1].text.strip())            \n",
      "                \n",
      "\n",
      "        #Cast Names\n",
      "        cast_info = soup.find(\"div\", { \"class\" : 'castSection '})\n",
      "        if cast_info is None:\n",
      "            movie[title][\"number of cast\"] = 0\n",
      "        else:\n",
      "            cast = []\n",
      "            counter = 0\n",
      "\n",
      "            for i in cast_info.findAll(\"a\", { \"class\" : 'unstyled articleLink'}):\n",
      "                string1 = i.text.strip()\n",
      "                if string1 == \"Show More Cast\":\n",
      "                    continue\n",
      "                else:\n",
      "                    cast.append(string1)\n",
      "                    counter += 1\n",
      "            movie[title][\"cast\"] = cast\n",
      "            movie[title][\"number of cast\"] = counter\n",
      "    \n",
      "    except Exception as e:\n",
      "        print(title)\n",
      "        #movie.pop(title, None)\n",
      "        \n",
      "\"\"\"main function interates through movie_urls.txt and calls scraper function for each url\"\"\"\n",
      "def main_scraper():\n",
      "    base_url = 'https://www.rottentomatoes.com'\n",
      "    \n",
      "    with open('movie_urls.txt') as data_file:    \n",
      "        data = json.load(data_file)\n",
      "    \n",
      "    for title in data:\n",
      "        movie[str(title)] = {}\n",
      "        add_on = data[str(title)]\n",
      "        url = base_url + add_on\n",
      "        scraper(title, url)\n",
      "            \n",
      "main_scraper()\n",
      "\n",
      "\"\"\"uncomment next 2 lines to save dictionary in pickle file - binary\"\"\"\n",
      "#with open('movie_data.pkl', 'wb') as f:\n",
      "#    pickle.dump(movie, f, pickle.HIGHEST_PROTOCOL)\n",
      "64/2: !pip install statsmodel\n",
      "64/3: !pip install statsmodels\n",
      "64/4:\n",
      "from bs4 import BeautifulSoup\n",
      "from selenium import webdriver\n",
      "import statsmodels.api as sm\n",
      "import locale\n",
      "import json\n",
      "import datetime\n",
      "import pickle\n",
      "locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' )\n",
      "\n",
      "#sequence to initiate webpage for scraping\n",
      "#change url for chromedriver accordingly\n",
      "driver = webdriver.Chrome(\"<url_for_chromedriver_here>/chromedriver\")\n",
      "movie = {}\n",
      "error = 0    #error tracker\n",
      "\n",
      "\n",
      "\"\"\"function for scraping one movie profile\"\"\"\n",
      "def scraper(title, url):\n",
      "#uncomment next line to set url manually \n",
      "#     url = \"https://www.rottentomatoes.com/m/bowfinger\"\n",
      "    driver.get(url)\n",
      "    html = driver.page_source\n",
      "    soup = BeautifulSoup(html, \"lxml\")\n",
      "\n",
      "    try:\n",
      "        \"\"\"All Critics\"\"\"\n",
      "        movie[title][\"all critics\"] = {}\n",
      "        \n",
      "        #All Critics - number\n",
      "        all_critics = soup.find(\"div\", { \"id\" : \"all-critics-numbers\" })\n",
      "\n",
      "        #All Critics - tomatometer score\n",
      "        movie[title][\"all critics\"][\"tomatometer\"] = int(all_critics.find(\"a\", { \"id\" : 'tomato_meter_link' }).text.strip().replace(\"%\", \"\"))\n",
      "\n",
      "        all_critics_2 = all_critics.find(\"div\", { \"id\" : \"scoreStats\" })\n",
      "\n",
      "        #All Critics - average rating\n",
      "        movie[title][\"all critics\"][\"average_rating\"] = float(all_critics_2.find(\"div\", { \"class\" : 'superPageFontColor' }).text.split(\"\\n\")[2].strip().replace(\"/10\", \"\"))\n",
      "        all_critics_info = all_critics_2.findAll(\"span\", { \"class\" : '' })\n",
      "\n",
      "        #All Critics - reviews counted\n",
      "        movie[title][\"all critics\"][\"reviews_counted\"] = int(all_critics_info[0].text)\n",
      "\n",
      "        #All Critics - fresh\n",
      "        movie[title][\"all critics\"][\"fresh\"] = int(all_critics_info[1].text)\n",
      "\n",
      "        #All Critics - rotten\n",
      "        movie[title][\"all critics\"][\"rotten\"] = int(all_critics_info[2].text)\n",
      "\n",
      "        \"\"\"Top Critics - not all movies have sufficient information for Top Critics\"\"\"\n",
      "        find_top = soup.find(\"div\", { \"id\" : \"top-critics-numbers\" }).text\n",
      "        if \"Not Available\" in find_top:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"top critics\"] = {}\n",
      "            \n",
      "            #Top Critics - number\n",
      "            top_critics = soup.find(\"div\", { \"id\" : \"top-critics-numbers\" })\n",
      "\n",
      "            #Top Critics - tomatometer score\n",
      "            movie[title][\"top critics\"][\"tomatometer\"] = int(top_critics.find(\"a\", { \"id\" : 'tomato_meter_link' }).text.strip().replace(\"%\", \"\"))\n",
      "\n",
      "            top_critics_2 = top_critics.find(\"div\", { \"id\" : \"scoreStats\" })\n",
      "\n",
      "            #Top Critics - average rating\n",
      "            movie[title][\"top critics\"][\"average_rating\"] = float(top_critics_2.find(\"div\", { \"class\" : 'superPageFontColor' }).text.split(\"\\n\")[2].strip().replace(\"/10\", \"\"))\n",
      "\n",
      "            top_critics_info = top_critics_2.findAll(\"span\", { \"class\" : '' })\n",
      "\n",
      "            #Top Critics - reviews counted\n",
      "            movie[title][\"top critics\"][\"reviews_counted\"] = int(top_critics_info[0].text)\n",
      "\n",
      "            #Top Critics - fresh\n",
      "            movie[title][\"top critics\"][\"fresh\"] = int(top_critics_info[1].text)  \n",
      "\n",
      "            #Top Critics - rotten\n",
      "            movie[title][\"top critics\"][\"rotten\"] = int(top_critics_info[2].text)\n",
      "\n",
      "\n",
      "        \"\"\"Audience\"\"\"\n",
      "        movie[title][\"audience\"] = {}\n",
      "        \n",
      "        #Audience - score\n",
      "        movie[title][\"audience\"][\"score\"] = int(soup.find(\"div\", { \"class\" : \"meter-value\" }).text.split(\"\\n\")[1].replace(\"%\",\"\"))\n",
      "\n",
      "        audience_info =  soup.find(\"div\", { \"class\" : 'audience-info hidden-xs superPageFontColor' }).text.split(\"\\n\")\n",
      "\n",
      "        #Audience - average rating\n",
      "        movie[title][\"audience\"][\"average rating\"] = float(audience_info[3].strip().replace(\"/5\",\"\"))\n",
      "        #Audience - number\n",
      "        movie[title][\"audience\"][\"number of ratings\"] = int(audience_info[7].strip().replace(\",\", \"\"))\n",
      "\n",
      "        \"\"\"Marketing Information\"\"\"\n",
      "        #Number of Videos\n",
      "        find_videos = soup.find(\"div\", { \"class\" : 'clickForMore viewMoreVideos' })\n",
      "        if find_videos is None:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"number of videos\"] = int(soup.find(\"div\", { \"class\" : 'clickForMore viewMoreVideos' }).text.strip().split(\" \")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
      "        \n",
      "        #Number of Photos\n",
      "        find_photos = soup.find(\"div\", { \"class\" : 'clickForMore viewMorePhotos' })\n",
      "        if find_photos is None:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"number of photos\"] = int(soup.find(\"div\", { \"class\" : 'clickForMore viewMorePhotos' }).text.strip().split(\" \")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
      "        \n",
      "        \n",
      "        \"\"\"General Movie Information\"\"\"\n",
      "        movie_info = soup.find(\"div\", { \"class\" : 'info' })\n",
      "        movie_info_list = movie_info.findAll(\"div\")\n",
      "        \n",
      "        for i in range(0, len(movie_info_list)):\n",
      "            #Age Rating\n",
      "            if \"Rating:\" in str(movie_info_list[i]):\n",
      "                movie[title][\"age rating\"] = movie_info_list[i+1].text.split(\" \")[0]\n",
      "                \n",
      "            #Genres\n",
      "            if \"Genre:\" in str(movie_info_list[i]):\n",
      "                list_of_genres = movie_info_list[i+1].text.split(\",\")\n",
      "                movie[title][\"number of genres\"] = len(list_of_genres)\n",
      "                genres = []\n",
      "                for j in list_of_genres:\n",
      "                    string1 = str(j.strip())\n",
      "                    genres.append(string1)\n",
      "                movie[title][\"genres\"] = genres\n",
      "            \n",
      "            #Directors\n",
      "            if \"Directed By:\" in str(movie_info_list[i]):\n",
      "                list_of_movie_directors = movie_info_list[i+1].text.strip().split(\",\")\n",
      "                movie[title][\"number of directors\"] = len(list_of_movie_directors)\n",
      "                directors = [] \n",
      "                for j in list_of_movie_directors:\n",
      "                    string1 = str(j.strip())\n",
      "                    directors.append(string1)\n",
      "                movie[title][\"directors\"] = directors\n",
      "            \n",
      "            #Writers\n",
      "            if \"Written By:\" in str(movie_info_list[i]):     \n",
      "                list_of_movie_writers = movie_info_list[i+1].text.strip().split(\",\")\n",
      "                movie[title][\"number of writers\"] = len(list_of_movie_writers)\n",
      "                writers = [] \n",
      "                for j in list_of_movie_writers:\n",
      "                    string1 = str(j.strip())\n",
      "                    writers.append(string1)\n",
      "                movie[title][\"writers\"] = writers\n",
      "            \n",
      "            #Release Date\n",
      "            if \"In Theaters:\" in str(movie_info_list[i]): \n",
      "                #Relase Date and Type\n",
      "                release_info = movie_info_list[i+1].text.strip().split(\"\\n\")\n",
      "\n",
      "                #Date in Theatres\n",
      "                string_of_release_date = str(release_info[0].replace(\",\",\"\"))\n",
      "                movie[title][\"release date\"] = datetime.datetime.strptime(string_of_release_date, '%b %d %Y').strftime('%d%m%y')\n",
      "\n",
      "                #Release Type\n",
      "                if release_info[1] is not None:\n",
      "                    movie[title][\"release type\"] = release_info[1].strip()\n",
      "\n",
      "            #DVD Release Date\n",
      "            if \"On DVD:\" in str(movie_info_list[i]):    \n",
      "                string_of_DVD_date = str(movie_info_list[i+1].text.strip().replace(\",\",\"\"))\n",
      "                movie[title][\"DVD date\"] = datetime.datetime.strptime(string_of_DVD_date, '%b %d %Y').strftime('%d%m%y')\n",
      "            \n",
      "            #Box Office\n",
      "            if \"Box Office:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"box office revenue\"] = float(movie_info_list[i+1].text.replace(\",\",\"\").replace(\"$\",\"\"))\n",
      "\n",
      "            #Runtime\n",
      "            if \"Runtime:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"runtime\"] = int(movie_info_list[i+1].text.strip().replace(\" minutes\",\"\"))\n",
      "            \n",
      "            #Studio\n",
      "            if \"Studio:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"studio\"] = str(movie_info_list[i+1].text.strip())            \n",
      "                \n",
      "\n",
      "        #Cast Names\n",
      "        cast_info = soup.find(\"div\", { \"class\" : 'castSection '})\n",
      "        if cast_info is None:\n",
      "            movie[title][\"number of cast\"] = 0\n",
      "        else:\n",
      "            cast = []\n",
      "            counter = 0\n",
      "\n",
      "            for i in cast_info.findAll(\"a\", { \"class\" : 'unstyled articleLink'}):\n",
      "                string1 = i.text.strip()\n",
      "                if string1 == \"Show More Cast\":\n",
      "                    continue\n",
      "                else:\n",
      "                    cast.append(string1)\n",
      "                    counter += 1\n",
      "            movie[title][\"cast\"] = cast\n",
      "            movie[title][\"number of cast\"] = counter\n",
      "    \n",
      "    except Exception as e:\n",
      "        print(title)\n",
      "        #movie.pop(title, None)\n",
      "        \n",
      "\"\"\"main function interates through movie_urls.txt and calls scraper function for each url\"\"\"\n",
      "def main_scraper():\n",
      "    base_url = 'https://www.rottentomatoes.com'\n",
      "    \n",
      "    with open('movie_urls.txt') as data_file:    \n",
      "        data = json.load(data_file)\n",
      "    \n",
      "    for title in data:\n",
      "        movie[str(title)] = {}\n",
      "        add_on = data[str(title)]\n",
      "        url = base_url + add_on\n",
      "        scraper(title, url)\n",
      "            \n",
      "main_scraper()\n",
      "\n",
      "\"\"\"uncomment next 2 lines to save dictionary in pickle file - binary\"\"\"\n",
      "#with open('movie_data.pkl', 'wb') as f:\n",
      "#    pickle.dump(movie, f, pickle.HIGHEST_PROTOCOL)\n",
      "64/5:\n",
      "!pip install statsmodels\n",
      "!pip install -U scipy\n",
      "64/6:\n",
      "from bs4 import BeautifulSoup\n",
      "from selenium import webdriver\n",
      "import statsmodels.api as sm\n",
      "import locale\n",
      "import json\n",
      "import datetime\n",
      "import pickle\n",
      "locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' )\n",
      "\n",
      "#sequence to initiate webpage for scraping\n",
      "#change url for chromedriver accordingly\n",
      "driver = webdriver.Chrome(\"<url_for_chromedriver_here>/chromedriver\")\n",
      "movie = {}\n",
      "error = 0    #error tracker\n",
      "\n",
      "\n",
      "\"\"\"function for scraping one movie profile\"\"\"\n",
      "def scraper(title, url):\n",
      "#uncomment next line to set url manually \n",
      "#     url = \"https://www.rottentomatoes.com/m/bowfinger\"\n",
      "    driver.get(url)\n",
      "    html = driver.page_source\n",
      "    soup = BeautifulSoup(html, \"lxml\")\n",
      "\n",
      "    try:\n",
      "        \"\"\"All Critics\"\"\"\n",
      "        movie[title][\"all critics\"] = {}\n",
      "        \n",
      "        #All Critics - number\n",
      "        all_critics = soup.find(\"div\", { \"id\" : \"all-critics-numbers\" })\n",
      "\n",
      "        #All Critics - tomatometer score\n",
      "        movie[title][\"all critics\"][\"tomatometer\"] = int(all_critics.find(\"a\", { \"id\" : 'tomato_meter_link' }).text.strip().replace(\"%\", \"\"))\n",
      "\n",
      "        all_critics_2 = all_critics.find(\"div\", { \"id\" : \"scoreStats\" })\n",
      "\n",
      "        #All Critics - average rating\n",
      "        movie[title][\"all critics\"][\"average_rating\"] = float(all_critics_2.find(\"div\", { \"class\" : 'superPageFontColor' }).text.split(\"\\n\")[2].strip().replace(\"/10\", \"\"))\n",
      "        all_critics_info = all_critics_2.findAll(\"span\", { \"class\" : '' })\n",
      "\n",
      "        #All Critics - reviews counted\n",
      "        movie[title][\"all critics\"][\"reviews_counted\"] = int(all_critics_info[0].text)\n",
      "\n",
      "        #All Critics - fresh\n",
      "        movie[title][\"all critics\"][\"fresh\"] = int(all_critics_info[1].text)\n",
      "\n",
      "        #All Critics - rotten\n",
      "        movie[title][\"all critics\"][\"rotten\"] = int(all_critics_info[2].text)\n",
      "\n",
      "        \"\"\"Top Critics - not all movies have sufficient information for Top Critics\"\"\"\n",
      "        find_top = soup.find(\"div\", { \"id\" : \"top-critics-numbers\" }).text\n",
      "        if \"Not Available\" in find_top:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"top critics\"] = {}\n",
      "            \n",
      "            #Top Critics - number\n",
      "            top_critics = soup.find(\"div\", { \"id\" : \"top-critics-numbers\" })\n",
      "\n",
      "            #Top Critics - tomatometer score\n",
      "            movie[title][\"top critics\"][\"tomatometer\"] = int(top_critics.find(\"a\", { \"id\" : 'tomato_meter_link' }).text.strip().replace(\"%\", \"\"))\n",
      "\n",
      "            top_critics_2 = top_critics.find(\"div\", { \"id\" : \"scoreStats\" })\n",
      "\n",
      "            #Top Critics - average rating\n",
      "            movie[title][\"top critics\"][\"average_rating\"] = float(top_critics_2.find(\"div\", { \"class\" : 'superPageFontColor' }).text.split(\"\\n\")[2].strip().replace(\"/10\", \"\"))\n",
      "\n",
      "            top_critics_info = top_critics_2.findAll(\"span\", { \"class\" : '' })\n",
      "\n",
      "            #Top Critics - reviews counted\n",
      "            movie[title][\"top critics\"][\"reviews_counted\"] = int(top_critics_info[0].text)\n",
      "\n",
      "            #Top Critics - fresh\n",
      "            movie[title][\"top critics\"][\"fresh\"] = int(top_critics_info[1].text)  \n",
      "\n",
      "            #Top Critics - rotten\n",
      "            movie[title][\"top critics\"][\"rotten\"] = int(top_critics_info[2].text)\n",
      "\n",
      "\n",
      "        \"\"\"Audience\"\"\"\n",
      "        movie[title][\"audience\"] = {}\n",
      "        \n",
      "        #Audience - score\n",
      "        movie[title][\"audience\"][\"score\"] = int(soup.find(\"div\", { \"class\" : \"meter-value\" }).text.split(\"\\n\")[1].replace(\"%\",\"\"))\n",
      "\n",
      "        audience_info =  soup.find(\"div\", { \"class\" : 'audience-info hidden-xs superPageFontColor' }).text.split(\"\\n\")\n",
      "\n",
      "        #Audience - average rating\n",
      "        movie[title][\"audience\"][\"average rating\"] = float(audience_info[3].strip().replace(\"/5\",\"\"))\n",
      "        #Audience - number\n",
      "        movie[title][\"audience\"][\"number of ratings\"] = int(audience_info[7].strip().replace(\",\", \"\"))\n",
      "\n",
      "        \"\"\"Marketing Information\"\"\"\n",
      "        #Number of Videos\n",
      "        find_videos = soup.find(\"div\", { \"class\" : 'clickForMore viewMoreVideos' })\n",
      "        if find_videos is None:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"number of videos\"] = int(soup.find(\"div\", { \"class\" : 'clickForMore viewMoreVideos' }).text.strip().split(\" \")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
      "        \n",
      "        #Number of Photos\n",
      "        find_photos = soup.find(\"div\", { \"class\" : 'clickForMore viewMorePhotos' })\n",
      "        if find_photos is None:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"number of photos\"] = int(soup.find(\"div\", { \"class\" : 'clickForMore viewMorePhotos' }).text.strip().split(\" \")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
      "        \n",
      "        \n",
      "        \"\"\"General Movie Information\"\"\"\n",
      "        movie_info = soup.find(\"div\", { \"class\" : 'info' })\n",
      "        movie_info_list = movie_info.findAll(\"div\")\n",
      "        \n",
      "        for i in range(0, len(movie_info_list)):\n",
      "            #Age Rating\n",
      "            if \"Rating:\" in str(movie_info_list[i]):\n",
      "                movie[title][\"age rating\"] = movie_info_list[i+1].text.split(\" \")[0]\n",
      "                \n",
      "            #Genres\n",
      "            if \"Genre:\" in str(movie_info_list[i]):\n",
      "                list_of_genres = movie_info_list[i+1].text.split(\",\")\n",
      "                movie[title][\"number of genres\"] = len(list_of_genres)\n",
      "                genres = []\n",
      "                for j in list_of_genres:\n",
      "                    string1 = str(j.strip())\n",
      "                    genres.append(string1)\n",
      "                movie[title][\"genres\"] = genres\n",
      "            \n",
      "            #Directors\n",
      "            if \"Directed By:\" in str(movie_info_list[i]):\n",
      "                list_of_movie_directors = movie_info_list[i+1].text.strip().split(\",\")\n",
      "                movie[title][\"number of directors\"] = len(list_of_movie_directors)\n",
      "                directors = [] \n",
      "                for j in list_of_movie_directors:\n",
      "                    string1 = str(j.strip())\n",
      "                    directors.append(string1)\n",
      "                movie[title][\"directors\"] = directors\n",
      "            \n",
      "            #Writers\n",
      "            if \"Written By:\" in str(movie_info_list[i]):     \n",
      "                list_of_movie_writers = movie_info_list[i+1].text.strip().split(\",\")\n",
      "                movie[title][\"number of writers\"] = len(list_of_movie_writers)\n",
      "                writers = [] \n",
      "                for j in list_of_movie_writers:\n",
      "                    string1 = str(j.strip())\n",
      "                    writers.append(string1)\n",
      "                movie[title][\"writers\"] = writers\n",
      "            \n",
      "            #Release Date\n",
      "            if \"In Theaters:\" in str(movie_info_list[i]): \n",
      "                #Relase Date and Type\n",
      "                release_info = movie_info_list[i+1].text.strip().split(\"\\n\")\n",
      "\n",
      "                #Date in Theatres\n",
      "                string_of_release_date = str(release_info[0].replace(\",\",\"\"))\n",
      "                movie[title][\"release date\"] = datetime.datetime.strptime(string_of_release_date, '%b %d %Y').strftime('%d%m%y')\n",
      "\n",
      "                #Release Type\n",
      "                if release_info[1] is not None:\n",
      "                    movie[title][\"release type\"] = release_info[1].strip()\n",
      "\n",
      "            #DVD Release Date\n",
      "            if \"On DVD:\" in str(movie_info_list[i]):    \n",
      "                string_of_DVD_date = str(movie_info_list[i+1].text.strip().replace(\",\",\"\"))\n",
      "                movie[title][\"DVD date\"] = datetime.datetime.strptime(string_of_DVD_date, '%b %d %Y').strftime('%d%m%y')\n",
      "            \n",
      "            #Box Office\n",
      "            if \"Box Office:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"box office revenue\"] = float(movie_info_list[i+1].text.replace(\",\",\"\").replace(\"$\",\"\"))\n",
      "\n",
      "            #Runtime\n",
      "            if \"Runtime:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"runtime\"] = int(movie_info_list[i+1].text.strip().replace(\" minutes\",\"\"))\n",
      "            \n",
      "            #Studio\n",
      "            if \"Studio:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"studio\"] = str(movie_info_list[i+1].text.strip())            \n",
      "                \n",
      "\n",
      "        #Cast Names\n",
      "        cast_info = soup.find(\"div\", { \"class\" : 'castSection '})\n",
      "        if cast_info is None:\n",
      "            movie[title][\"number of cast\"] = 0\n",
      "        else:\n",
      "            cast = []\n",
      "            counter = 0\n",
      "\n",
      "            for i in cast_info.findAll(\"a\", { \"class\" : 'unstyled articleLink'}):\n",
      "                string1 = i.text.strip()\n",
      "                if string1 == \"Show More Cast\":\n",
      "                    continue\n",
      "                else:\n",
      "                    cast.append(string1)\n",
      "                    counter += 1\n",
      "            movie[title][\"cast\"] = cast\n",
      "            movie[title][\"number of cast\"] = counter\n",
      "    \n",
      "    except Exception as e:\n",
      "        print(title)\n",
      "        #movie.pop(title, None)\n",
      "        \n",
      "\"\"\"main function interates through movie_urls.txt and calls scraper function for each url\"\"\"\n",
      "def main_scraper():\n",
      "    base_url = 'https://www.rottentomatoes.com'\n",
      "    \n",
      "    with open('movie_urls.txt') as data_file:    \n",
      "        data = json.load(data_file)\n",
      "    \n",
      "    for title in data:\n",
      "        movie[str(title)] = {}\n",
      "        add_on = data[str(title)]\n",
      "        url = base_url + add_on\n",
      "        scraper(title, url)\n",
      "            \n",
      "main_scraper()\n",
      "\n",
      "\"\"\"uncomment next 2 lines to save dictionary in pickle file - binary\"\"\"\n",
      "#with open('movie_data.pkl', 'wb') as f:\n",
      "#    pickle.dump(movie, f, pickle.HIGHEST_PROTOCOL)\n",
      "64/7:\n",
      "from bs4 import BeautifulSoup\n",
      "from selenium import webdriver\n",
      "import statsmodels.api as sm\n",
      "import locale\n",
      "import json\n",
      "import datetime\n",
      "import pickle\n",
      "locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' )\n",
      "\n",
      "#sequence to initiate webpage for scraping\n",
      "#change url for chromedriver accordingly\n",
      "driver = webdriver.Chrome(\"C:/Users/burni/Desktop/python\")\n",
      "movie = {}\n",
      "error = 0    #error tracker\n",
      "\n",
      "\n",
      "\"\"\"function for scraping one movie profile\"\"\"\n",
      "def scraper(title, url):\n",
      "#uncomment next line to set url manually \n",
      "#     url = \"https://www.rottentomatoes.com/m/bowfinger\"\n",
      "    driver.get(url)\n",
      "    html = driver.page_source\n",
      "    soup = BeautifulSoup(html, \"lxml\")\n",
      "\n",
      "    try:\n",
      "        \"\"\"All Critics\"\"\"\n",
      "        movie[title][\"all critics\"] = {}\n",
      "        \n",
      "        #All Critics - number\n",
      "        all_critics = soup.find(\"div\", { \"id\" : \"all-critics-numbers\" })\n",
      "\n",
      "        #All Critics - tomatometer score\n",
      "        movie[title][\"all critics\"][\"tomatometer\"] = int(all_critics.find(\"a\", { \"id\" : 'tomato_meter_link' }).text.strip().replace(\"%\", \"\"))\n",
      "\n",
      "        all_critics_2 = all_critics.find(\"div\", { \"id\" : \"scoreStats\" })\n",
      "\n",
      "        #All Critics - average rating\n",
      "        movie[title][\"all critics\"][\"average_rating\"] = float(all_critics_2.find(\"div\", { \"class\" : 'superPageFontColor' }).text.split(\"\\n\")[2].strip().replace(\"/10\", \"\"))\n",
      "        all_critics_info = all_critics_2.findAll(\"span\", { \"class\" : '' })\n",
      "\n",
      "        #All Critics - reviews counted\n",
      "        movie[title][\"all critics\"][\"reviews_counted\"] = int(all_critics_info[0].text)\n",
      "\n",
      "        #All Critics - fresh\n",
      "        movie[title][\"all critics\"][\"fresh\"] = int(all_critics_info[1].text)\n",
      "\n",
      "        #All Critics - rotten\n",
      "        movie[title][\"all critics\"][\"rotten\"] = int(all_critics_info[2].text)\n",
      "\n",
      "        \"\"\"Top Critics - not all movies have sufficient information for Top Critics\"\"\"\n",
      "        find_top = soup.find(\"div\", { \"id\" : \"top-critics-numbers\" }).text\n",
      "        if \"Not Available\" in find_top:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"top critics\"] = {}\n",
      "            \n",
      "            #Top Critics - number\n",
      "            top_critics = soup.find(\"div\", { \"id\" : \"top-critics-numbers\" })\n",
      "\n",
      "            #Top Critics - tomatometer score\n",
      "            movie[title][\"top critics\"][\"tomatometer\"] = int(top_critics.find(\"a\", { \"id\" : 'tomato_meter_link' }).text.strip().replace(\"%\", \"\"))\n",
      "\n",
      "            top_critics_2 = top_critics.find(\"div\", { \"id\" : \"scoreStats\" })\n",
      "\n",
      "            #Top Critics - average rating\n",
      "            movie[title][\"top critics\"][\"average_rating\"] = float(top_critics_2.find(\"div\", { \"class\" : 'superPageFontColor' }).text.split(\"\\n\")[2].strip().replace(\"/10\", \"\"))\n",
      "\n",
      "            top_critics_info = top_critics_2.findAll(\"span\", { \"class\" : '' })\n",
      "\n",
      "            #Top Critics - reviews counted\n",
      "            movie[title][\"top critics\"][\"reviews_counted\"] = int(top_critics_info[0].text)\n",
      "\n",
      "            #Top Critics - fresh\n",
      "            movie[title][\"top critics\"][\"fresh\"] = int(top_critics_info[1].text)  \n",
      "\n",
      "            #Top Critics - rotten\n",
      "            movie[title][\"top critics\"][\"rotten\"] = int(top_critics_info[2].text)\n",
      "\n",
      "\n",
      "        \"\"\"Audience\"\"\"\n",
      "        movie[title][\"audience\"] = {}\n",
      "        \n",
      "        #Audience - score\n",
      "        movie[title][\"audience\"][\"score\"] = int(soup.find(\"div\", { \"class\" : \"meter-value\" }).text.split(\"\\n\")[1].replace(\"%\",\"\"))\n",
      "\n",
      "        audience_info =  soup.find(\"div\", { \"class\" : 'audience-info hidden-xs superPageFontColor' }).text.split(\"\\n\")\n",
      "\n",
      "        #Audience - average rating\n",
      "        movie[title][\"audience\"][\"average rating\"] = float(audience_info[3].strip().replace(\"/5\",\"\"))\n",
      "        #Audience - number\n",
      "        movie[title][\"audience\"][\"number of ratings\"] = int(audience_info[7].strip().replace(\",\", \"\"))\n",
      "\n",
      "        \"\"\"Marketing Information\"\"\"\n",
      "        #Number of Videos\n",
      "        find_videos = soup.find(\"div\", { \"class\" : 'clickForMore viewMoreVideos' })\n",
      "        if find_videos is None:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"number of videos\"] = int(soup.find(\"div\", { \"class\" : 'clickForMore viewMoreVideos' }).text.strip().split(\" \")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
      "        \n",
      "        #Number of Photos\n",
      "        find_photos = soup.find(\"div\", { \"class\" : 'clickForMore viewMorePhotos' })\n",
      "        if find_photos is None:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"number of photos\"] = int(soup.find(\"div\", { \"class\" : 'clickForMore viewMorePhotos' }).text.strip().split(\" \")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
      "        \n",
      "        \n",
      "        \"\"\"General Movie Information\"\"\"\n",
      "        movie_info = soup.find(\"div\", { \"class\" : 'info' })\n",
      "        movie_info_list = movie_info.findAll(\"div\")\n",
      "        \n",
      "        for i in range(0, len(movie_info_list)):\n",
      "            #Age Rating\n",
      "            if \"Rating:\" in str(movie_info_list[i]):\n",
      "                movie[title][\"age rating\"] = movie_info_list[i+1].text.split(\" \")[0]\n",
      "                \n",
      "            #Genres\n",
      "            if \"Genre:\" in str(movie_info_list[i]):\n",
      "                list_of_genres = movie_info_list[i+1].text.split(\",\")\n",
      "                movie[title][\"number of genres\"] = len(list_of_genres)\n",
      "                genres = []\n",
      "                for j in list_of_genres:\n",
      "                    string1 = str(j.strip())\n",
      "                    genres.append(string1)\n",
      "                movie[title][\"genres\"] = genres\n",
      "            \n",
      "            #Directors\n",
      "            if \"Directed By:\" in str(movie_info_list[i]):\n",
      "                list_of_movie_directors = movie_info_list[i+1].text.strip().split(\",\")\n",
      "                movie[title][\"number of directors\"] = len(list_of_movie_directors)\n",
      "                directors = [] \n",
      "                for j in list_of_movie_directors:\n",
      "                    string1 = str(j.strip())\n",
      "                    directors.append(string1)\n",
      "                movie[title][\"directors\"] = directors\n",
      "            \n",
      "            #Writers\n",
      "            if \"Written By:\" in str(movie_info_list[i]):     \n",
      "                list_of_movie_writers = movie_info_list[i+1].text.strip().split(\",\")\n",
      "                movie[title][\"number of writers\"] = len(list_of_movie_writers)\n",
      "                writers = [] \n",
      "                for j in list_of_movie_writers:\n",
      "                    string1 = str(j.strip())\n",
      "                    writers.append(string1)\n",
      "                movie[title][\"writers\"] = writers\n",
      "            \n",
      "            #Release Date\n",
      "            if \"In Theaters:\" in str(movie_info_list[i]): \n",
      "                #Relase Date and Type\n",
      "                release_info = movie_info_list[i+1].text.strip().split(\"\\n\")\n",
      "\n",
      "                #Date in Theatres\n",
      "                string_of_release_date = str(release_info[0].replace(\",\",\"\"))\n",
      "                movie[title][\"release date\"] = datetime.datetime.strptime(string_of_release_date, '%b %d %Y').strftime('%d%m%y')\n",
      "\n",
      "                #Release Type\n",
      "                if release_info[1] is not None:\n",
      "                    movie[title][\"release type\"] = release_info[1].strip()\n",
      "\n",
      "            #DVD Release Date\n",
      "            if \"On DVD:\" in str(movie_info_list[i]):    \n",
      "                string_of_DVD_date = str(movie_info_list[i+1].text.strip().replace(\",\",\"\"))\n",
      "                movie[title][\"DVD date\"] = datetime.datetime.strptime(string_of_DVD_date, '%b %d %Y').strftime('%d%m%y')\n",
      "            \n",
      "            #Box Office\n",
      "            if \"Box Office:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"box office revenue\"] = float(movie_info_list[i+1].text.replace(\",\",\"\").replace(\"$\",\"\"))\n",
      "\n",
      "            #Runtime\n",
      "            if \"Runtime:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"runtime\"] = int(movie_info_list[i+1].text.strip().replace(\" minutes\",\"\"))\n",
      "            \n",
      "            #Studio\n",
      "            if \"Studio:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"studio\"] = str(movie_info_list[i+1].text.strip())            \n",
      "                \n",
      "\n",
      "        #Cast Names\n",
      "        cast_info = soup.find(\"div\", { \"class\" : 'castSection '})\n",
      "        if cast_info is None:\n",
      "            movie[title][\"number of cast\"] = 0\n",
      "        else:\n",
      "            cast = []\n",
      "            counter = 0\n",
      "\n",
      "            for i in cast_info.findAll(\"a\", { \"class\" : 'unstyled articleLink'}):\n",
      "                string1 = i.text.strip()\n",
      "                if string1 == \"Show More Cast\":\n",
      "                    continue\n",
      "                else:\n",
      "                    cast.append(string1)\n",
      "                    counter += 1\n",
      "            movie[title][\"cast\"] = cast\n",
      "            movie[title][\"number of cast\"] = counter\n",
      "    \n",
      "    except Exception as e:\n",
      "        print(title)\n",
      "        #movie.pop(title, None)\n",
      "        \n",
      "\"\"\"main function interates through movie_urls.txt and calls scraper function for each url\"\"\"\n",
      "def main_scraper():\n",
      "    base_url = 'https://www.rottentomatoes.com'\n",
      "    \n",
      "    with open('movie_urls.txt') as data_file:    \n",
      "        data = json.load(data_file)\n",
      "    \n",
      "    for title in data:\n",
      "        movie[str(title)] = {}\n",
      "        add_on = data[str(title)]\n",
      "        url = base_url + add_on\n",
      "        scraper(title, url)\n",
      "            \n",
      "main_scraper()\n",
      "\n",
      "\"\"\"uncomment next 2 lines to save dictionary in pickle file - binary\"\"\"\n",
      "#with open('movie_data.pkl', 'wb') as f:\n",
      "#    pickle.dump(movie, f, pickle.HIGHEST_PROTOCOL)\n",
      "64/8:\n",
      "from bs4 import BeautifulSoup\n",
      "from selenium import webdriver\n",
      "import statsmodels.api as sm\n",
      "import locale\n",
      "import json\n",
      "import datetime\n",
      "import pickle\n",
      "locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' )\n",
      "\n",
      "#sequence to initiate webpage for scraping\n",
      "#change url for chromedriver accordingly\n",
      "driver = webdriver.Chrome(\"C:/Users/burni/Desktop/python/chromedriver\")\n",
      "movie = {}\n",
      "error = 0    #error tracker\n",
      "\n",
      "\n",
      "\"\"\"function for scraping one movie profile\"\"\"\n",
      "def scraper(title, url):\n",
      "#uncomment next line to set url manually \n",
      "#     url = \"https://www.rottentomatoes.com/m/bowfinger\"\n",
      "    driver.get(url)\n",
      "    html = driver.page_source\n",
      "    soup = BeautifulSoup(html, \"lxml\")\n",
      "\n",
      "    try:\n",
      "        \"\"\"All Critics\"\"\"\n",
      "        movie[title][\"all critics\"] = {}\n",
      "        \n",
      "        #All Critics - number\n",
      "        all_critics = soup.find(\"div\", { \"id\" : \"all-critics-numbers\" })\n",
      "\n",
      "        #All Critics - tomatometer score\n",
      "        movie[title][\"all critics\"][\"tomatometer\"] = int(all_critics.find(\"a\", { \"id\" : 'tomato_meter_link' }).text.strip().replace(\"%\", \"\"))\n",
      "\n",
      "        all_critics_2 = all_critics.find(\"div\", { \"id\" : \"scoreStats\" })\n",
      "\n",
      "        #All Critics - average rating\n",
      "        movie[title][\"all critics\"][\"average_rating\"] = float(all_critics_2.find(\"div\", { \"class\" : 'superPageFontColor' }).text.split(\"\\n\")[2].strip().replace(\"/10\", \"\"))\n",
      "        all_critics_info = all_critics_2.findAll(\"span\", { \"class\" : '' })\n",
      "\n",
      "        #All Critics - reviews counted\n",
      "        movie[title][\"all critics\"][\"reviews_counted\"] = int(all_critics_info[0].text)\n",
      "\n",
      "        #All Critics - fresh\n",
      "        movie[title][\"all critics\"][\"fresh\"] = int(all_critics_info[1].text)\n",
      "\n",
      "        #All Critics - rotten\n",
      "        movie[title][\"all critics\"][\"rotten\"] = int(all_critics_info[2].text)\n",
      "\n",
      "        \"\"\"Top Critics - not all movies have sufficient information for Top Critics\"\"\"\n",
      "        find_top = soup.find(\"div\", { \"id\" : \"top-critics-numbers\" }).text\n",
      "        if \"Not Available\" in find_top:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"top critics\"] = {}\n",
      "            \n",
      "            #Top Critics - number\n",
      "            top_critics = soup.find(\"div\", { \"id\" : \"top-critics-numbers\" })\n",
      "\n",
      "            #Top Critics - tomatometer score\n",
      "            movie[title][\"top critics\"][\"tomatometer\"] = int(top_critics.find(\"a\", { \"id\" : 'tomato_meter_link' }).text.strip().replace(\"%\", \"\"))\n",
      "\n",
      "            top_critics_2 = top_critics.find(\"div\", { \"id\" : \"scoreStats\" })\n",
      "\n",
      "            #Top Critics - average rating\n",
      "            movie[title][\"top critics\"][\"average_rating\"] = float(top_critics_2.find(\"div\", { \"class\" : 'superPageFontColor' }).text.split(\"\\n\")[2].strip().replace(\"/10\", \"\"))\n",
      "\n",
      "            top_critics_info = top_critics_2.findAll(\"span\", { \"class\" : '' })\n",
      "\n",
      "            #Top Critics - reviews counted\n",
      "            movie[title][\"top critics\"][\"reviews_counted\"] = int(top_critics_info[0].text)\n",
      "\n",
      "            #Top Critics - fresh\n",
      "            movie[title][\"top critics\"][\"fresh\"] = int(top_critics_info[1].text)  \n",
      "\n",
      "            #Top Critics - rotten\n",
      "            movie[title][\"top critics\"][\"rotten\"] = int(top_critics_info[2].text)\n",
      "\n",
      "\n",
      "        \"\"\"Audience\"\"\"\n",
      "        movie[title][\"audience\"] = {}\n",
      "        \n",
      "        #Audience - score\n",
      "        movie[title][\"audience\"][\"score\"] = int(soup.find(\"div\", { \"class\" : \"meter-value\" }).text.split(\"\\n\")[1].replace(\"%\",\"\"))\n",
      "\n",
      "        audience_info =  soup.find(\"div\", { \"class\" : 'audience-info hidden-xs superPageFontColor' }).text.split(\"\\n\")\n",
      "\n",
      "        #Audience - average rating\n",
      "        movie[title][\"audience\"][\"average rating\"] = float(audience_info[3].strip().replace(\"/5\",\"\"))\n",
      "        #Audience - number\n",
      "        movie[title][\"audience\"][\"number of ratings\"] = int(audience_info[7].strip().replace(\",\", \"\"))\n",
      "\n",
      "        \"\"\"Marketing Information\"\"\"\n",
      "        #Number of Videos\n",
      "        find_videos = soup.find(\"div\", { \"class\" : 'clickForMore viewMoreVideos' })\n",
      "        if find_videos is None:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"number of videos\"] = int(soup.find(\"div\", { \"class\" : 'clickForMore viewMoreVideos' }).text.strip().split(\" \")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
      "        \n",
      "        #Number of Photos\n",
      "        find_photos = soup.find(\"div\", { \"class\" : 'clickForMore viewMorePhotos' })\n",
      "        if find_photos is None:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"number of photos\"] = int(soup.find(\"div\", { \"class\" : 'clickForMore viewMorePhotos' }).text.strip().split(\" \")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
      "        \n",
      "        \n",
      "        \"\"\"General Movie Information\"\"\"\n",
      "        movie_info = soup.find(\"div\", { \"class\" : 'info' })\n",
      "        movie_info_list = movie_info.findAll(\"div\")\n",
      "        \n",
      "        for i in range(0, len(movie_info_list)):\n",
      "            #Age Rating\n",
      "            if \"Rating:\" in str(movie_info_list[i]):\n",
      "                movie[title][\"age rating\"] = movie_info_list[i+1].text.split(\" \")[0]\n",
      "                \n",
      "            #Genres\n",
      "            if \"Genre:\" in str(movie_info_list[i]):\n",
      "                list_of_genres = movie_info_list[i+1].text.split(\",\")\n",
      "                movie[title][\"number of genres\"] = len(list_of_genres)\n",
      "                genres = []\n",
      "                for j in list_of_genres:\n",
      "                    string1 = str(j.strip())\n",
      "                    genres.append(string1)\n",
      "                movie[title][\"genres\"] = genres\n",
      "            \n",
      "            #Directors\n",
      "            if \"Directed By:\" in str(movie_info_list[i]):\n",
      "                list_of_movie_directors = movie_info_list[i+1].text.strip().split(\",\")\n",
      "                movie[title][\"number of directors\"] = len(list_of_movie_directors)\n",
      "                directors = [] \n",
      "                for j in list_of_movie_directors:\n",
      "                    string1 = str(j.strip())\n",
      "                    directors.append(string1)\n",
      "                movie[title][\"directors\"] = directors\n",
      "            \n",
      "            #Writers\n",
      "            if \"Written By:\" in str(movie_info_list[i]):     \n",
      "                list_of_movie_writers = movie_info_list[i+1].text.strip().split(\",\")\n",
      "                movie[title][\"number of writers\"] = len(list_of_movie_writers)\n",
      "                writers = [] \n",
      "                for j in list_of_movie_writers:\n",
      "                    string1 = str(j.strip())\n",
      "                    writers.append(string1)\n",
      "                movie[title][\"writers\"] = writers\n",
      "            \n",
      "            #Release Date\n",
      "            if \"In Theaters:\" in str(movie_info_list[i]): \n",
      "                #Relase Date and Type\n",
      "                release_info = movie_info_list[i+1].text.strip().split(\"\\n\")\n",
      "\n",
      "                #Date in Theatres\n",
      "                string_of_release_date = str(release_info[0].replace(\",\",\"\"))\n",
      "                movie[title][\"release date\"] = datetime.datetime.strptime(string_of_release_date, '%b %d %Y').strftime('%d%m%y')\n",
      "\n",
      "                #Release Type\n",
      "                if release_info[1] is not None:\n",
      "                    movie[title][\"release type\"] = release_info[1].strip()\n",
      "\n",
      "            #DVD Release Date\n",
      "            if \"On DVD:\" in str(movie_info_list[i]):    \n",
      "                string_of_DVD_date = str(movie_info_list[i+1].text.strip().replace(\",\",\"\"))\n",
      "                movie[title][\"DVD date\"] = datetime.datetime.strptime(string_of_DVD_date, '%b %d %Y').strftime('%d%m%y')\n",
      "            \n",
      "            #Box Office\n",
      "            if \"Box Office:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"box office revenue\"] = float(movie_info_list[i+1].text.replace(\",\",\"\").replace(\"$\",\"\"))\n",
      "\n",
      "            #Runtime\n",
      "            if \"Runtime:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"runtime\"] = int(movie_info_list[i+1].text.strip().replace(\" minutes\",\"\"))\n",
      "            \n",
      "            #Studio\n",
      "            if \"Studio:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"studio\"] = str(movie_info_list[i+1].text.strip())            \n",
      "                \n",
      "\n",
      "        #Cast Names\n",
      "        cast_info = soup.find(\"div\", { \"class\" : 'castSection '})\n",
      "        if cast_info is None:\n",
      "            movie[title][\"number of cast\"] = 0\n",
      "        else:\n",
      "            cast = []\n",
      "            counter = 0\n",
      "\n",
      "            for i in cast_info.findAll(\"a\", { \"class\" : 'unstyled articleLink'}):\n",
      "                string1 = i.text.strip()\n",
      "                if string1 == \"Show More Cast\":\n",
      "                    continue\n",
      "                else:\n",
      "                    cast.append(string1)\n",
      "                    counter += 1\n",
      "            movie[title][\"cast\"] = cast\n",
      "            movie[title][\"number of cast\"] = counter\n",
      "    \n",
      "    except Exception as e:\n",
      "        print(title)\n",
      "        #movie.pop(title, None)\n",
      "        \n",
      "\"\"\"main function interates through movie_urls.txt and calls scraper function for each url\"\"\"\n",
      "def main_scraper():\n",
      "    base_url = 'https://www.rottentomatoes.com'\n",
      "    \n",
      "    with open('movie_urls.txt') as data_file:    \n",
      "        data = json.load(data_file)\n",
      "    \n",
      "    for title in data:\n",
      "        movie[str(title)] = {}\n",
      "        add_on = data[str(title)]\n",
      "        url = base_url + add_on\n",
      "        scraper(title, url)\n",
      "            \n",
      "main_scraper()\n",
      "\n",
      "\"\"\"uncomment next 2 lines to save dictionary in pickle file - binary\"\"\"\n",
      "#with open('movie_data.pkl', 'wb') as f:\n",
      "#    pickle.dump(movie, f, pickle.HIGHEST_PROTOCOL)\n",
      "63/39: links\n",
      "63/40: names = [a.attrs.get('title') for a in mytable.select('a')]\n",
      "63/41: names\n",
      "63/42: names = [a.attrs.get('class') for a in mytable.select('a')]\n",
      "63/43: names\n",
      "63/44: mytable.get_text()\n",
      "63/45: mylink.get_text()\n",
      "63/46: mylinks.get_text()\n",
      "63/47: mytable.get_text()\n",
      "63/48: mytable.get_text().strip('\\n')\n",
      "63/49: name = mytable.get_text()\n",
      "63/50: name.strip(\"\\n\")\n",
      "63/51: name.split(\")\")\n",
      "63/52: str(name.split(\")\"))\n",
      "63/53: name.split(\"\\n\")\n",
      "63/54: name.lstrip(\" \")\n",
      "63/55: name.strip(\" \")\n",
      "63/56: name = mytable.get_text()\n",
      "63/57: name = name.split(\"\\n\")\n",
      "63/58: name.strip(\" \")\n",
      "63/59: name.remove(' ')\n",
      "63/60: name\n",
      "63/61: name.remove(' ')\n",
      "63/62: name.remove( )\n",
      "63/63: name.remove(\\b)\n",
      "63/64: name.clear(' ')\n",
      "63/65: name.clear()\n",
      "63/66: name = name.split(\"\\n\")\n",
      "63/67: name = mytable.get_text()\n",
      "63/68: name = name.split(\"\\n\")\n",
      "63/69: str(name).strip('n')\n",
      "63/70: str(name).strip('n').strip(\"\\'\\'\")\n",
      "63/71: str(name).strip('n').strip(\"\\'\\'\").strip(\" \\'\\', \\'\\', \\'\\', \\'\\', \\'\")\n",
      "63/72: name = str(name).strip('n')\n",
      "63/73: name = strip(\" \\'\\', \\'\\', \\'\\', \\'\\', \\'\")\n",
      "63/74: name = name.strip(\" \\'\\', \\'\\', \\'\\', \\'\\', \\'\")\n",
      "63/75:\n",
      "name = name.strip(\" \\'\\', \\'\\', \\'\\', \\'\\', \\'\")\n",
      "name\n",
      "63/76:\n",
      "name = name.strip(\"\\'\")\n",
      "name\n",
      "63/77:\n",
      "name = name.split(\"\\'\")\n",
      "name\n",
      "63/78: name = mytable.get_text()\n",
      "63/79: name = name.split(\"\\n\")\n",
      "63/80:\n",
      "name = name.split(\"\\n\")\n",
      "name\n",
      "63/81: name = mytable.get_text()\n",
      "63/82:\n",
      "name = name.split(\"\\n\")\n",
      "name\n",
      "63/83:\n",
      "def remove_values_from_list(the_list, val):\n",
      "   return [value for value in the_list if value != val]\n",
      "name = remove_values_from_list(name, ' ')\n",
      "63/84: name\n",
      "63/85:\n",
      "def remove_values_from_list(the_list, val):\n",
      "   return [value for value in the_list if value != val]\n",
      "name = remove_values_from_list(name, '')\n",
      "63/86: name\n",
      "63/87:\n",
      "def remove_values_from_list(the_list, val):\n",
      "   return [value for value in the_list if value != val]\n",
      "names = remove_values_from_list(name, '')\n",
      "63/88:\n",
      "def remove_values_from_list(the_list, val):\n",
      "   return [value for value in the_list if value != val]\n",
      "names = remove_values_from_list(name, '')\n",
      "len(name)\n",
      "63/89:\n",
      "def remove_values_from_list(the_list, val):\n",
      "   return [value for value in the_list if value != val]\n",
      "names = remove_values_from_list(name, '')\n",
      "len(names)\n",
      "63/90:\n",
      "name = name.split(\"\\n\")\n",
      "name\n",
      "63/91: name = mytable.get_text()\n",
      "63/92:\n",
      "name = name.split(\"\\n\")\n",
      "name\n",
      "63/93:\n",
      "def remove_values_from_list(the_list, val):\n",
      "   return [value for value in the_list if value != val]\n",
      "names = remove_values_from_list(name, '')\n",
      "len(names)\n",
      "63/94:\n",
      "titles = []\n",
      "i = 7\n",
      "while i < 403:\n",
      "    n=names[i]\n",
      "    titles.append(n)\n",
      "    i =+ 4\n",
      "65/1:\n",
      "import json\n",
      "import requests\n",
      "65/2:\n",
      "import json\n",
      "import requests\n",
      "65/3: url = \"https://www.rottentomatoes.com/top/bestofrt/\"\n",
      "65/4: rottentomatoes = requests.get(url)\n",
      "65/5: rottentomatoes.text\n",
      "65/6: import bs4\n",
      "65/7: mypage = bs4.BeautifulSoup(rottentomatoes.text)\n",
      "65/8: mytable = mypage.find_next('table')\n",
      "65/9: from bs4 import BeautifulSoup\n",
      "65/10: mytable\n",
      "65/11: mytable = mypage.find(\"table\", {\"class\":\"table\"})\n",
      "65/12: mytable\n",
      "65/13: mylinks = mytable.select('a')\n",
      "65/14: mylinks\n",
      "65/15: links = [a.attrs.get('href') for a in mytable.select('a')]\n",
      "65/16: name = mytable.get_text()\n",
      "65/17:\n",
      "name = name.split(\"\\n\")\n",
      "name\n",
      "65/18:\n",
      "def remove_values_from_list(the_list, val):\n",
      "   return [value for value in the_list if value != val]\n",
      "names = remove_values_from_list(name, '')\n",
      "len(names)\n",
      "65/19:\n",
      "titles = []\n",
      "i = 7\n",
      "while i < 403:\n",
      "    n=names[i]\n",
      "    titles.append(n)\n",
      "    i =+ 4\n",
      "67/1:\n",
      "import json\n",
      "import requests\n",
      "67/2:\n",
      "import json\n",
      "import requests\n",
      "67/3: url = \"https://www.rottentomatoes.com/top/bestofrt/\"\n",
      "67/4: rottentomatoes = requests.get(url)\n",
      "67/5: rottentomatoes.text\n",
      "67/6: import bs4\n",
      "67/7: mypage = bs4.BeautifulSoup(rottentomatoes.text)\n",
      "67/8: mytable = mypage.find_next('table')\n",
      "67/9: from bs4 import BeautifulSoup\n",
      "67/10: mytable\n",
      "67/11: mytable = mypage.find(\"table\", {\"class\":\"table\"})\n",
      "67/12: mytable\n",
      "67/13: mylinks = mytable.select('a')\n",
      "67/14: mylinks\n",
      "67/15: links = [a.attrs.get('href') for a in mytable.select('a')]\n",
      "67/16: name = mytable.get_text()\n",
      "67/17:\n",
      "name = name.split(\"\\n\")\n",
      "name\n",
      "67/18:\n",
      "def remove_values_from_list(the_list, val):\n",
      "   return [value for value in the_list if value != val]\n",
      "names = remove_values_from_list(name, '')\n",
      "len(names)\n",
      "67/19:\n",
      "titles = []\n",
      "i = 7\n",
      "while a < 403:\n",
      "    \n",
      "    titles.append(names[a])\n",
      "    a =+ 4\n",
      "67/20:\n",
      "titles = []\n",
      "a = 7\n",
      "while a < 403:\n",
      "    \n",
      "    titles.append(names[a])\n",
      "    a =+ 4\n",
      "68/1:\n",
      "import json\n",
      "import requests\n",
      "68/2: url = \"https://www.rottentomatoes.com/top/bestofrt/\"\n",
      "68/3: rottentomatoes = requests.get(url)\n",
      "68/4: rottentomatoes.text\n",
      "68/5: import bs4\n",
      "68/6: mypage = bs4.BeautifulSoup(rottentomatoes.text)\n",
      "68/7: mytable = mypage.find_next('table')\n",
      "68/8: from bs4 import BeautifulSoup\n",
      "68/9: mytable\n",
      "68/10: mytable = mypage.find(\"table\", {\"class\":\"table\"})\n",
      "68/11: mytable\n",
      "68/12: mylinks = mytable.select('a')\n",
      "68/13: mylinks\n",
      "68/14: links = [a.attrs.get('href') for a in mytable.select('a')]\n",
      "68/15: name = mytable.get_text()\n",
      "68/16:\n",
      "name = name.split(\"\\n\")\n",
      "name\n",
      "68/17:\n",
      "def remove_values_from_list(the_list, val):\n",
      "   return [value for value in the_list if value != val]\n",
      "names = remove_values_from_list(name, '')\n",
      "len(names)\n",
      "68/18:\n",
      "titles = []\n",
      "a = 7\n",
      "while a < 403:\n",
      "    \n",
      "    titles.append(names[a])\n",
      "    a += 4\n",
      "68/19: titles\n",
      "68/20:\n",
      "titles = []\n",
      "a = 6\n",
      "while a < 403:\n",
      "    \n",
      "    titles.append(names[a])\n",
      "    a += 4\n",
      "68/21: titles\n",
      "68/22: titles[0]\n",
      "68/23:\n",
      "for title in titles:\n",
      "    title = str(title).replace('\\t', '')\n",
      "68/24:\n",
      "for title in titles:\n",
      "    title = str(title).replace('\\t', '')\n",
      "titles\n",
      "68/25:\n",
      "titlec = []\n",
      "for title in titles:\n",
      "    title = str(title).replace('\\t', '')\n",
      "    titlec.append(title)\n",
      "titlec\n",
      "68/26: str(titles[0]).replace('\\t', '')\n",
      "68/27: str(titles[0]).strip('')\n",
      "68/28: str(titles[0]).strip(''')\n",
      "68/29: str(titles[0]).strip('\\'')\n",
      "68/30: str(titles[0]).strip(\"'\")\n",
      "68/31: str(titles[0])\n",
      "68/32: titles\n",
      "68/33:\n",
      "name = mytable.get_text()\n",
      "name\n",
      "68/34:\n",
      "name = name.split(\"\\n\")\n",
      "name\n",
      "68/35: titles[18]\n",
      "68/36:\n",
      "fulllinks = []\n",
      "for link in links:\n",
      "    fulllink = 'https://www.rottentomatoes.com' + link\n",
      "    \n",
      "    fulllinks.append(fulllink)\n",
      "68/37: fulllinks\n",
      "68/38: file = open('movie_urls.txt','w')\n",
      "68/39: links\n",
      "68/40: movie_list\n",
      "68/41: movie_list = dict(zip(titles, links))\n",
      "68/42: movies_list\n",
      "68/43: movie_list = dict(zip(titles, links))\n",
      "68/44: movies_list\n",
      "68/45: movie_list\n",
      "68/46: file.closed()\n",
      "68/47: file.close()\n",
      "68/48:\n",
      "open('movie_urls.txt','w') as file:\n",
      "        json.dump(movie_list,file, indent=4)\n",
      "68/49:\n",
      "file = open('movie_urls.txt','w') \n",
      "json.dump(movie_list,file, indent=4)\n",
      "68/50: file.close\n",
      "68/51: file.close()\n",
      "64/9:\n",
      "from bs4 import BeautifulSoup\n",
      "from selenium import webdriver\n",
      "import statsmodels.api as sm\n",
      "import locale\n",
      "import json\n",
      "import datetime\n",
      "import pickle\n",
      "locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' )\n",
      "\n",
      "#sequence to initiate webpage for scraping\n",
      "#change url for chromedriver accordingly\n",
      "driver = webdriver.Chrome(\"C:/Users/burni/Desktop/python/chromedriver\")\n",
      "movie = {}\n",
      "error = 0    #error tracker\n",
      "\n",
      "\n",
      "\"\"\"function for scraping one movie profile\"\"\"\n",
      "def scraper(title, url):\n",
      "#uncomment next line to set url manually \n",
      "#     url = \"https://www.rottentomatoes.com/m/bowfinger\"\n",
      "    driver.get(url)\n",
      "    html = driver.page_source\n",
      "    soup = BeautifulSoup(html, \"lxml\")\n",
      "\n",
      "    try:\n",
      "        \"\"\"All Critics\"\"\"\n",
      "        movie[title][\"all critics\"] = {}\n",
      "        \n",
      "        #All Critics - number\n",
      "        all_critics = soup.find(\"div\", { \"id\" : \"all-critics-numbers\" })\n",
      "\n",
      "        #All Critics - tomatometer score\n",
      "        movie[title][\"all critics\"][\"tomatometer\"] = int(all_critics.find(\"a\", { \"id\" : 'tomato_meter_link' }).text.strip().replace(\"%\", \"\"))\n",
      "\n",
      "        all_critics_2 = all_critics.find(\"div\", { \"id\" : \"scoreStats\" })\n",
      "\n",
      "        #All Critics - average rating\n",
      "        movie[title][\"all critics\"][\"average_rating\"] = float(all_critics_2.find(\"div\", { \"class\" : 'superPageFontColor' }).text.split(\"\\n\")[2].strip().replace(\"/10\", \"\"))\n",
      "        all_critics_info = all_critics_2.findAll(\"span\", { \"class\" : '' })\n",
      "\n",
      "        #All Critics - reviews counted\n",
      "        movie[title][\"all critics\"][\"reviews_counted\"] = int(all_critics_info[0].text)\n",
      "\n",
      "        #All Critics - fresh\n",
      "        movie[title][\"all critics\"][\"fresh\"] = int(all_critics_info[1].text)\n",
      "\n",
      "        #All Critics - rotten\n",
      "        movie[title][\"all critics\"][\"rotten\"] = int(all_critics_info[2].text)\n",
      "\n",
      "        \"\"\"Top Critics - not all movies have sufficient information for Top Critics\"\"\"\n",
      "        find_top = soup.find(\"div\", { \"id\" : \"top-critics-numbers\" }).text\n",
      "        if \"Not Available\" in find_top:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"top critics\"] = {}\n",
      "            \n",
      "            #Top Critics - number\n",
      "            top_critics = soup.find(\"div\", { \"id\" : \"top-critics-numbers\" })\n",
      "\n",
      "            #Top Critics - tomatometer score\n",
      "            movie[title][\"top critics\"][\"tomatometer\"] = int(top_critics.find(\"a\", { \"id\" : 'tomato_meter_link' }).text.strip().replace(\"%\", \"\"))\n",
      "\n",
      "            top_critics_2 = top_critics.find(\"div\", { \"id\" : \"scoreStats\" })\n",
      "\n",
      "            #Top Critics - average rating\n",
      "            movie[title][\"top critics\"][\"average_rating\"] = float(top_critics_2.find(\"div\", { \"class\" : 'superPageFontColor' }).text.split(\"\\n\")[2].strip().replace(\"/10\", \"\"))\n",
      "\n",
      "            top_critics_info = top_critics_2.findAll(\"span\", { \"class\" : '' })\n",
      "\n",
      "            #Top Critics - reviews counted\n",
      "            movie[title][\"top critics\"][\"reviews_counted\"] = int(top_critics_info[0].text)\n",
      "\n",
      "            #Top Critics - fresh\n",
      "            movie[title][\"top critics\"][\"fresh\"] = int(top_critics_info[1].text)  \n",
      "\n",
      "            #Top Critics - rotten\n",
      "            movie[title][\"top critics\"][\"rotten\"] = int(top_critics_info[2].text)\n",
      "\n",
      "\n",
      "        \"\"\"Audience\"\"\"\n",
      "        movie[title][\"audience\"] = {}\n",
      "        \n",
      "        #Audience - score\n",
      "        movie[title][\"audience\"][\"score\"] = int(soup.find(\"div\", { \"class\" : \"meter-value\" }).text.split(\"\\n\")[1].replace(\"%\",\"\"))\n",
      "\n",
      "        audience_info =  soup.find(\"div\", { \"class\" : 'audience-info hidden-xs superPageFontColor' }).text.split(\"\\n\")\n",
      "\n",
      "        #Audience - average rating\n",
      "        movie[title][\"audience\"][\"average rating\"] = float(audience_info[3].strip().replace(\"/5\",\"\"))\n",
      "        #Audience - number\n",
      "        movie[title][\"audience\"][\"number of ratings\"] = int(audience_info[7].strip().replace(\",\", \"\"))\n",
      "\n",
      "        \"\"\"Marketing Information\"\"\"\n",
      "        #Number of Videos\n",
      "        find_videos = soup.find(\"div\", { \"class\" : 'clickForMore viewMoreVideos' })\n",
      "        if find_videos is None:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"number of videos\"] = int(soup.find(\"div\", { \"class\" : 'clickForMore viewMoreVideos' }).text.strip().split(\" \")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
      "        \n",
      "        #Number of Photos\n",
      "        find_photos = soup.find(\"div\", { \"class\" : 'clickForMore viewMorePhotos' })\n",
      "        if find_photos is None:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"number of photos\"] = int(soup.find(\"div\", { \"class\" : 'clickForMore viewMorePhotos' }).text.strip().split(\" \")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
      "        \n",
      "        \n",
      "        \"\"\"General Movie Information\"\"\"\n",
      "        movie_info = soup.find(\"div\", { \"class\" : 'info' })\n",
      "        movie_info_list = movie_info.findAll(\"div\")\n",
      "        \n",
      "        for i in range(0, len(movie_info_list)):\n",
      "            #Age Rating\n",
      "            if \"Rating:\" in str(movie_info_list[i]):\n",
      "                movie[title][\"age rating\"] = movie_info_list[i+1].text.split(\" \")[0]\n",
      "                \n",
      "            #Genres\n",
      "            if \"Genre:\" in str(movie_info_list[i]):\n",
      "                list_of_genres = movie_info_list[i+1].text.split(\",\")\n",
      "                movie[title][\"number of genres\"] = len(list_of_genres)\n",
      "                genres = []\n",
      "                for j in list_of_genres:\n",
      "                    string1 = str(j.strip())\n",
      "                    genres.append(string1)\n",
      "                movie[title][\"genres\"] = genres\n",
      "            \n",
      "            #Directors\n",
      "            if \"Directed By:\" in str(movie_info_list[i]):\n",
      "                list_of_movie_directors = movie_info_list[i+1].text.strip().split(\",\")\n",
      "                movie[title][\"number of directors\"] = len(list_of_movie_directors)\n",
      "                directors = [] \n",
      "                for j in list_of_movie_directors:\n",
      "                    string1 = str(j.strip())\n",
      "                    directors.append(string1)\n",
      "                movie[title][\"directors\"] = directors\n",
      "            \n",
      "            #Writers\n",
      "            if \"Written By:\" in str(movie_info_list[i]):     \n",
      "                list_of_movie_writers = movie_info_list[i+1].text.strip().split(\",\")\n",
      "                movie[title][\"number of writers\"] = len(list_of_movie_writers)\n",
      "                writers = [] \n",
      "                for j in list_of_movie_writers:\n",
      "                    string1 = str(j.strip())\n",
      "                    writers.append(string1)\n",
      "                movie[title][\"writers\"] = writers\n",
      "            \n",
      "            #Release Date\n",
      "            if \"In Theaters:\" in str(movie_info_list[i]): \n",
      "                #Relase Date and Type\n",
      "                release_info = movie_info_list[i+1].text.strip().split(\"\\n\")\n",
      "\n",
      "                #Date in Theatres\n",
      "                string_of_release_date = str(release_info[0].replace(\",\",\"\"))\n",
      "                movie[title][\"release date\"] = datetime.datetime.strptime(string_of_release_date, '%b %d %Y').strftime('%d%m%y')\n",
      "\n",
      "                #Release Type\n",
      "                if release_info[1] is not None:\n",
      "                    movie[title][\"release type\"] = release_info[1].strip()\n",
      "\n",
      "            #DVD Release Date\n",
      "            if \"On DVD:\" in str(movie_info_list[i]):    \n",
      "                string_of_DVD_date = str(movie_info_list[i+1].text.strip().replace(\",\",\"\"))\n",
      "                movie[title][\"DVD date\"] = datetime.datetime.strptime(string_of_DVD_date, '%b %d %Y').strftime('%d%m%y')\n",
      "            \n",
      "            #Box Office\n",
      "            if \"Box Office:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"box office revenue\"] = float(movie_info_list[i+1].text.replace(\",\",\"\").replace(\"$\",\"\"))\n",
      "\n",
      "            #Runtime\n",
      "            if \"Runtime:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"runtime\"] = int(movie_info_list[i+1].text.strip().replace(\" minutes\",\"\"))\n",
      "            \n",
      "            #Studio\n",
      "            if \"Studio:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"studio\"] = str(movie_info_list[i+1].text.strip())            \n",
      "                \n",
      "\n",
      "        #Cast Names\n",
      "        cast_info = soup.find(\"div\", { \"class\" : 'castSection '})\n",
      "        if cast_info is None:\n",
      "            movie[title][\"number of cast\"] = 0\n",
      "        else:\n",
      "            cast = []\n",
      "            counter = 0\n",
      "\n",
      "            for i in cast_info.findAll(\"a\", { \"class\" : 'unstyled articleLink'}):\n",
      "                string1 = i.text.strip()\n",
      "                if string1 == \"Show More Cast\":\n",
      "                    continue\n",
      "                else:\n",
      "                    cast.append(string1)\n",
      "                    counter += 1\n",
      "            movie[title][\"cast\"] = cast\n",
      "            movie[title][\"number of cast\"] = counter\n",
      "    \n",
      "    except Exception as e:\n",
      "        print(title)\n",
      "        #movie.pop(title, None)\n",
      "        \n",
      "\"\"\"main function interates through movie_urls.txt and calls scraper function for each url\"\"\"\n",
      "def main_scraper():\n",
      "    base_url = 'https://www.rottentomatoes.com'\n",
      "    \n",
      "    with open('movie_urls.txt') as data_file:    \n",
      "        data = json.load(data_file)\n",
      "    \n",
      "    for title in data:\n",
      "        movie[str(title)] = {}\n",
      "        add_on = data[str(title)]\n",
      "        url = base_url + add_on\n",
      "        scraper(title, url)\n",
      "            \n",
      "main_scraper()\n",
      "\n",
      "\"\"\"uncomment next 2 lines to save dictionary in pickle file - binary\"\"\"\n",
      "#with open('movie_data.pkl', 'wb') as f:\n",
      "#    pickle.dump(movie, f, pickle.HIGHEST_PROTOCOL)\n",
      "64/10:\n",
      "from bs4 import BeautifulSoup\n",
      "from selenium import webdriver\n",
      "import statsmodels.api as sm\n",
      "import locale\n",
      "import json\n",
      "import datetime\n",
      "import pickle\n",
      "locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' )\n",
      "\n",
      "#sequence to initiate webpage for scraping\n",
      "#change url for chromedriver accordingly\n",
      "driver = webdriver.Chrome(\"C:/Users/burni/Desktop/python/chromedriver\")\n",
      "movie = {}\n",
      "error = 0    #error tracker\n",
      "\n",
      "\n",
      "\"\"\"function for scraping one movie profile\"\"\"\n",
      "def scraper(title, url):\n",
      "#uncomment next line to set url manually \n",
      "#     url = \"https://www.rottentomatoes.com/m/bowfinger\"\n",
      "    driver.get(url)\n",
      "    html = driver.page_source\n",
      "    soup = BeautifulSoup(html, \"lxml\")\n",
      "\n",
      "    try:\n",
      "        \"\"\"All Critics\"\"\"\n",
      "        movie[title][\"all critics\"] = {}\n",
      "        \n",
      "        #All Critics - number\n",
      "        all_critics = soup.find(\"div\", { \"id\" : \"all-critics-numbers\" })\n",
      "\n",
      "        #All Critics - tomatometer score\n",
      "        movie[title][\"all critics\"][\"tomatometer\"] = int(all_critics.find(\"a\", { \"id\" : 'tomato_meter_link' }).text.strip().replace(\"%\", \"\"))\n",
      "\n",
      "        all_critics_2 = all_critics.find(\"div\", { \"id\" : \"scoreStats\" })\n",
      "\n",
      "        #All Critics - average rating\n",
      "        movie[title][\"all critics\"][\"average_rating\"] = float(all_critics_2.find(\"div\", { \"class\" : 'superPageFontColor' }).text.split(\"\\n\")[2].strip().replace(\"/10\", \"\"))\n",
      "        all_critics_info = all_critics_2.findAll(\"span\", { \"class\" : '' })\n",
      "\n",
      "        #All Critics - reviews counted\n",
      "        movie[title][\"all critics\"][\"reviews_counted\"] = int(all_critics_info[0].text)\n",
      "\n",
      "        #All Critics - fresh\n",
      "        movie[title][\"all critics\"][\"fresh\"] = int(all_critics_info[1].text)\n",
      "\n",
      "        #All Critics - rotten\n",
      "        movie[title][\"all critics\"][\"rotten\"] = int(all_critics_info[2].text)\n",
      "\n",
      "        \"\"\"Top Critics - not all movies have sufficient information for Top Critics\"\"\"\n",
      "        find_top = soup.find(\"div\", { \"id\" : \"top-critics-numbers\" }).text\n",
      "        if \"Not Available\" in find_top:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"top critics\"] = {}\n",
      "            \n",
      "            #Top Critics - number\n",
      "            top_critics = soup.find(\"div\", { \"id\" : \"top-critics-numbers\" })\n",
      "\n",
      "            #Top Critics - tomatometer score\n",
      "            movie[title][\"top critics\"][\"tomatometer\"] = int(top_critics.find(\"a\", { \"id\" : 'tomato_meter_link' }).text.strip().replace(\"%\", \"\"))\n",
      "\n",
      "            top_critics_2 = top_critics.find(\"div\", { \"id\" : \"scoreStats\" })\n",
      "\n",
      "            #Top Critics - average rating\n",
      "            movie[title][\"top critics\"][\"average_rating\"] = float(top_critics_2.find(\"div\", { \"class\" : 'superPageFontColor' }).text.split(\"\\n\")[2].strip().replace(\"/10\", \"\"))\n",
      "\n",
      "            top_critics_info = top_critics_2.findAll(\"span\", { \"class\" : '' })\n",
      "\n",
      "            #Top Critics - reviews counted\n",
      "            movie[title][\"top critics\"][\"reviews_counted\"] = int(top_critics_info[0].text)\n",
      "\n",
      "            #Top Critics - fresh\n",
      "            movie[title][\"top critics\"][\"fresh\"] = int(top_critics_info[1].text)  \n",
      "\n",
      "            #Top Critics - rotten\n",
      "            movie[title][\"top critics\"][\"rotten\"] = int(top_critics_info[2].text)\n",
      "\n",
      "\n",
      "        \"\"\"Audience\"\"\"\n",
      "        movie[title][\"audience\"] = {}\n",
      "        \n",
      "        #Audience - score\n",
      "        movie[title][\"audience\"][\"score\"] = int(soup.find(\"div\", { \"class\" : \"meter-value\" }).text.split(\"\\n\")[1].replace(\"%\",\"\"))\n",
      "\n",
      "        audience_info =  soup.find(\"div\", { \"class\" : 'audience-info hidden-xs superPageFontColor' }).text.split(\"\\n\")\n",
      "\n",
      "        #Audience - average rating\n",
      "        movie[title][\"audience\"][\"average rating\"] = float(audience_info[3].strip().replace(\"/5\",\"\"))\n",
      "        #Audience - number\n",
      "        movie[title][\"audience\"][\"number of ratings\"] = int(audience_info[7].strip().replace(\",\", \"\"))\n",
      "\n",
      "        \"\"\"Marketing Information\"\"\"\n",
      "        #Number of Videos\n",
      "        find_videos = soup.find(\"div\", { \"class\" : 'clickForMore viewMoreVideos' })\n",
      "        if find_videos is None:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"number of videos\"] = int(soup.find(\"div\", { \"class\" : 'clickForMore viewMoreVideos' }).text.strip().split(\" \")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
      "        \n",
      "        #Number of Photos\n",
      "        find_photos = soup.find(\"div\", { \"class\" : 'clickForMore viewMorePhotos' })\n",
      "        if find_photos is None:\n",
      "            pass\n",
      "        else:\n",
      "            movie[title][\"number of photos\"] = int(soup.find(\"div\", { \"class\" : 'clickForMore viewMorePhotos' }).text.strip().split(\" \")[3].replace(\"(\",\"\").replace(\")\",\"\"))\n",
      "        \n",
      "        \n",
      "        \"\"\"General Movie Information\"\"\"\n",
      "        movie_info = soup.find(\"div\", { \"class\" : 'info' })\n",
      "        movie_info_list = movie_info.findAll(\"div\")\n",
      "        \n",
      "        for i in range(0, len(movie_info_list)):\n",
      "            #Age Rating\n",
      "            if \"Rating:\" in str(movie_info_list[i]):\n",
      "                movie[title][\"age rating\"] = movie_info_list[i+1].text.split(\" \")[0]\n",
      "                \n",
      "            #Genres\n",
      "            if \"Genre:\" in str(movie_info_list[i]):\n",
      "                list_of_genres = movie_info_list[i+1].text.split(\",\")\n",
      "                movie[title][\"number of genres\"] = len(list_of_genres)\n",
      "                genres = []\n",
      "                for j in list_of_genres:\n",
      "                    string1 = str(j.strip())\n",
      "                    genres.append(string1)\n",
      "                movie[title][\"genres\"] = genres\n",
      "            \n",
      "            #Directors\n",
      "            if \"Directed By:\" in str(movie_info_list[i]):\n",
      "                list_of_movie_directors = movie_info_list[i+1].text.strip().split(\",\")\n",
      "                movie[title][\"number of directors\"] = len(list_of_movie_directors)\n",
      "                directors = [] \n",
      "                for j in list_of_movie_directors:\n",
      "                    string1 = str(j.strip())\n",
      "                    directors.append(string1)\n",
      "                movie[title][\"directors\"] = directors\n",
      "            \n",
      "            #Writers\n",
      "            if \"Written By:\" in str(movie_info_list[i]):     \n",
      "                list_of_movie_writers = movie_info_list[i+1].text.strip().split(\",\")\n",
      "                movie[title][\"number of writers\"] = len(list_of_movie_writers)\n",
      "                writers = [] \n",
      "                for j in list_of_movie_writers:\n",
      "                    string1 = str(j.strip())\n",
      "                    writers.append(string1)\n",
      "                movie[title][\"writers\"] = writers\n",
      "            \n",
      "            #Release Date\n",
      "            if \"In Theaters:\" in str(movie_info_list[i]): \n",
      "                #Relase Date and Type\n",
      "                release_info = movie_info_list[i+1].text.strip().split(\"\\n\")\n",
      "\n",
      "                #Date in Theatres\n",
      "                string_of_release_date = str(release_info[0].replace(\",\",\"\"))\n",
      "                movie[title][\"release date\"] = datetime.datetime.strptime(string_of_release_date, '%b %d %Y').strftime('%d%m%y')\n",
      "\n",
      "                #Release Type\n",
      "                if release_info[1] is not None:\n",
      "                    movie[title][\"release type\"] = release_info[1].strip()\n",
      "\n",
      "            #DVD Release Date\n",
      "            if \"On DVD:\" in str(movie_info_list[i]):    \n",
      "                string_of_DVD_date = str(movie_info_list[i+1].text.strip().replace(\",\",\"\"))\n",
      "                movie[title][\"DVD date\"] = datetime.datetime.strptime(string_of_DVD_date, '%b %d %Y').strftime('%d%m%y')\n",
      "            \n",
      "            #Box Office\n",
      "            if \"Box Office:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"box office revenue\"] = float(movie_info_list[i+1].text.replace(\",\",\"\").replace(\"$\",\"\"))\n",
      "\n",
      "            #Runtime\n",
      "            if \"Runtime:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"runtime\"] = int(movie_info_list[i+1].text.strip().replace(\" minutes\",\"\"))\n",
      "            \n",
      "            #Studio\n",
      "            if \"Studio:\" in str(movie_info_list[i]):    \n",
      "                movie[title][\"studio\"] = str(movie_info_list[i+1].text.strip())            \n",
      "                \n",
      "\n",
      "        #Cast Names\n",
      "        cast_info = soup.find(\"div\", { \"class\" : 'castSection '})\n",
      "        if cast_info is None:\n",
      "            movie[title][\"number of cast\"] = 0\n",
      "        else:\n",
      "            cast = []\n",
      "            counter = 0\n",
      "\n",
      "            for i in cast_info.findAll(\"a\", { \"class\" : 'unstyled articleLink'}):\n",
      "                string1 = i.text.strip()\n",
      "                if string1 == \"Show More Cast\":\n",
      "                    continue\n",
      "                else:\n",
      "                    cast.append(string1)\n",
      "                    counter += 1\n",
      "            movie[title][\"cast\"] = cast\n",
      "            movie[title][\"number of cast\"] = counter\n",
      "    \n",
      "    except Exception as e:\n",
      "        print(title)\n",
      "        #movie.pop(title, None)\n",
      "        \n",
      "\"\"\"main function interates through movie_urls.txt and calls scraper function for each url\"\"\"\n",
      "def main_scraper():\n",
      "    base_url = 'https://www.rottentomatoes.com'\n",
      "    \n",
      "    with open('movie_urls.txt') as data_file:    \n",
      "        data = json.load(data_file)\n",
      "    \n",
      "    for title in data:\n",
      "        movie[str(title)] = {}\n",
      "        add_on = data[str(title)]\n",
      "        url = base_url + add_on\n",
      "        scraper(title, url)\n",
      "            \n",
      "main_scraper()\n",
      "\n",
      "\"\"\"uncomment next 2 lines to save dictionary in pickle file - binary\"\"\"\n",
      "with open('movie_data.pkl', 'wb') as f:\n",
      "    pickle.dump(movie, f, pickle.HIGHEST_PROTOCOL)\n",
      "64/11: import pickle\n",
      "64/12:\n",
      "pickle_off = open(\"movie_data.pkl\",\"rb\")\n",
      "emp = pickle.load(pickle_off)\n",
      "print(emp)\n",
      "69/1:\n",
      "import csv\n",
      "from six.moves import cPickle as pickle\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def main(path_pickle,path_csv):\n",
      "\n",
      "    x = []\n",
      "    with open(path_pickle,'rb') as f:\n",
      "        x = pickle.load(f)\n",
      "\n",
      "    with open(path_csv,'wb') as f:\n",
      "        writer = csv.writer(f)\n",
      "        for line in x: writer.writerow(line)\n",
      "69/2: main('movie_data.pkl', 'movie_data.csv')\n",
      "69/3:\n",
      "import csv\n",
      "from six.moves import cPickle as pickle\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def main(path_pickle,path_csv):\n",
      "\n",
      "    x = []\n",
      "    with open(path_pickle,'rb') as f:\n",
      "        x = pickle.load(f)\n",
      "\n",
      "    with open(path_csv,'wb') as f:\n",
      "        writer = csv.writer(f)\n",
      "        for line in x: \n",
      "            writer.writerow(line)\n",
      "69/4: main('movie_data.pkl', 'movie_data.csv')\n",
      "69/5:\n",
      "import csv\n",
      "from six.moves import cPickle as pickle\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def main(path_pickle,path_csv):\n",
      "\n",
      "    x = []\n",
      "    with open(path_pickle,'rb') as f:\n",
      "        x = pickle.load(f)\n",
      "\n",
      "    with open(path_csv,'wb') as file:\n",
      "        pickle.dump(x, file)\n",
      "        file.close()\n",
      "69/6: main('movie_data.pkl', 'movie_data.csv')\n",
      "69/7:\n",
      "with open('movie_data.csv', 'rb') as c:\n",
      "    print(c)\n",
      "69/8:\n",
      "with open('movie_data.csv', 'wb') as csvfile:\n",
      "...     spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
      "...     for row in spamreader:\n",
      "...         print(', '.join(row))\n",
      "69/9:\n",
      "with open('movie_data.csv', 'wb') as csvfile:\n",
      "...     spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
      "...     for row in spamreader:\n",
      "...         print(','.join(row))\n",
      "69/10: main('movie_data.pkl', 'movie_data.csv')\n",
      "69/11:\n",
      "with open('movie_data.csv', 'wb') as csvfile: \n",
      "    spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
      "    for row in spamreader:\n",
      "        print(','.join(row))\n",
      "69/12:\n",
      "import csv\n",
      "from six.moves import cPickle as pickle\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def main(path_pickle,path_csv):\n",
      "\n",
      "    \n",
      "    with open(path_pickle,'rb') as f:\n",
      "        x = pickle.load(f)\n",
      "\n",
      "    with open(path_csv,'wb') as file:\n",
      "        pickle.dump(x, file)\n",
      "        file.close()\n",
      "69/13: main('movie_data.pkl', 'movie_data.csv')\n",
      "69/14:\n",
      "with open('movie_data.csv', 'wb') as csvfile: \n",
      "    spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
      "    for row in spamreader:\n",
      "        print(','.join(row))\n",
      "69/15: pick.load(\"movie_data.pkl\", 'rb')\n",
      "69/16: pickle.load(\"movie_data.pkl\", 'rb')\n",
      "69/17: pickle.load(\"movie_data.pkl\")\n",
      "69/18:\n",
      "with open(path_pickle,'rb') as f:\n",
      "     x = pickle.load(f)\n",
      "69/19:\n",
      "with open(path_pickle,'rb') as f:\n",
      "    x = pickle.load(f)\n",
      "69/20:\n",
      "with open('movie_data.pkl','rb') as f:\n",
      "    x = pickle.load(f)\n",
      "69/21: print(x)\n",
      "69/22:\n",
      "import csv\n",
      "with open('movies_data.csv','w',newline='',encoding='utf-8') as f:\n",
      "    writer = csv.writer(f)\n",
      "    \n",
      "    writer.writerows(x)\n",
      "69/23:\n",
      "import csv\n",
      "with open('movies_data.csv','w',newline='',encoding='utf-8') as f:\n",
      "    writer = csv.writer(f)\n",
      "    \n",
      "    writer.writerows(x)\n",
      "    f.close()\n",
      "69/24: x.get['            The Wizard of Oz (1939)']\n",
      "69/25: x.get('            The Wizard of Oz (1939)')\n",
      "69/26: x.get('            The Wizard of Oz (1939)'.'all crititcs')\n",
      "69/27: x.get('            The Wizard of Oz (1939)')(.'all crititcs')\n",
      "69/28: x.get('            The Wizard of Oz (1939)')('all crititcs')\n",
      "69/29: x.get('            The Wizard of Oz (1939)').get('all crititcs')\n",
      "69/30: x.get('            The Wizard of Oz (1939)')\n",
      "69/31: metadata = x.get('            The Wizard of Oz (1939)')\n",
      "69/32: all_critics = metadata.get(\"all critics\")\n",
      "69/33: all_critics\n",
      "69/34: tomotameter = all_critics(\"tomatometer\")\n",
      "69/35: tomotameter = all_critics[\"tomatometer\"]\n",
      "69/36: tomatometer\n",
      "69/37: tomatometer = all_critics[\"tomatometer\"]\n",
      "69/38: tomatometer\n",
      "71/1: import pickle\n",
      "71/2:\n",
      "with open(\"movie_list.txt\", \"wb\") as fp:   #Pickling\n",
      "    pickle.dump(titles, fp)\n",
      "71/3:\n",
      "import json\n",
      "import requests\n",
      "71/4: url = \"https://www.rottentomatoes.com/top/bestofrt/\"\n",
      "71/5: rottentomatoes = requests.get(url)\n",
      "71/6: rottentomatoes.text\n",
      "71/7: import bs4\n",
      "71/8: mypage = bs4.BeautifulSoup(rottentomatoes.text)\n",
      "71/9: mytable = mypage.find_next('table')\n",
      "71/10: from bs4 import BeautifulSoup\n",
      "71/11: mytable = mypage.find(\"table\", {\"class\":\"table\"})\n",
      "71/12: mylinks = mytable.select('a')\n",
      "71/13: links = [a.attrs.get('href') for a in mytable.select('a')]\n",
      "71/14: name = mytable.get_text()\n",
      "71/15: name = name.split(\"\\n\")\n",
      "71/16:\n",
      "def remove_values_from_list(the_list, val):\n",
      "   return [value for value in the_list if value != val]\n",
      "names = remove_values_from_list(name, '')\n",
      "len(names)\n",
      "71/17:\n",
      "titles = []\n",
      "a = 6\n",
      "while a < 403:\n",
      "    \n",
      "    titles.append(names[a])\n",
      "    a += 4\n",
      "71/18: import pickle\n",
      "71/19:\n",
      "with open(\"movie_list.txt\", \"wb\") as fp:   #Pickling\n",
      "    pickle.dump(titles, fp)\n",
      "71/20: movie_list = dict(zip(titles, links))\n",
      "71/21:\n",
      "fulllinks = []\n",
      "i=0\n",
      "while i < 100:\n",
      "    for fulllink in fulllinks:\n",
      "        fulllink = 'https://www.rottentomatoes.com' + links[i]\n",
      "        i = i+1\n",
      "        fulllinks.append(fulllink)\n",
      "   1: import requests\n",
      "   2: url = \"https://www.rottentomatoes.com/top/bestofrt/\"\n",
      "   3: rottentomatoes = requests.get(url)\n",
      "   4: import bs4\n",
      "   5: mypage = bs4.BeautifulSoup(rottentomatoes.text)\n",
      "   6: mytable = mypage.find_next('table')\n",
      "   7: from bs4 import BeautifulSoup\n",
      "   8: mytable = mypage.find(\"table\", {\"class\":\"table\"})\n",
      "   9: mylinks = mytable.select('a')\n",
      "  10: links = [a.attrs.get('href') for a in mytable.select('a')]\n",
      "  11: %history\n",
      "  12: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = mytable.get_text()\n",
    "name = name.split(\"\\n\")\n",
    "\n",
    "def remove_values_from_list(the_list, val):\n",
    "    return [value for value in the_list if value != val]\n",
    "\n",
    "names = remove_values_from_list(name, '')\n",
    "\n",
    "titles = []\n",
    "a = 6\n",
    "while a < 403:\n",
    "    \n",
    "    titles.append(names[a])\n",
    "    a += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"movie_list.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(titles, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "movie_list = dict(zip(titles, links))\n",
    "file = open('movie_urls.txt','w')\n",
    "json.dump(movie_list ,file, indent=4)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
